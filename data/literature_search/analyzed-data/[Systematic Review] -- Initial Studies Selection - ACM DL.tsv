authors	title	abstract	publication_year	doi	url	IS_ACCEPTED
Ermilov, Timofey and Moussallem, Diego and Usbeck, Ricardo and Ngomo, Axel-Cyrille Ngonga	GENESIS: A Generic RDF Data Access Interface	The availability of billions of facts represented in RDF on the Web provides novel opportunities for data discovery and access. In particular, keyword search and question answering approaches enable even lay people to access this data. However, the interpretation of the results of these systems, as well as the navigation through these results, remains challenging. In this paper, we present Genesis, a generic RDF data access interface. Genesis can be deployed on top of any knowledge base and search engine with minimal effort and allows for the representation of RDF data in a layperson-friendly way. This is facilitated by the modular architecture for reusable components underlying our framework. Currently, these include a generic search back-end, together with corresponding interactive user interface components based on a service for similar and related entities as well as verbalization services to bridge between RDF and natural language.	2017	https://doi.org/10.1145/3106426.3106514	https://doi.org/10.1145/3106426.3106514	FALSE
Thakkar, Harsh and Endris, Kemele M. and Gimenez-Garcia, Jose M. and Debattista, Jeremy and Lange, Christoph and Auer, S\"{o}ren	Are Linked Datasets Fit for Open-Domain Question Answering? A Quality Assessment	The current decade is a witness to an enormous explosion of data being published on the Web as Linked Data to maximise its reusability. Answering questions that users speak or write in natural language is an increasingly popular application scenario for Web Data, especially when the domain of the questions is not limited to a domain where dedicated curated datasets exist, like in medicine. The increasing use of Web Data in this and other settings has highlighted the importance of assessing its quality. While quite some work has been done with regard to assessing the quality of Linked Data, only few efforts have been dedicated to quality assessment of linked data from the question answering domain's perspective. From the linked data quality metrics that have so far been well documented in the literature, we have identified those that are most relevant for QA. We apply these quality metrics, implemented in the Luzzu framework, to subsets of two datasets of crucial importance to open domain QA -- DBpedia and Wikidata -- and thus present the first assessment of the quality of these datasets for QA. From these datasets, we assess slices covering the specific domains of restaurants, politicians, films and soccer players. The results of our experiments suggest that for most of these domains, the quality of Wikidata with regard to the majority of relevant metrics is higher than that of DBpedia.	2016	https://doi.org/10.1145/2912845.2912857	https://doi.org/10.1145/2912845.2912857	FALSE
Diop, Ibrahima and Dieng, Youssou and Faye, Youssou and Malack, Camir A. and Cisse, Ousseynou and Diouf, Boubacar	Decrease the Number of Patients Lost to Follow-up in the Monitoring of PLHIV in Cross-Border Areas between The Gambia, Senegal and Guinea Bissau	The fight against AIDS in West Africa is a big challenge of public health. The main difficulties are related to the availability of antiretrovirals (AVR) and mainly to the observance of the treatment. In the cross-borderland areas of the Gambia, Senegal and Guinea Bissau, these difficulties of the observance of the treatment are amplified by the vulnerability, the high mobility of the populations and the problems of communication between the actors of the monitoring of PLHIV who speak three different languages. That's what makes difficult the fight against AIDS in those areas and justify the phenomenon of the patients lost to follow-up, which is a great indicator for the following of the People Living with HIV-virus (PLHIV). This paper presents the decrease of the number of patients lost to follow-up in the monitoring of the PLHIV in the cross-borderland areas of the Gambia, Senegal and Guinea Bissau by a multilingual semantic web platform of reference, counter reference and auto reference.	2019	https://doi.org/10.1145/3361570.3361596	https://doi.org/10.1145/3361570.3361596	FALSE
Fetahu, Besnik and Fang, Anjie and Rokhlenko, Oleg and Malmasi, Shervin	Gazetteer Enhanced Named Entity Recognition for Code-Mixed Web Queries	Named entity recognition (NER) for Web queries is very challenging. Queries often do not consist of well-formed sentences, and contain very little context, with highly ambiguous queried entities. Code-mixed queries, with entities in a different language than the rest of the query, pose a particular challenge in domains like e-commerce (e.g. queries containing movie or product names). This work tackles NER for code-mixed queries, where entities and non-entity query terms co-exist simultaneously in different languages. Our contributions are twofold. First, to address the lack of code-mixed NER data we create EMBER, a large-scale dataset in six languages with four different scripts. Based on Bing query data, we include numerous language combinations that showcase real-world search scenarios. Secondly, we propose a novel gated architecture that enhances existing multi-lingual Transformers with a Mixture-of-Experts model to dynamically infuse multi-lingual gazetteers, allowing it to simultaneously differentiate and handle entities and non-entity query terms in multiple languages. Experimental evaluation on code-mixed queries in several languages shows that our approach efficiently utilizes gazetteers to recognize entities in code-mixed queries with an F1=68%, an absolute improvement of +31% over a non-gazetteer baseline.	2021		https://doi.org/10.1145/3404835.3463102	FALSE
Thalhammer, Andreas and Thoma, Steffen and Harth, Andreas and Studer, Rudi	Entity-Centric Data Fusion on the Web	A lot of current web pages include structured data which can directly be processed and used. Search engines, in particular, gather that structured data and provide question answering capabilities over the integrated data with an entity-centric presentation of the results. Due to the decentralized nature of the web, multiple structured data sources can provide similar information about an entity. But data from different sources may involve different vocabularies and modeling granularities, which makes integration difficult. We present an approach that identifies similar entity-specific data across sources, independent of the vocabulary and data modeling choices. We apply our method along the scenario of a trustable knowledge panel, conduct experiments in which we identify and process entity data from web sources, and compare the output to a competing system. The results underline the advantages of the presented entity-centric data fusion approach.	2017	https://doi.org/10.1145/3078714.3078717	https://doi.org/10.1145/3078714.3078717	FALSE
Shekarpour, Saeedeh and Auer, Soren and Ngomo, Axel-Cyrille Ngonga and Gerber, Daniel and Hellmann, Sebastian and Stadler, Claus	Keyword-Driven SPARQL Query Generation Leveraging Background Knowledge	The search for information on the Web of Data is becoming increasingly difficult due to its dramatic growth. Especially novice users need to acquire both knowledge about the underlying ontology structure and proficiency in formulating formal queries (e. g. SPARQL queries) to retrieve information from Linked Data sources. So as to simplify and automate the querying and retrieval of information from such sources, we present in this paper a novel approach for constructing SPARQL queries based on user-supplied keywords. Our approach utilizes a set of predefined basic graph pattern templates for generating adequate interpretations of user queries. This is achieved by obtaining ranked lists of candidate resource identifiers for the supplied keywords and then injecting these identifiers into suitable positions in the graph pattern templates. The main advantages of our approach are that it is completely agnostic of the underlying knowledge base and ontology schema, that it scales to large knowledge bases and is simple to use. We evaluate17 possible valid graph pattern templates by measuring their precision and recall on 53 queries against DBpedia. Our results show that 8 of these basic graph pattern templates return results with a precision above 70%. Our approach is implemented as a Web search interface and performs sufficiently fast to return instant answers to the user even with large knowledge bases.	2011	https://doi.org/10.1109/WI-IAT.2011.70	https://doi.org/10.1109/WI-IAT.2011.70	FALSE
Deb Nath, Rudra Pratap and Hose, Katja and Pedersen, Torben Bach	Towards a Programmable Semantic Extract-Transform-Load Framework for Semantic Data Warehouses	In order to create better decisions for business analytics, organizations increasingly use external data, structured, semi-structured and unstructured, in addition to the (mostly structured) internal data. Current Extract-Transform-Load (ETL) tools are not suitable for this "open world scenario" because they do not consider semantic issues in the integration process. Also, current ETL tools neither support processing semantic-aware data nor create a Semantic Data Warehouse (DW) as a semantic repository of semantically integrated data. This paper describes SETL: a (Python-based) programmable Semantic ETL framework. SETL builds on Semantic Web (SW) standards and tools and supports developers by offering a number of powerful modules, classes and methods for (dimensional and semantic) DW constructs and tasks. Thus it supports semantic-aware data sources, semantic integration, and creating a semantic DW, composed of an ontology and its instances. A comprehensive experimental evaluation comparing SETL to a solution made with traditional tools (requiring much more hand-coding) on a concrete use case, shows that SETL provides better performance, knowledge base quality and programmer productivity.	2015	https://doi.org/10.1145/2811222.2811229	https://doi.org/10.1145/2811222.2811229	FALSE
Suchanek, Fabian and Weikum, Gerhard	Knowledge Harvesting in the Big-Data Era	The proliferation of knowledge-sharing communities such as Wikipedia and the progress in scalable information extraction from Web and text sources have enabled the automatic construction of very large knowledge bases. Endeavors of this kind include projects such as DBpedia, Freebase, KnowItAll, ReadTheWeb, and YAGO. These projects provide automatically constructed knowledge bases of facts about named entities, their semantic classes, and their mutual relationships. They contain millions of entities and hundreds of millions of facts about them. Such world knowledge in turn enables cognitive applications and knowledge-centric services like disambiguating natural-language text, semantic search for entities and relations in Web and enterprise data, and entity-oriented analytics over unstructured contents. Prominent examples of how knowledge bases can be harnessed include the Google Knowledge Graph and the IBM Watson question answering system. This tutorial presents state-of-the-art methods, recent advances, research opportunities, and open challenges along this avenue of knowledge harvesting and its applications. Particular emphasis will be on the twofold role of knowledge bases for big-data analytics: using scalable distributed algorithms for harvesting knowledge from Web and text sources, and leveraging entity-centric knowledge for deeper interpretation of and better intelligence with Big Data.	2013	https://doi.org/10.1145/2463676.2463724	https://doi.org/10.1145/2463676.2463724	FALSE
Hinze, Annika and Taube-Schock, Craig and Bainbridge, David and Matamua, Rangi and Downie, J. Stephen	Improving Access to Large-Scale Digital Libraries ThroughSemantic-Enhanced Search and Disambiguation	With 13,000,000 volumes comprising 4.5 billion pages of text, it is currently very difficult for scholars to locate relevant sets of documents that are useful in their research from the HathiTrust Digital Libary (HTDL) using traditional lexically-based retrieval techniques. Existing document search tools and document clustering approaches use purely lexical analysis, which cannot address the inherent ambiguity of natural language. A semantic search approach offers the potential to overcome the shortcoming of lexical search, but even if an appropriate network of ontologies could be decided upon it would require a full semantic markup of each document. In this paper, we present a conceptual design and report on the initial implementation of a new framework that affords the benefits of semantic search while minimizing the problems associated with applying existing semantic analysis at scale. Our approach avoids the need for complete semantic document markup using pre-existing ontologies by developing an automatically generated Concept-in-Context (CiC) network seeded by a priori analysis of Wikipedia texts and identification of semantic metadata. Our Capisco system analyzes documents by the semantics and context of their content. The disambiguation of search queries is done interactively, to fully utilize the domain knowledge of the scholar. Our method achieves a form of semantic-enhanced search that simultaneously exploits the proven scale benefits provided by lexical indexing.	2015	https://doi.org/10.1145/2756406.2756920	https://doi.org/10.1145/2756406.2756920	FALSE
Christianson, Aaron and Dadvar, Maral and Eckert, Kai	Reverse-Transliteration of Hebrew Script for Entity Disambiguation	JudaicaLink is a novel domain-specific knowledge base for Jewish culture, history, and studies. JudaicaLink is built by extracting structured, multilingual knowledge from different sources and it is mainly used for contextualization and entity linking. One of the main challenges in the process of aggregating Jewish digital resources is the use of the Hebrew script. The proof of materials in German central cataloging systems is based on the conversion of the original script of the publication into the Latin script, known as Romanization. Many of our datasets, especially those from library catalogs, contain Hebrew authors' names and titles which are only in Latin script without their Hebrew script. Therefore, it is not possible to identify them in and link them to other corresponding Hebrew resources. To overcome this problem, we designed a reverse-transliteration model which reconstructs the Hebrew script from the Romanization and consequently makes the entities more accessible.	2019	https://doi.org/10.1145/3366030.3366099	https://doi.org/10.1145/3366030.3366099	FALSE
Liu, Jialu and Liu, Tianqi and Yu, Cong	NewsEmbed: Modeling News through Pre-Trained Document Representations	Effectively modeling text-rich fresh content such as news articles at document-level is a challenging problem. To ensure a content-based model generalize well to a broad range of applications, it is critical to have a training dataset that is large beyond the scale of human labels while achieving desired quality. In this work, we address those two challenges by proposing a novel approach to mine semantically-relevant fresh documents, and their topic labels, with little human supervision. Meanwhile, we design a multitask model called NewsEmbed that alternatively trains a contrastive learning with a multi-label classification to derive a universal document encoder. We show that the proposed approach can provide billions of high quality organic training examples and can be naturally extended to multilingual setting where texts in different languages are encoded in the same semantic space. We experimentally demonstrate NewsEmbed's competitive performance across multiple natural language understanding tasks, both supervised and unsupervised.	2021	https://doi.org/10.1145/3447548.3467392	https://doi.org/10.1145/3447548.3467392	FALSE
Bast, Hannah and Haussmann, Elmar	More Accurate Question Answering on Freebase	Real-world factoid or list questions often have a simple structure, yet are hard to match to facts in a given knowledge base due to high representational and linguistic variability. For example, to answer "who is the ceo of apple" on Freebase requires a match to an abstract "leadership" entity with three relations "role", "organization" and "person", and two other entities "apple inc" and "managing director". Recent years have seen a surge of research activity on learning-based solutions for this method. We further advance the state of the art by adopting learning-to-rank methodology and by fully addressing the inherent entity recognition problem, which was neglected in recent works.We evaluate our system, called Aqqu, on two standard benchmarks, Free917 and WebQuestions, improving the previous best result for each benchmark considerably. These two benchmarks exhibit quite different challenges, and many of the existing approaches were evaluated (and work well) only for one of them. We also consider efficiency aspects and take care that all questions can be answered interactively (that is, within a second). Materials for full reproducibility are available on our website: http://ad.informatik.uni-freiburg.de/publications.	2015	https://doi.org/10.1145/2806416.2806472	https://doi.org/10.1145/2806416.2806472	FALSE
Al-Nazer, Ahmed and Helmy, Tarek	Toward a Cross-Cultural and Cross-Language Multi-Agent Recommendation Model for Food and Nutrition	In this paper, we present our efforts to develop a multi-agent-based framework for cross-cultural and cross-language personalized health and nutrition semantic search. Many agents that share either the same culture or same language or same health profile could share valuable information found through their semantic search. Their interests in a common viewpoint have some similarities which could be used to shorten the learning curve and give good search results effectively. Each agent is representing a user with a specific culture, language and personal health profile. What an agent likes as culture is likely similar to a different agent with the same culture. The same thing applies with language and health profiles. For example, an agent with a diabetes profile could help another agent that has a similar health profile with its valuable findings. We introduce how multi-agents could team up to contribute to each other, learn from each other, and share valuable information with each other using collaborative profile ontology. We propose a cross-cultural, cross-language, health- and nutrition-based ontology profile that could be used as the basis for collaboration between different agents.	2012	https://doi.org/10.1109/WI-IAT.2012.263	https://doi.org/10.1109/WI-IAT.2012.263	FALSE
Rumi\'{n}ski, Dariusz and Walczak, Krzysztof	An Architecture for Distributed Semantic Augmented Reality Services	In this paper, we present an architecture for distributed augmented reality (AR) services. The presented architecture is based on a client-server design, which supports semantic modeling and building contextual AR presentations for a large number of users. It encompasses two approaches: SOA and CARE. The Service-Oriented Architecture enables building distributed systems that provide application functionality as services to either end-user applications or other services. In turn, CARE allows semantic modeling of AR environments, which combined with SOA enables dividing responsibility between loosely coupled semantic services distributed on the internet. We also provide the results of an experimental evaluation of the performance of a prototype based on the above-mentioned architecture. The findings are promising and demonstrate that an application of semantic web techniques can be an effective approach to implementation of large-scale contextual distributed AR services.	2018	https://doi.org/10.1145/3208806.3208829	https://doi.org/10.1145/3208806.3208829	FALSE
Metzger, Steffen and Elbassuoni, Shady and Hose, Katja and Schenkel, Ralf	S3K: Seeking Statement-Supporting Top-K Witnesses	Traditional information retrieval techniques based on keyword search help to identify a ranked set of relevant documents, which often contains many documents in the top ranks that do not meet the user's intention. By considering the semantics of the keywords and their relationships, both precision and recall can be improved. Using an ontology and mapping keywords to entities/concepts and identifying the relationship between them that the user is interested in, allows for retrieving documents that actually meet the user's intention. In this paper, we present a framework that enables semantic-aware document retrieval. User queries are mapped to semantic statements based on entities and their relationships. The framework searches for documents expressing these statements in different variations, e.g., synonymous names for entities or different textual expressions for relations between them. The size of potential result sets makes ranking documents according to their relevance to the user an essential component of such a system. The ranking model proposed in this paper is based on statistical language-models and considers aspects such as the authority of a document and the confidence in the textual pattern representing the queried information.	2011	https://doi.org/10.1145/2063576.2063587	https://doi.org/10.1145/2063576.2063587	FALSE
Bharadwaj, S. and Chiticariu, L. and Danilevsky, M. and Dhingra, S. and Divekar, S. and Carreno-Fuentes, A. and Gupta, H. and Gupta, N. and Han, S.-D. and Hern\'{a}ndez, M. and Ho, H. and Jain, P. and Joshi, S. and Karanam, H. and Krishnan, S. and Krishnamurthy, R. and Li, Y. and Manivannan, S. and Mittal, A. and \"{O}zcan, F. and Quamar, A. and Raman, P. and Saha, D. and Sankaranarayanan, K. and Sen, J. and Sen, P. and Vaithyanathan, S. and Vasa, M. and Wang, H. and Zhu, H.	Creation and Interaction with Large-Scale Domain-Specific Knowledge Bases	The ability to create and interact with large-scale domain-specific knowledge bases from unstructured/semi-structured data is the foundation for many industry-focused cognitive systems. We will demonstrate the Content Services system that provides cloud services for creating and querying high-quality domain-specific knowledge bases by analyzing and integrating multiple (un/semi)structured content sources. We will showcase an instantiation of the system for a financial domain. We will also demonstrate both cross-lingual natural language queries and programmatic API calls for interacting with this knowledge base.	2017	https://doi.org/10.14778/3137765.3137820	https://doi.org/10.14778/3137765.3137820	FALSE
Chechev, Milen and Gonz\`{a}lez, Meritxell and M\`{a}rquez, Llu\'{\i}s and Espa\~{n}a-Bonet, Cristina	The Patents Retrieval Prototype in the MOLTO Project	This paper describes the patents retrieval prototype developed within the MOLTO project. The prototype aims to provide a multilingual natural language interface for querying the content of patent documents. The developed system is focused on the biomedical and pharmaceutical domain and includes the translation of the patent claims and abstracts into English, French and German. Aiming at the best retrieval results of the patent information and text content, patent documents are preprocessed and semantically annotated. Then, the annotations are stored and indexed in an OWLIM semantic repository, which contains a patent specific ontology and others from different domains. The prototype, accessible online at http://molto-patents.ontotext.com, presents a multilingual natural language interface to query the retrieval system. In MOLTO, the multilingualism of the queries is addressed by means of the GF Tool, which provides an easy way to build and maintain controlled language grammars for interlingual translation in limited domains. The abstract representation obtained from the GF is used to retrieve both the matched RDF instances and the list of patents semantically related to the user's search criteria. The online interface allows to browse the retrieved patents and shows on the text the semantic annotations that explain the reason why any particular patent has matched the user's criteria.	2012	https://doi.org/10.1145/2187980.2188016	https://doi.org/10.1145/2187980.2188016	FALSE
Tran, Nam Khanh and Nieder\'{e}e, Claudia	A Neural Network-Based Framework for Non-Factoid Question Answering	In this paper, we present a neural network based framework for answering non-factoid questions. The framework consists of two main components: Answer Retriever and Answer Ranker. In the first component, we leverage off-the-shelf retrieval models (e.g. bm25) to retrieve a pool of candidate answers regarding to the input question. Answer Ranker is then used to select the most suitable answer. In this work, we adopt two typical deep learning based frameworks for our Answer Ranker component. One is based on Siamese architecture and the other is the Compare-Aggregate framework. The Answer Ranker component is evaluated separately based on popular answer selection datasets. Our overall system is evaluated using FiQA dataset, a newly released dataset for financial domain and shows promising results.	2018	https://doi.org/10.1145/3184558.3191830	https://doi.org/10.1145/3184558.3191830	FALSE
Moussallem, Diego and Ngonga Ngomo, Axel-Cyrille and Buitelaar, Paul and Arcan, Mihael	Utilizing Knowledge Graphs for Neural Machine Translation Augmentation	While neural networks have led to substantial progress in machine translation, their success depends heavily on large amounts of training data. However, parallel training corpora are not always readily available. Moreover, out-of-vocabulary words---mostly entities and terminological expressions---pose a difficult challenge to Neural Machine Translation systems. Recent efforts have tried to alleviate the data sparsity problem by augmenting the training data using different strategies, such as external knowledge injection. In this paper, we hypothesize that knowledge graphs enhance the semantic feature extraction of neural models, thus optimizing the translation of entities and terminological expressions in texts and consequently leading to better translation quality. We investigate two different strategies for incorporating knowledge graphs into neural models without modifying the neural network architectures. Additionally, we examine the effectiveness of our augmented models on domain-specific texts and ontologies. Our knowledge-graph-augmented neural translation model, dubbed KG-NMT, achieves significant and consistent improvements of +3 BLEU, METEOR and chrF3 on average on the newstest datasets between 2015 and 2018 for the WMT English-German translation task.	2019	https://doi.org/10.1145/3360901.3364423	https://doi.org/10.1145/3360901.3364423	FALSE
Bonacin, Rodrigo and Nabuco, Olga Fernanda and Junior, Ivo Pierozzi	Conceptualizing the Impacts of Agriculture on Water Resources: Experiences and Ontology Engineering Challenges	Water is no longer considered an unlimited resource. As a matter of fact, many studies have shown a recent increase of water scarcity. By far, the largest demand for water resources comes from agricultural activities. Various researchers from multidisciplinary fields address the impacts of agricultural activities on water resources, as well as the impacts from possible climatic changes. In this context, the Embrapa's research network AgroHidro aims to support integration and information sharing among a range of institutions and researchers. In this paper, we present the first version of a Web Ontology to represent key aspects of this problem. The objective is to provide the basis for computational mechanisms in support of knowledge sharing and recovery, e.g., semantic search, text mining and visualization techniques. The paper also discusses the reuse and integration strategies, limitations, and challenges to be faced in the ontology engineering process.	2013	https://doi.org/10.1145/2536146.2536185	https://doi.org/10.1145/2536146.2536185	FALSE
Bhattacharya, Paheli and Goyal, Pawan and Sarkar, Sudeshna	Using Communities of Words Derived from Multilingual Word Vectors for Cross-Language Information Retrieval in Indian Languages	We investigate the use of word embeddings for query translation to improve precision in cross-language information retrieval (CLIR). Word vectors represent words in a distributional space such that syntactically or semantically similar words are close to each other in this space. Multilingual word embeddings are constructed in such a way that similar words across languages have similar vector representations. We explore the effective use of bilingual and multilingual word embeddings learned from comparable corpora of Indic languages to the task of CLIR.We propose a clustering method based on the multilingual word vectors to group similar words across languages. For this we construct a graph with words from multiple languages as nodes and with edges connecting words with similar vectors. We use the Louvain method for community detection to find communities in this graph. We show that choosing target language words as query translations from the clusters or communities containing the query terms helps in improving CLIR. We also find that better-quality query translations are obtained when words from more languages are used to do the clustering even when the additional languages are neither the source nor the target languages. This is probably because having more similar words across multiple languages helps define well-defined dense subclusters that help us obtain precise query translations.In this article, we demonstrate the use of multilingual word embeddings and word clusters for CLIR involving Indic languages. We also make available a tool for obtaining related words and the visualizations of the multilingual word vectors for English, Hindi, Bengali, Marathi, Gujarati, and Tamil.	2018	https://doi.org/10.1145/3208358	https://doi.org/10.1145/3208358	FALSE
Jagarlamudi, Jagadeesh and Bennett, Paul N. and Svore, Krysta M.	Leveraging Interlingual Classification to Improve Web Search	In this paper we address the problem of improving accuracy of web search in a smaller, data-limited search market (search language) using behavioral data from a larger, data-rich market (assist language). Specifically, we use interlingual classification to infer the search language query's intent using the assist language click-through data. We use these improved estimates of query intent, along with the query intent based on the search language data, to compute features that encode the similarity between a search result (URL) and the query. These features are subsequently fed into the ranking model to improve the relevance ranking of the documents. Our experimental results on German and French languages show the effectiveness of using assist language behavioral data especially, when the search language queries have small click-through data.	2012	https://doi.org/10.1145/2187980.2188114	https://doi.org/10.1145/2187980.2188114	FALSE
Jin, Qiao and Yuan, Zheng and Xiong, Guangzhi and Yu, Qianlan and Ying, Huaiyuan and Tan, Chuanqi and Chen, Mosha and Huang, Songfang and Liu, Xiaozhong and Yu, Sheng	Biomedical Question Answering: A Survey of Approaches and Challenges	Automatic Question Answering (QA) has been successfully applied in various domains such as search engines and chatbots. Biomedical QA (BQA), as an emerging QA task, enables innovative applications to effectively perceive, access, and understand complex biomedical knowledge. There have been tremendous developments of BQA in the past two decades, which we classify into five distinctive approaches: classic, information retrieval, machine reading comprehension, knowledge base, and question entailment approaches. In this survey, we introduce available datasets and representative methods of each BQA approach in detail. Despite the developments, BQA systems are still immature and rarely used in real-life settings. We identify and characterize several key challenges in BQA that might lead to this issue, and we discuss some potential future directions to explore.	2022	https://doi.org/10.1145/3490238	https://doi.org/10.1145/3490238	TRUE
Vertan, Cristina	Two Approaches for Integrating Translation and Retrieval in Real Applications	In this paper we present two approaches for integrating translation into cross-lingual search engines: the first approach relies on term translation via a language ontology, the other one is based on machine translation of specific information.	2012			FALSE
Singh, Kuldeep and Mulang', Isaiah Onando and Lytra, Ioanna and Jaradeh, Mohamad Yaser and Sakor, Ahmad and Vidal, Maria-Esther and Lange, Christoph and Auer, S\"{o}ren	Capturing Knowledge in Semantically-Typed Relational Patterns to Enhance Relation Linking	Transforming natural language questions into formal queries is an integral task in Question Answering (QA) systems. QA systems built on knowledge graphs like DBpedia, require a step after natural language processing for linking words, specifically including named entities and relations, to their corresponding entities in a knowledge graph. To achieve this task, several approaches rely on background knowledge bases containing semantically-typed relations, e.g., PATTY, for an extra disambiguation step. Two major factors may affect the performance of relation linking approaches whenever background knowledge bases are accessed: a) limited availability of such semantic knowledge sources, and b) lack of a systematic approach on how to maximize the benefits of the collected knowledge. We tackle this problem and devise SIBKB, a semantic-based index able to capture knowledge encoded on background knowledge bases like PATTY. SIBKB represents a background knowledge base as a bi-partite and a dynamic index over the relation patterns included in the knowledge base. Moreover, we develop a relation linking component able to exploit SIBKB features. The benefits of SIBKB are empirically studied on existing QA benchmarks and observed results suggest that SIBKB is able to enhance the accuracy of relation linking by up to three times.	2017	https://doi.org/10.1145/3148011.3148031	https://doi.org/10.1145/3148011.3148031	FALSE
Buscaldi, Davide and Zargayouna, Ha\"{\i}fa	YaSemIR: Yet Another Semantic Information Retrieval System	In this paper we present YaSemIR, a free open-source Semantic Information Retrieval system based on Lucene. It takes one or more ontologies in OWL format and a terminology associated to each ontology in SKOS format to index semantically a text collection. The terminology is used to annotate concepts in documents, while the ontology is used to exploit the taxonomic information in order to expand these with their subsumers. YaSemIR is a flexible system that may be configured to work with different ontologies, on various types of documents.	2013	https://doi.org/10.1145/2513204.2513211	https://doi.org/10.1145/2513204.2513211	FALSE
Maia, Macedo and Handschuh, Siegfried and Freitas, Andr\'{e} and Davis, Brian and McDermott, Ross and Zarrouk, Manel and Balahur, Alexandra	WWW'18 Open Challenge: Financial Opinion Mining and Question Answering	The growing maturity of Natural Language Processing (NLP) techniques and resources is dramatically changing the landscape of many application domains which are dependent on the analysis of unstructured data at scale. The finance domain, with its reliance on the interpretation of multiple unstructured and structured data sources and its demand for fast and comprehensive decision making is already emerging as a primary ground for the experimentation of NLP, Web Mining and Information Retrieval (IR) techniques for the automatic analysis of financial news and opinions online. This challenge focuses on advancing the state-of-the-art of aspect-based sentiment analysis and opinion-based Question Answering for the financial domain.	2018	https://doi.org/10.1145/3184558.3192301	https://doi.org/10.1145/3184558.3192301	FALSE
Tatu, Marta and Werner, Steven and Balakrishna, Mithun and Erekhinskaya, Tatiana and Moldovan, Dan	Semantic Question Answering on Big Data	This article describes a high-precision semantic question answering (SQA) engine for large datasets. We employ an RDF store to index the semantic information extracted from large document collections and a natural language to SPARQL conversion module to find desired information. In order to be able to find answers to complex questions in structured/unstructured data resources, our system produces rich semantic structures from the data resources and then transforms the extracted knowledge into an RDF representation. In order to facilitate easy access to the information stored in the RDF semantic index, our system accepts a user's natural language questions, translates them into SPARQL queries and returns a precise answer back to the user. Our improvements in performance over a regular free text search index-based question answering engine prove that SQA can benefit greatly from the addition and consumption of deep semantic information.	2016	https://doi.org/10.1145/2928294.2928302	https://doi.org/10.1145/2928294.2928302	FALSE
Kolikipogu, Ramakrishna and Rani, B. Padmaja and Kakulapati, Vijayalakshmi	Reformulation of Telugu Web Query Using Word Semantic Relationships	Use of Internet becomes more popular in India to avail the information needs. A major area of Information browsing includes Education, Medical, Agriculture, Geographical, Business and other social domains. Availability of electronic documents for Indian Language is growing day by day. The people living throughout India speak different languages. The government of India has given "languages of the 8th Schedule" official status for 22 languages. Compare to European languages and other Indian languages, processing of Telugu language electronic documents is more difficult in nature. This is due to multi --- encoding formats of the text. Indian languages are encoded using Unicode, ISCII. To fasten the retrieval process the Unicode or ISCII is need to be converted into simple and standard encoding which makes Information Retrieval as easy task. Once the information processing system is build for a mono-lingual, it is the base to go for Multi-lingual and Cross --- lingual information processing. In Information Retrieval process users expects exact results for the given query. It depends on the vocabulary expertization of the end user in building the root query. Word mismatch is common problem of all languages in Information Retrieval process. Query Expansion gives a solution to the word mismatch problem. In Query Expansion the top ranked documents are used to expand the query terms. Sometimes user need to judge the relevance of the expanded query to iterate search. The relevance judgment of the user depends on the knowledge (i.e Language knowledge to describe the con text of the query) of the user. If the con cept hierarchy is properly defined, then user involvement is void in this scenario. This can be easily test on English language, but applying Query Reformulation technique directly on Indian languages is not stands good, because the nature of Indian languages is not simple like English. The Paper is aimed to reduce the mismatch between user query and retrieved documents by using semantic relationships between query terms and document terms. To test the proposed model, Telugu language, one of the Indian languages is taken as a case study. True translation from English to Telugu and vice versa is not possible due to high word conflation in Indian languages. This paper is an attempt to adopt Semantic Network with semantic relationships between terms of a query to reformulate and iterate the search. Method of Relevance Feedback improves recall without compromising precision, but it works well on limited corpus. Reformulation of query by embedding WordNet, ConceptNet relationsh ips gave better results, but great fall of precision is observed. Comparison between initial query test results and reformulated query search results are made in result analysis.	2012	https://doi.org/10.1145/2345396.2345523	https://doi.org/10.1145/2345396.2345523	FALSE
Surdeanu, Mihai and Ciaramita, Massimiliano and Zaragoza, Hugo	Learning to Rank Answers to Non-Factoid Questions from Web Collections	This work investigates the use of linguistically motivated features to improve search, in particular for ranking answers to non-factoid questions. We show that it is possible to exploit existing large collections of question-answer pairs (from online social Question Answering sites) to extract such features and train ranking models which combine them effectively. We investigate a wide range of feature types, some exploiting natural language processing such as coarse word sense disambiguation, named-entity identification, syntactic parsing, and semantic role labeling. Our experiments demonstrate that linguistic features, in combination, yield considerable improvements in accuracy. Depending on the system settings we measure relative improvements of 14% to 21% in Mean Reciprocal Rank and Precision@1, providing one of the most compelling evidence to date that complex linguistic features such as word senses and semantic roles can have a significant impact on large-scale information retrieval tasks.	2011	https://doi.org/10.1162/COLI_a_00051	https://doi.org/10.1162/COLI_a_00051	FALSE
Dargahi Nobari, Arash and Askari, Arian and Hasibi, Faegheh and Neshati, Mahmood	Query Understanding via Entity Attribute Identification	Understanding searchers' queries is an essential component of semantic search systems. In many cases, search queries involve specific attributes of an entity in a knowledge base (KB), which can be further used to find query answers. In this study, we aim to move forward the understanding of queries by identifying their related entity attributes from a knowledge base. To this end, we introduce the task of entity attribute identification and propose two methods to address it: (i) a model based on Markov Random Field, and (ii) a learning to rank model. We develop a human annotated test collection and show that our proposed methods can bring significant improvements over the baseline methods.	2018	https://doi.org/10.1145/3269206.3269245	https://doi.org/10.1145/3269206.3269245	FALSE
Khullar, Aman and Santosh, M and Kumar, Praveen and Rahman, Shoaib and Tripathi, Rajeshwari and Kumar, Deepak and Saini, Sangeeta and Pandey, Rachit and Seth, Aaditeshwar	Early Results from Automating Voice-Based Question-Answering Services Among Low-Income Populations in India	 Question-answering systems where users can ask questions based on emergent needs which are then answered by experts or peers, have emerged as an important information seeking modality on digital platforms. Automating this process has been an active area of research since many years, to identify relevant answers from pre-existing question-answer databases. We report on the feasibility of running automated question-answering systems in the context of rural and less-literate users in India, accessed through IVR (Interactive Voice Response) systems. We use commercial speech recognition APIs to convert audio questions asked by users into their equivalent transcripts in real time, in Hindi, and use deep-learning based architectures to retrieve corresponding candidate answers which are instantly played to the users. We report several insights from an earlier phase of running question-answering programmes through a manual operation, to how it was transitioned to an automated setup, and document the user experiences during this journey.	2021		https://doi.org/10.1145/3460112.3471946	FALSE
Filip, David and Lewis, Dave and Sasaki, Felix	The Multilingual Web: Report on Multilingualweb Initiative	We report on the MultilingualWeb initiative, a collaboration between the W3C Internationalization Activity and the European Commission, realized as a series of EC-funded projects. We review the outcomes of "MultilingualWeb", which conducted 4 workshops analyzing "gaps" within Web standardization that currently hinder multilinguality. Gap analysis led to formation of "MultilingualWeb-LT" - project and W3C Working Group with cross industry representation that will address priority issues via standardization of interoperability metadata.	2012	https://doi.org/10.1145/2187980.2188021	https://doi.org/10.1145/2187980.2188021	FALSE
Ma, Pingchuan and Wang, Shuai	MT-Teql: Evaluating and Augmenting Neural NLIDB on Real-World Linguistic and Schema Variations	Natural Language Interface to Database (NLIDB) translates human utterances into SQL queries and enables database interactions for non-expert users. Recently, neural network models have become a major approach to implementing NLIDB. However, neural NLIDB faces challenges due to variations in natural language and database schema design. For instance, one user intent or database conceptual model can be expressed in various forms. However, existing benchmarks, using hold-out datasets, cannot provide thorough understanding of how good neural NLIDBs really are in real-world situations and its robustness against such variations. A key difficulty is to annotate SQL queries for inputs under real-world variations, requiring considerable manual effort and expert knowledge.To systematically assess the robustness of neural NLIDBs without extensive manual effort, we propose MT-Teql, a unified framework to benchmark NLIDBs against real-world language and schema variations. Inspired by recent advances in DBMS metamorphic testing, MT-Teql implements semantics-preserving transformations on utterances and database schemas to generate their variants. NLIDBs can thus be examined for robustness utilizing utterances/schemas and their variants without requiring manual intervention.We benchmarked nine neural NLIDBs using 62,430 inputs and identified 15,433 defects. We analyzed potential root causes of defects and conducted a user study to show how MT-Teql can assist developers to systematically assess NLIDBs. We further show that the transformed (error-triggering) inputs can be used to augment popular NLIDBs and eliminate 46.5%(±5.0%) errors made by them without compromising their accuracy on standard benchmarks. We summarize lessons from this study that can provide insights to select and design NLIDBs that fit particular usage scenarios.	2021	https://doi.org/10.14778/3494124.3494139	https://doi.org/10.14778/3494124.3494139	FALSE
Valdestilhas, Andr\'{e} and Soru, Tommaso and Ngomo, Axel-Cyrille Ngonga	CEDAL: Time-Efficient Detection of Erroneous Links in Large-Scale Link Repositories	More than 500 million facts on the Linked Data Web are statements across knowledge bases. These links are of crucial importance for the Linked Data Web as they make a large number of tasks possible, including cross-ontology, question answering and federated queries. However, a large number of these links are erroneous and can thus lead to these applications producing absurd results. We present a time-efficient and complete approach for the detection of erroneous links for properties that are transitive. To this end, we make use of the semantics of URIs on the Data Web and combine it with an efficient graph partitioning algorithm. We then apply our algorithm to the LinkLion repository and show that we can analyze 19,200,114 links in 4.6 minutes. Our results show that at least 13% of the owl :sameAs links we considered are erroneous. In addition, our analysis of the provenance of links allows discovering agents and knowledge bases that commonly display poor linking. Our algorithm can be easily executed in parallel and on a GPU. We show that these implementations are up to two orders of magnitude faster than classical reasoners and a non-parallel implementation.	2017	https://doi.org/10.1145/3106426.3106497	https://doi.org/10.1145/3106426.3106497	FALSE
Rocha, Oscar Rodr\'{\i}guez and Criminisi, Carmen and Mondin, Fabio and Goix, Laurent-Walter	LODifying Personal Content Sharing	The advent of contemporary mobile devices and their increasing computing power and location capabilities combined with the most innovative Web technologies has provided mobile users with new possibilities to share experiences on-the-go. The growing quantity of multimedia content present on the Web makes it difficult for mobile users to retrieve suitable content. Typically, users looking for interesting content related to their current position or POI (point of interest), access Web search engines relying on keywords to describe their ideas. Unfortunately such descriptions are often subjective and thus retrieval can be ineffective. To address these issues, our platform provides users with an application targeted for modern mobile devices that allows content acquisition and publication. Published content is automatically analyzed and stored on our server with semantic annotations based on the user's context and content, for further semantic search. We describe how and why we migrated from a triple-tags technology to semantics, hoping for related Linked Data.	2012	https://doi.org/10.1145/2320765.2320805	https://doi.org/10.1145/2320765.2320805	FALSE
Bacchelli, Alberto and Ponzanelli, Luca and Lanza, Michele	Harnessing Stack Overflow for the IDE	Developers often consult online tutorials and message boards to find solutions to their programming issues. Among the many online resources, Question &amp; Answer websites are gaining popularity. This is no wonder if we consider a case like Stack Overflow, where more than 92% questions on expert topics are answered in a median time of 11 minutes. This new resource has scarcely been acknowledged by any Integrated Development Environment (IDE): Even though developers spend a large part of their working time in IDEs, and the usage of Q&amp;A services has dramatically increased, developers can only use such resources using external applications.We introduce Seahawk, an Eclipse plugin to integrate Stack Overflow crowd knowledge in the IDE. It allows developers to seamlessly access Stack Overflow data, thus obtaining answers without switching the context. We present our preliminary work on Seahawk: It allows users to (1) retrieve Q&amp;A from Stack Overflow, (2) link relevant discussions to any source code in Eclipse, and (3) attach explanative comments to the links.	2012			FALSE
Noor, Umara and Rashid, Zahid and Rauf, Azhar	TODWEB: Training-Less Ontology Based Deep Web Source Classification	Today, deep web comprises of a large part of web contents. Because of this large volume of data, the technologies related to deep web have gained larger attention in recent years. Deep web mostly comprises of online domain specific databases, which are accessed by using web query interfaces. These highly relevant domain specific databases are more suitable for satisfying the information needs of the users. In order to make the extraction of relevant information easier, there is a need to classify the deep web databases into subject-specific self-descriptive categories. In this paper we present a novel training-less classification approach TODWEB based on common sense world knowledge (in the form of ontology or any external lexical resource) for the automatic deep web source classification; which will help in building highly scalable, domain focused and efficient semantic information retrieval systems (i.e. metasearch engine and search engine directories). One of the important aspects of this approach is the classification method which is completely training less and uses Wikipedia category network and domain-independent ontologies to analyze the semantics in the meta-information of the deep web sources. The large number of fine grained Wikipedia categories are employed to analyze semantic relatedness among concepts and finally the URL of deep web search source is mapped to the category hierarchy offered by Wikipedia. The experiments conducted on a collection of search sources shows that this approach results in a highly accurate and fine grained classification as compared to existing approaches, nearly identical to the results achieved by manual classification.	2011	https://doi.org/10.1145/2095536.2095569	https://doi.org/10.1145/2095536.2095569	FALSE
Conover, Michael and Hayes, Matthew and Blackburn, Scott and Skomoroch, Pete and Shah, Sam	Pangloss: Fast Entity Linking in Noisy Text Environments	Entity linking is the task of mapping potentially ambiguous terms in text to their constituent entities in a knowledge base like Wikipedia. This is useful for organizing content, extracting structured data from textual documents, and in machine learning relevance applications like semantic search, knowledge graph construction, and question answering. Traditionally, this work has focused on text that has been well-formed, like news articles, but in common real world datasets such as messaging, resumes, or short-form social media, non-grammatical, loosely-structured text adds a new dimension to this problem. This paper presents Pangloss, a production system for entity disambiguation on noisy text. Pangloss combines a probabilistic linear-time key phrase identification algorithm with a semantic similarity engine based on context-dependent document embeddings to achieve better than state-of-the-art results (&gt;5% in F1) compared to other research or commercially available systems. In addition, Pangloss leverages a local embedded database with a tiered architecture to house its statistics and metadata, which allows rapid disambiguation in streaming contexts and on-device disambiguation in low-memory environments such as mobile phones.	2018	https://doi.org/10.1145/3219819.3219899	https://doi.org/10.1145/3219819.3219899	FALSE
Le-Hong, Phuong and Bui, Duc-Thien	A Factoid Question Answering System for Vietnamese	In this paper, we describe the development of an end-to-end factoid question answering system for the Vietnamese language. This system combines both statistical models and ontology-based methods in a chain of processing modules to provide high-quality mappings from natural language text to entities. We present the challenges in the development of such an intelligent user interface for an isolating language like Vietnamese and show that techniques developed for inflectional languages cannot be applied as is. Our question answering system can answer a wide range of general knowledge questions with promising accuracy on a test set.	2018	https://doi.org/10.1145/3184558.3191535	https://doi.org/10.1145/3184558.3191535	FALSE
Severyn, Aliaksei and Moschitti, Alessandro	Structural Relationships for Large-Scale Learning of Answer Re-Ranking	Supervised learning applied to answer re-ranking can highly improve on the overall accuracy of question answering (QA) systems. The key aspect is that the relationships and properties of the question/answer pair composed of a question and the supporting passage of an answer candidate, can be efficiently compared with those captured by the learnt model.In this paper, we define novel supervised approaches that exploit structural relationships between a question and their candidate answer passages to learn a re-ranking model. We model structural representations of both questions and answers and their mutual relationships by just using an off-the-shelf shallow syntactic parser. We encode structures in Support Vector Machines (SVMs) by means of sequence and tree kernels, which can implicitly represent question and answer pairs in huge feature spaces. Such models together with the latest approach to fast kernel-based learning enabled the training of our rerankers on hundreds of thousands of instances, which previously rendered intractable for kernelized SVMs. The results on two different QA datasets, e.g., Answerbag and Jeopardy! data, show that our models deliver large improvement on passage re-ranking tasks, reducing the error in Recall of BM25 baseline by about 18%. One of the key findings of this work is that, despite its simplicity, shallow syntactic trees allow for learning complex relational structures, which exhibits a steep learning curve with the increase in the training size.	2012	https://doi.org/10.1145/2348283.2348383	https://doi.org/10.1145/2348283.2348383	FALSE
Xin, Kexuan and Sun, Zequn and Hua, Wen and Hu, Wei and Zhou, Xiaofang	Informed Multi-Context Entity Alignment	Entity alignment is a crucial step in integrating knowledge graphs (KGs) from multiple sources. Previous attempts at entity alignment have explored different KG structures, such as neighborhood-based and path-based contexts, to learn entity embeddings, but they are limited in capturing the multi-context features. Moreover, most approaches directly utilize the embedding similarity to determine entity alignment without considering the global interaction among entities and relations. In this work, we propose an Informed Multi-context Entity Alignment (IMEA) model to address these issues. In particular, we introduce Transformer to flexibly capture the relation, path, and neighborhood contexts, and design holistic reasoning to estimate alignment probabilities based on both embedding similarity and the relation/entity functionality. The alignment evidence obtained from holistic reasoning is further injected back into the Transformer via the proposed soft label editing to inform embedding learning. Experimental results on several benchmark datasets demonstrate the superiority of our IMEA model compared with existing state-of-the-art entity alignment methods.	2022	https://doi.org/10.1145/3488560.3498523	https://doi.org/10.1145/3488560.3498523	FALSE
Wolf, Lina and Knuth, Magnus and Osterhoff, Johannes and Sack, Harald	RISQ! Renowned Individuals Semantic Quiz: A Jeopardy like Quiz Game for Ranking Facts	In 2011 the IBM Computer Watson was beating its human opponents in the American TV quiz show Jeopardy!. However, the questions for the quiz have been developed by a team of human authors. Authoring questions is a difficult task, because in a Jeopardy! game the questions should be neither too easy nor too hard and should fit the general scope of knowledge of the audience and players. Linked Open Data (LOD) provides huge amounts of information that is growing daily. Yet, there is no ranking that determines the importance of LOD facts, as e. g. by querying LOD for movies starring a distinct actor provides numerous answers, whereas it cannot be answered, which of the movies was the most important for this actor. To rank search results for semantic search various heuristics have been developed to cope with the problem of missing rank in the semantic web. This paper proposes a Jeopardy! like quiz game with questions automatically generated from LOD facts to gather ranking information for persons to provide a basis for the evaluation of semantic ranking heuristics.	2011	https://doi.org/10.1145/2063518.2063528	https://doi.org/10.1145/2063518.2063528	FALSE
Basu, Joyanta and Khan, Soma and Roy, Rajib and Bepari, Milton S.	Commodity Price Retrieval System in Bangla: An IVR Based Application	Present paper describes the overall architecture, design and evaluation process of an Interactive Voice Response based Commodity Price Retrieval System for semiliterate or illiterate farmers. Here users just need to dial a 24x7 helpline number to get market wise latest price information on recognition of district and commodity names in Bangla language. Telephonic speech data has been collected automatically from all geographical regions of West Bengal to cover major dialectal variations. Sustained efforts have been given in real-time speech data collection, transcription, system design, evaluation and field trial analysis. Additionally to meet users' expectations in field conditions, distinctive error recovery methods like Signal Analysis and Decision, Confidence Measure and Polling, Complementary Information, Runtime model generation etc. are incorporated to confirm performance enhancement in final trial. This system is providing excellent value-addition to the already existing AGMARKNET website managed by ministry of agriculture, India; as no computers, internet, or even reading/writing skills are required for this application.	2013	https://doi.org/10.1145/2525194.2525310	https://doi.org/10.1145/2525194.2525310	FALSE
Gupta, Dhruv and Berberich, Klaus	Optimizing Hyper-Phrase Queries	A hyper-phrase query (HPQ) consists of a sequence of phrase sets. Such queries naturally arise when attempting to spot knowledge graph (KG) facts or sets of KG facts in large document collections to establish their provenance. Our approach addresses this challenge by proposing query operators to detect text regions in documents that correspond to the HPQ as combinations of n-grams and skip-grams. The optimization lies in identifying the most cost-efficient order of query operators that can be executed to identify the text regions containing the HPQ. We show the efficiency of our optimizations on spotting facts from Wikidata in document collections amounting to more than thirty million documents.	2020	https://doi.org/10.1145/3409256.3409827	https://doi.org/10.1145/3409256.3409827	FALSE
Dram\'{e}, Khadim and Smits, Gr\'{e}gory and Pivert, Olivier	Coarse to Fine Keyword Queries with User Interactions	A large amount of linked data is now available, but to retrieve knowledge from these data, queries have to be formulated using formal query languages. While expressive query languages are developed, their use by end users, generally not familiar with formal languages, is limited. Keyword-based search is considered as a convenient and intuitive way for users to express their information needs. Keyword search over structured data is thus an interesting alternative but which raises challenging issues. The main challenge is to determine the meaning of a keyword query in order to translate it into a target formal query language, SPARQL in our case. In this paper, we address this challenge and propose a novel approach that relies on user interactions to determine the correct interpretation of the keyword query. The principle is to first ask the user to define a coarse keyword query, then to suggest candidate interpretations expressed in an explicit, thus unambiguous, and human readable form. Once the correct interpretation has been selected, the query may be refined with aggregate functions and comparatives. Experiments conducted on a large knowledge base show the effectiveness and the efficiency of the proposed approach.	2015	https://doi.org/10.1145/2837185.2837210	https://doi.org/10.1145/2837185.2837210	FALSE
Mousavi, Hamid and Atzori, Maurizio and Gao, Shi and Zaniolo, Carlo	Text-Mining, Structured Queries, and Knowledge Management on Web Document Corpora	Wikipedia's InfoBoxes play a crucial role in advanced applications and provide the main knowledge source for DBpedia and the powerful structured queries it supports. However, InfoBoxes, which were created by crowdsourcing for human rather than computer consumption, suffer from incompleteness, inconsistencies, and inaccuracies. To overcome these problems, we have developed (i) the IBminer system that extracts InfoBox information by text-mining Wikipedia pages, (ii) the IKBStore system that integrates the information derived by IBminer with that of DBpedia, YAGO2,WikiData,WordNet, and other sources, and (iii) SWiPE and InfoBox Editor (IBE) that provide a user-friendly interfaces for querying and revising the knowledge base. Thus, IBminer uses a deep NLP-based approach to extract from text a semantic representation structure called TextGraph from which the system detects patterns and derives subject-attribute-value relations, as well as domain-specific synonyms for the knowledge base. IKBStore and IBE complement the powerful, user-friendly, by-example structured queries of SWiPE by supporting the validation and provenance history for the information contained in the knowledge base, along with the ability of upgrading its knowledge when this is found incomplete, incorrect, or outdated.	2014	https://doi.org/10.1145/2694428.2694437	https://doi.org/10.1145/2694428.2694437	FALSE
Ensan, Faezeh and Bagheri, Ebrahim	Document Retrieval Model Through Semantic Linking	This paper addresses the task of document retrieval based on the degree of document relatedness to the meanings of a query by presenting a semantic-enabled language model. Our model relies on the use of semantic linking systems for forming a graph representation of documents and queries, where nodes represent concepts extracted from documents and edges represent semantic relatedness between concepts. Based on this graph, our model adopts a probabilistic reasoning model for calculating the conditional probability of a query concept given values assigned to document concepts. We present an integration framework for interpolating other retrieval systems with the presented model in this paper. Our empirical experiments on a number of TREC collections show that the semantic retrieval has a synergetic impact on the results obtained through state of the art keyword-based approaches, and the consideration of semantic information obtained from entity linking on queries and documents can complement and enhance the performance of other retrieval models.	2017	https://doi.org/10.1145/3018661.3018692	https://doi.org/10.1145/3018661.3018692	FALSE
Stabauer, Martin and Grossmann, Georg and Stumptner, Markus	State of the Art in Knowledge Extraction from Online Polls: A Survey of Current Technologies	The ongoing research and development in the field of Natural Language Processing has lead to a great number of technologies in its context. There have been major benefits when it comes to bringing together the worlds of natural language and semantic technologies, so more and more potential areas of application emerge. One of these is the subject of this paper, in particular the possible ways of knowledge extraction from single-question online polls.With concepts of the Social Web, internet users want to contribute and express their opinion. As a consequence, the popularity of online polls is rapidly increasing; they can be found in news articles of media sites, on blogs etc. It would be desirable to bring intelligence to the application of polls by using technologies of the SemanticWeb and Natural Language Processing as this would allow to build a great knowledge base and to draw conclusions from it.This paper surveys the current landscape of tools and state-of-the-art technologies and analyses them with regard to pre-defined requirements that need to be accomplished, in order to be useful for extracting knowledge from the results generated by online polls.	2016	https://doi.org/10.1145/2843043.2843378	https://doi.org/10.1145/2843043.2843378	FALSE
Wang, Fang and Xie, Yongqiang and Zhang, Kai and Xia, Rui	A Joint Model of Adaptive Clustering and Multi-Kernel Learning for Entity Alignment		2021	https://doi.org/10.1145/3502300.3502313	https://doi.org/10.1145/3502300.3502313	FALSE
Kejriwal, Mayank and Szekely, Pedro	Supervised Typing of Big Graphs Using Semantic Embeddings	We propose a supervised algorithm for generating type embeddings in the same semantic vector space as a given set of entity embeddings. The algorithm is agnostic to the derivation of the underlying entity embeddings. It does not require any manual feature engineering, generalizes well to hundreds of types and achieves near-linear scaling on Big Graphs containing many millions of triples and instances by virtue of an incremental execution. We demonstrate the utility of the embeddings on a type recommendation task, outperforming a non-parametric feature-agnostic baseline while achieving 15\texttimes{} speedup and near-constant memory usage on a full partition of DBpedia. Using state-of-the-art visualization, we illustrate the agreement of our extensionally derived DBpedia type embeddings with the manually curated domain ontology. Finally, we use the embeddings to probabilistically cluster about 4 million DBpedia instances into 415 types in the DBpedia ontology.	2017	https://doi.org/10.1145/3066911.3066918	https://doi.org/10.1145/3066911.3066918	FALSE
Punjani, D. and Singh, K. and Both, A. and Koubarakis, M. and Angelidis, I. and Bereta, K. and Beris, T. and Bilidas, D. and Ioannidis, T. and Karalis, N. and Lange, C. and Pantazi, D. and Papaloukas, C. and Stamoulis, G.	Template-Based Question Answering over Linked Geospatial Data	Large amounts of geospatial data have been made available recently on the linked open data cloud and on the portals of many national cartographic agencies (e.g., OpenStreetMap data, administrative geographies of various countries, or land cover/land use data sets). These datasets use various geospatial vocabularies and can be queried using SPARQL or its OGC-standardized extension GeoSPARQL. In this paper we go beyond these approaches to offer a question answering service on top of linked geospatial data sources. Our system has been implemented as re-usable components of the Qanary question answering architecture to provide benefits for future research tasks. We give a detailed description of the architecture of the system, its underlying algorithms and its evaluation using a set of 201 natural language questions.	2018	https://doi.org/10.1145/3281354.3281362	https://doi.org/10.1145/3281354.3281362	TRUE
Tamine, Lynda and Goeuriot, Lorraine	Semantic Information Retrieval on Medical Texts: Research Challenges, Survey, and Open Issues	The explosive growth and widespread accessibility of medical information on the Internet have led to a surge of research activity in a wide range of scientific communities including health informatics and information retrieval (IR). One of the common concerns of this research, across these disciplines, is how to design either clinical decision support systems or medical search engines capable of providing adequate support for both novices (e.g., patients and their next-of-kin) and experts (e.g., physicians, clinicians) tackling complex tasks (e.g., search for diagnosis, search for a treatment). However, despite the significant multi-disciplinary research advances, current medical search systems exhibit low levels of performance. This survey provides an overview of the state of the art in the disciplines of IR and health informatics, and bridging these disciplines shows how semantic search techniques can facilitate medical IR. First,we will give a broad picture of semantic search and medical IR and then highlight the major scientific challenges. Second, focusing on the semantic gap challenge, we will discuss representative state-of-the-art work related to feature-based as well as semantic-based representation and matching models that support medical search systems. In addition to seminal works, we will present recent works that rely on research advancements in deep learning. Third, we make a thorough cross-model analysis and provide some findings and lessons learned. Finally, we discuss some open issues and possible promising directions for future research trends.	2021	https://doi.org/10.1145/3462476	https://doi.org/10.1145/3462476	FALSE
Wong, Wilson and Liu, Wei and Bennamoun, Mohammed	Ontology Learning from Text: A Look Back and into the Future	Ontologies are often viewed as the answer to the need for interoperable semantics in modern information systems. The explosion of textual information on the Read/Write Web coupled with the increasing demand for ontologies to power the Semantic Web have made (semi-)automatic ontology learning from text a very promising research area. This together with the advanced state in related areas, such as natural language processing, have fueled research into ontology learning over the past decade. This survey looks at how far we have come since the turn of the millennium and discusses the remaining challenges that will define the research directions in this area in the near future.	2012	https://doi.org/10.1145/2333112.2333115	https://doi.org/10.1145/2333112.2333115	FALSE
Costa-juss\`{a}, Marta R. and Espa\~{n}a-Bonet, Cristina and Fung, Pascale and Smith, Noah A.	Multilingual and Interlingual Semantic Representations for Natural Language Processing: A Brief Introduction	We introduce the Computational Linguistics special issue on Multilingual and Interlingual Semantic Representations for Natural Language Processing. We situate the special issue’s five articles in the context of our fast-changing field, explaining our motivation for this project. We offer a brief summary of the work in the issue, which includes developments on lexical and sentential semantic representations, from symbolic and neural perspectives.	2020	https://doi.org/10.1162/coli_a_00373	https://doi.org/10.1162/coli_a_00373	FALSE
Marx, Edgard and Usbeck, Ricardo and Ngomo, Axel-Cyrille Ngonga and H\"{o}ffner, Konrad and Lehmann, Jens and Auer, S\"{o}ren	Towards an Open Question Answering Architecture	Billions of facts pertaining to a multitude of domains are now available on the Web as RDF data. However, accessing this data is still a difficult endeavour for non-expert users. In order to meliorate the access to this data, approaches imposing minimal hurdles to their users are required. Although many question answering systems over Linked Data have being proposed, retrieving the desired data is still significantly challenging. In addition, developing and evaluating question answering systems remains a very complex task. To overcome these obstacles, we present a modular and extensible open-source question answering framework. We demonstrate how the framework can be used by integrating two state-of-the-art question answering systems. As a result our evaluation shows that overall better results can be achieved by the use of combination rather than individual stand-alone versions.	2014	https://doi.org/10.1145/2660517.2660519	https://doi.org/10.1145/2660517.2660519	FALSE
Hasibi, Faegheh and Balog, Krisztian and Bratsberg, Svein Erik	Dynamic Factual Summaries for Entity Cards	Entity cards are being used frequently in modern web search engines to offer a concise overview of an entity directly on the results page. These cards are composed of various elements, one of them being the entity summary: a selection of facts describing the entity from an underlying knowledge base. These summaries, while presenting a synopsis of the entity, can also directly address users' information needs. In this paper, we make the first effort towards generating and evaluating such factual summaries. We introduce and address the novel problem of dynamic entity summarization for entity cards, and break it down to two specific subtasks: fact ranking and summary generation. We perform an extensive evaluation of our method using crowdsourcing. Our results show the effectiveness of our fact ranking approach and validate that users prefer dynamic summaries over static ones.	2017	https://doi.org/10.1145/3077136.3080810	https://doi.org/10.1145/3077136.3080810	FALSE
Yu, Dongfei and Fu, Jianlong and Tian, Xinmei and Mei, Tao	Multi-Source Multi-Level Attention Networks for Visual Question Answering	In recent years, Visual Question Answering (VQA) has attracted increasing attention due to its requirement on cross-modal understanding and reasoning of vision and language. VQA is proposed to automatically answer natural language questions with reference to a given image. VQA is challenging, because the reasoning process on a visual domain needs a full understanding of the spatial relationship, semantic concepts, as well as the common sense for a real image. However, most existing approaches jointly embed the abstract low-level visual features and high-level question features to infer answers. These works have limited reasoning ability due to the lack of modeling of the rich spatial context of regions, high-level semantics of images, and knowledge across multiple sources. To solve the challenges, we propose multi-source multi-level attention networks for visual question answering that can benefit both spatial inferences by visual attention on context-aware region representation and reasoning by semantic attention on concepts as well as external knowledge. Indeed, we learn to reason on image representation by question-guided attention at different levels across multiple sources, including region and concept level representation from image source as well as sentence level representation from the external knowledge base. First, we encode region-based middle-level outputs from Convolutional Neural Networks (CNNs) into spatially embedded representation by a multi-directional two-dimensional recurrent neural network and, further, locate the answer-related regions by Multiple Layer Perceptron as visual attention. Second, we generate semantic concepts from high-level semantics in CNNs and select those question-related concepts as concept attention. Third, we query semantic knowledge from the general knowledge base by concepts and selected question-related knowledge as knowledge attention. Finally, we jointly optimize visual attention, concept attention, knowledge attention, and question embedding by a softmax classifier to infer the final answer. Extensive experiments show the proposed approach achieved significant improvement on two very challenging VQA datasets.	2019	https://doi.org/10.1145/3316767	https://doi.org/10.1145/3316767	FALSE
Rosales-M\'{e}ndez, Henry	Towards Better Entity Linking Evaluation	The Entity Linking (EL) task is concerned with linking entity mentions in a text collection with their corresponding knowledge-base entries. Despite the progress made in the evaluation of EL systems, there is still much work to be done, where this Ph.D. research tackles issues concerning EL evaluation. Among these issues, we stress (a)&nbsp;the lack of consensus about the definition of “entity” and the lack of evaluation metrics that allow for different notions of entities, (b)&nbsp;the lack of datasets that allow for cross-language comparison, and (c)&nbsp;the focus on evaluating high-level systems rather than low-level techniques. By addressing these challenges and better understanding the performance of EL systems, our hypothesis is that we can create a more general, more configurable EL framework that can be better adapted to the needs of a particular application. In the early stages of this PhD work, we have identified these problems and begun to address (a) and (b), publishing initial results that constitute a significant step forward in our investigation. However, there are still further challenges that must be addressed before we reach our goal. Our next steps thus involve proposing a more fluid definition of “entity” adaptable to different applications, the definition of quality measures that allow for comparing EL approaches targeting different types of entities, as well as the creation of a customizable EL framework that allows for composing and evaluating individual techniques as appropriate to a particular task.	2019	https://doi.org/10.1145/3308560.3314199	https://doi.org/10.1145/3308560.3314199	FALSE
Abdelaziz, Ibrahim and Dolby, Julian and McCusker, Jamie and Srinivas, Kavitha	A Toolkit for Generating Code Knowledge Graphs	Knowledge graphs have been proven extremely useful in powering diverse applications in semantic search and natural language understanding. In this work, we present GraphGen4Code, a toolkit to build code knowledge graphs that can similarly power various applications such as program search, code understanding, bug detection, and code automation. GraphGen4Code uses generic techniques to capture code semantics with the key nodes in the graph representing classes, functions and methods. Edges indicate function usage (e.g., how data flows through function calls, as derived from program analysis of real code), and documentation about functions (e.g., code documentation, usage documentation, or forum discussions such as StackOverflow). Our toolkit uses named graphs in RDF to model graphs per program, or can output graphs as JSON. We show the scalability of the toolkit by applying it to 1.3 million Python files drawn from GitHub, 2,300 Python modules, and 47 million forum posts. This results in an integrated code graph with over 2 billion triples. We make the toolkit to build such graphs as well as the sample extraction of the 2 billion triples graph publicly available to the community for use.	2021	https://doi.org/10.1145/3460210.3493578	https://doi.org/10.1145/3460210.3493578	FALSE
Ferro, Nicola	CLEF 15th Birthday: Past, Present, and Future	2014 marks the 15th birthday for CLEF, an evaluation campaign activity which has applied the Cranfield evaluation paradigm to the testing of multilingual and multimodal information access systems in Europe. This paper provides a summary of the motivations which led to the establishment of CLEF, and a description of how it has evolved over the years, the major achievements, and what we see as the next challenges.	2014	https://doi.org/10.1145/2701583.2701587	https://doi.org/10.1145/2701583.2701587	FALSE
Serdyukov, Pavel and Braslavski, Pavel and Kamps, Jaap	ECIR 2013: 35th European Conference on Information Retrieval		2013	https://doi.org/10.1145/2568388.2568395	https://doi.org/10.1145/2568388.2568395	FALSE
Wood, Peter T.	Query Languages for Graph Databases	Query languages for graph databases started to be investigated some 25 years ago. With much current data, such as linked data on the Web and social network data, being graph-structured, there has been a recent resurgence in interest in graph query languages. We provide a brief survey of many of the graph query languages that have been proposed, focussing on the core functionality provided in these languages. We also consider issues such as expressive power and the computational complexity of query evaluation.	2012	https://doi.org/10.1145/2206869.2206879	https://doi.org/10.1145/2206869.2206879	FALSE
Ali, Taqdir and Hussain, Maqbool and Khan, Wajahat Ali and Afzal, Muhammad and Lee, Sungyoung	Customized Clinical Domain Ontology Extraction for Knowledge Authoring Tool	Clinical Decision Support Systems (CDSS) require a shareable and adaptable knowledge base. However, sharing and reusing the expert's knowledge is a challenging task. The proposed approach designs a web based application that acquires and adapts the clinical expert's knowledge into shareable knowledge base. The system, Intelligent Knowledge Authoring Tool (I-KAT) creates rules in the form of Medical Logic Module (MLM) using HL7 standard Arden Syntax. These rules are easily shareable with HL7 complaint clinical institutions and organizations. To achieve interoperability using MLM, the system uses a mechanism for integration of terminology standard (SNOMED CT) concepts with CDSS standard (Virtual Medical Record (vMR)). The SNOMED CT ontology is comprehensive; containing more than 0.3 million concepts but 10--15% concepts of total ontology is normally used in rule creation for a specific domain. Semantically defining relationships between SNOMED CT concepts and vMR concepts require domain ontology development from the SNOMED ontology. In this paper we focus on automatic extraction of domain ontology from overall SNOMED CT ontology on the basis of vMR schema concepts and their attributes mapping with corresponding SNOMED CT concepts. The extracted domain ontology will increase the efficiency and effectiveness of searching mechanism in contextual selection process.	2014	https://doi.org/10.1145/2557977.2558063	https://doi.org/10.1145/2557977.2558063	FALSE
Barcel\'{o} Baeza, Pablo	Querying Graph Databases	Graph databases have gained renewed interest in the last years, due to its applications in areas such as the Semantic Web and Social Networks Analysis. We study the problem of querying graph databases, and, in particular, the expressiveness and complexity of evaluation for several general-purpose query languages, such as the regular path queries and its extensions with conjunctions and inverses. We distinguish between two semantics for these languages. The first one, based on simple paths, easily leads to intractability, while the second one, based on arbitrary paths, allows tractable evaluation for an expressive family of languages.We also study two recent extensions of these languages that have been motivated by modern applications of graph databases. The first one allows to treat paths as first-class citizens, while the second one permits to express queries that combine the topology of the graph with its underlying data.	2013	https://doi.org/10.1145/2463664.2465216	https://doi.org/10.1145/2463664.2465216	FALSE
Meditskos, Georgios and Dasiopoulou, Stamatia and Vrochidis, Stefanos and Wanner, Leo and Kompatsiaris, Ioannis	Question Answering over Pattern-Based User Models	In this paper we present an ontology-driven framework for natural language question analysis and answering over user models (e.g. preferences, habits and health problems of individuals) that are formally captured using ontology design patterns. Pattern-based modelling is extremely useful for capturing n-ary relations in a well-defined and axiomatised manner, but it introduces additional challenges in building NL interfaces for accessing the underlying content. This is mainly due to the encapsulation of domain semantics inside conceptual layers of abstraction (e.g. using reification or container classes) that demand flexible, context-aware approaches for query analysis and interpretation. We describe the coupling of a frame-based formalisation of natural language user utterances with a context-aware query interpretation towards question answering over pattern-based RDF knowledge bases. The proposed framework is part of a human-like socially communicative agent that acts as an intermediate between elderly migrants and care personnel, assisting the latter to solicit personal information about care recipients (e.g. medical history, care needs, preferences, routines, habits, etc.).	2016	https://doi.org/10.1145/2993318.2993331	https://doi.org/10.1145/2993318.2993331	FALSE
Looser, Dominic and Ma, Hui and Schewe, Klaus-Dieter	Using Formal Concept Analysis for Ontology Maintenance in Human Resource Recruitment	Ontologies have been proven useful for many applications by enabling semantic search and reasoning. Human resource management has recently attracted interest by researchers and practitioners seeking to exploit ontologies for improving the efficiency and effectiveness of the job recruitment process. However, the quality of semantic search and decision making intimately depends on the quality of the ontology used. Most current efforts concentrate on the development of general ontologies that find wide approval by the HR community worldwide. In order to be useful for automatic matchmaking between job offers and job seekers, such high-level ontologies need to be adequately enriched with detailed domain-specific knowledge and adapted to the particular needs of individual job markets. We present an approach for enriching and adapting an existing ontology using formal concept analysis.	2013			FALSE
Van Laere, Olivier and Bordino, Ilaria and Mejova, Yelena and Lalmas, Mounia	DEESSE: Entity-Driven Exploratory and SErendipitous Search SystEm	We present DEESSE [1], a tool that enables an exploratory and serendipitous exploration - at entity level, of the content of two different social media: Wikipedia, a user-curated online encyclopedia, and Yahoo Answers, a more unconstrained question/answering forum. DEESSE represents the content of each source as an entity network, which is further enriched with metadata about sentiment, writing quality, and topical category. Given a query entity, entity results are retrieved from the network by employing an algorithm based on a random walk with restart to the query. Following the emerging paradigm of composite retrieval, we organize the results into topically coherent bundles instead of showing them in a simple ranked list.	2014	https://doi.org/10.1145/2661829.2661853	https://doi.org/10.1145/2661829.2661853	FALSE
Maree, Mohammed and Rattrout, Amjad and Altawil, Muhanad and Belkhatir, Mohammed	Multi-Modality Search and Recommendation on Palestinian Cultural Heritage Based on the Holy-Land Ontology and Extrinsic Semantic Resources	The Cultural Heritage (CH) sector and its associated tourism services have been affected notably by the advancement of the Internet as well as the explosive growth of smartphones and other handheld devices. These days, visitors can access reliable CH content using Web and mobile-based interfaces. However, conventional CH systems still lack the ability to provide meaningful semantically overt results that precisely meet user information needs in this domain. In addition, they often ignore the user search context and experience, which hinders their ability to adapt their behavior to the preferences, tasks, interests, and other user functionalities. In this article, we aim to address the issue of designing a precision-oriented multilingual and multi-criteria semantic-based mobile recommender system specifically targeting Palestine's CH, a country with great historical and cultural importance. We aim to better facilitate users’ access to CH content by providing them with multiple search functionalities. In this context, a user can search for relevant information using keywords (a.k.a. tags) or sentence-like queries and the system retrieves all relevant documents based on their semantic similarity. A second option is to search using current location information to retrieve correlated historical places and events. Finally, starting from a picture of interest, a third option makes it possible to extract captions describing its content that can be used to search for additional contextually relevant information. Additionally, the proposed system aims at personalizing users’ experience through progressively delivering output that meets their information needs based on a number of parameters such as users' logging data, interests, previous searches, and location-based information. A prototype of the proposed system has been developed and tested using Android smartphones and a manually constructed ontology enriched with CH links to the Art &amp; Architecture Thesaurus (AAT) and DBpedia. By comparing our system with similar systems in this domain, findings demonstrate that it provides additional search features and functionalities to users. The proposed Holy-Land ontology is the first of its kind attempting to encode knowledge about Palestine's CH. It plays a crucial role in our proposal, serving as a pivotal entity in the combination of language-based, location-based, and visual-based retrieval strategies.	2021	https://doi.org/10.1145/3447523	https://doi.org/10.1145/3447523	TRUE
	WWW '19: Companion Proceedings of The 2019 World Wide Web Conference	It is our great pleasure to welcome you to <i>The Web Conference 2019</i>. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.	2019			FALSE
Gupta, Deepak and Ekbal, Asif and Bhattacharyya, Pushpak	A Deep Neural Network Framework for English Hindi Question Answering	In this article, we propose a unified deep neural network framework for multilingual question answering (QA). The proposed network deals with the multilingual questions and answers snippets. The input to the network is a pair of factoid question and snippet in the multilingual environment (English and Hindi), and output is the relevant answer from the snippet. We begin by generating the snippet using a graph-based language-independent algorithm, which exploits the lexico-semantic similarity between the sentences. The soft alignment of the question words from the English and Hindi languages has been used to learn the shared representation of the question. The learned shared representation of question and attention-based snippet representation are passed as an input to the answer extraction layer of the network, which extracts the answer span from the snippet. Evaluation on a standard multilingual QA dataset shows the state-of-the-art performance with 39.44 Exact Match (EM) and 44.97 F1 values. Similarly, we achieve the performance of 50.11 Exact Match (EM) and 53.77 F1 values on Translated SQuAD dataset.	2019	https://doi.org/10.1145/3359988	https://doi.org/10.1145/3359988	FALSE
Zhang, Ruqing and Guo, Jiafeng and Chen, Lu and Fan, Yixing and Cheng, Xueqi	A Review on Question Generation from Natural Language Text	Question generation is an important yet challenging problem in Artificial Intelligence (AI), which aims to generate natural and relevant questions from various input formats, e.g., natural language text, structure database, knowledge base, and image. In this article, we focus on question generation from natural language text, which has received tremendous interest in recent years due to the widespread applications such as data augmentation for question answering systems. During the past decades, many different question generation models have been proposed, from traditional rule-based methods to advanced neural network-based methods. Since there have been a large variety of research works proposed, we believe it is the right time to summarize the current status, learn from existing methodologies, and gain some insights for future development. In contrast to existing reviews, in this survey, we try to provide a more comprehensive taxonomy of question generation tasks from three different perspectives, i.e., the types of the input context text, the target answer, and the generated question. We take a deep look into existing models from different dimensions to analyze their underlying ideas, major design principles, and training strategies We compare these models through benchmark tasks to obtain an empirical understanding of the existing techniques. Moreover, we discuss what is missing in the current literature and what are the promising and desired future directions.	2021	https://doi.org/10.1145/3468889	https://doi.org/10.1145/3468889	FALSE
de A. J\'{u}nior, Jos\'{e} Gildo and Schiel, Ulrich and Marinho, Leandro Balby	An Approach for Building Lexical-Semantic Resources Based on Heterogeneous Information Sources	Lexical-semantic resources (LSRs) have an important role in many information retrieval and extraction tasks. However, in order to be effective, LSRs need to cover a broad spectrum of knowledge facets about terms, e.g., encyclopedic, linguistic, and common sense. These different knowledge facets are usually found in different and heterogeneous knowledge sources spread over the Web, turning the task of integrating them into an unified LSR a hard one. In this work, we propose a new approach to automatically build LSRs that are tailored to semantic search engines, i.e., our approach builds LSRs that favor disambiguation and faceted search. Moreover, while most of the related work is limited to using encyclopedic sources only, such as Wikipedia, we instantiate our approach using additional and heterogeneous knowledge sources, such as dictionaries (WordNet), common sense (Open Mind Common Sense), and semantic networks (Conceptnet5). For evaluation, we compare our approach with UBY, a open source and state-of-the-art LSR, in terms of the number of terms covered; the degree of semantic connections established for each term; and the strength of the extracted semantic relations between terms. We conducted experiments in several test sets and show that our approach is superior under the aforementioned aspects.	2015	https://doi.org/10.1145/2695664.2695896	https://doi.org/10.1145/2695664.2695896	FALSE
Dochev, Danail and Agre, Gennady and Pavlov, Radoslav	User Authoring in Learning-by-Doing Situations	The paper deals with learning-by-doing activities through learner's authoring of analytical materials in well-defined learning situations, considering some specifics of education in humanities. These `learning-by-authoring' activities are facilitated by Semantic Web techniques to support the learners in the access and filtration of necessary information objects to be analysed during the authoring process. The paper discusses a framework for Technology Enhanced Learning environment under experimental realisation for a concrete humanitarian domain - Bulgarian Iconography covered in a set of educational disciplines like iconography, arts, history, culture studies, theology, etc. The paper contains an example with structured formulation of a concrete learning task, used in the environment to help in the execution of the steps of collection development and in the evaluation of the adequacy of the selected representative subset of objects.	2011	https://doi.org/10.1145/2023607.2023703	https://doi.org/10.1145/2023607.2023703	FALSE
Metzler, Donald and Tay, Yi and Bahri, Dara and Najork, Marc	Rethinking Search: Making Domain Experts out of Dilettantes	When experiencing an information need, users want to engage with a domain expert, but often turn to an information retrieval system, such as a search engine, instead. Classical information retrieval systems do not answer information needs directly, but instead provide references to (hopefully authoritative) answers. Successful question answering systems offer a limited corpus created on-demand by human experts, which is neither timely nor scalable. Pre-trained language models, by contrast, are capable of directly generating prose that may be responsive to an information need, but at present they are dilettantes rather than domain experts - they do not have a true understanding of the world, they are prone to hallucinating, and crucially they are incapable of justifying their utterances by referring to supporting documents in the corpus they were trained over. This paper examines how ideas from classical information retrieval and pre-trained language models can be synthesized and evolved into systems that truly deliver on the promise of domain expert advice.	2021	https://doi.org/10.1145/3476415.3476428	https://doi.org/10.1145/3476415.3476428	FALSE
Nasar, Zara and Jaffry, Syed Waqar and Malik, Muhammad Kamran	Named Entity Recognition and Relation Extraction: State-of-the-Art	With the advent of Web 2.0, there exist many online platforms that result in massive textual-data production. With ever-increasing textual data at hand, it is of immense importance to extract information nuggets from this data. One approach towards effective harnessing of this unstructured textual data could be its transformation into structured text. Hence, this study aims to present an overview of approaches that can be applied to extract key insights from textual data in a structured way. For this, Named Entity Recognition and Relation Extraction are being majorly addressed in this review study. The former deals with identification of named entities, and the latter deals with problem of extracting relation between set of entities. This study covers early approaches as well as the developments made up till now using machine learning models. Survey findings conclude that deep-learning-based hybrid and joint models are currently governing the state-of-the-art. It is also observed that annotated benchmark datasets for various textual-data generators such as Twitter and other social forums are not available. This scarcity of dataset has resulted into relatively less progress in these domains. Additionally, the majority of the state-of-the-art techniques are offline and computationally expensive. Last, with increasing focus on deep-learning frameworks, there is need to understand and explain the under-going processes in deep architectures.	2021	https://doi.org/10.1145/3445965	https://doi.org/10.1145/3445965	FALSE
Singla, Krishma and Dua, Mohit and Nanda, Garima	A Language Based Comparison of Different Similarity Functions and Classifiers Using Web Based Bilingual Question Answering System Developed Using Machine Learning Approach	Contemporary information techniques and services offered by the Internet are going through the dilemma of determining and managing an increasing amount of textual information, to which ingress is often difficult. But recently Machine Learning approaches have shown their outstanding performance and elasticity in many applications such as Artificial Intelligence and Pattern Recognition. Question Answering (QA) System is an Information Retrieval system in which the expected response given is directly the answer as requested by the user instead of list of references which have some probability of being the answer. The main intention of this research is to present the knowledge and fetch the answer for a given query by employing machine learning approach. The query will be matched to the knowledge database by computing their similarity. The stated research portrays the Web Based Bilingual Question Answering system constituting of Hindi and English language employing by machine learning approach.	2016	https://doi.org/10.1145/2905055.2905336	https://doi.org/10.1145/2905055.2905336	FALSE
Sun, Xiaolei and Zhang, Yu	A Review of Domain Knowledge Representation for Robot Task Planning	The review analyzes and defines the related key concepts firstly. Then the article reviews the knowledge representation (KR) formalism and the existing knowledge systems in robotics. The review divides KR in robotics into three categories, namely semantic networks, rules and logical knowledge representation. Then the existing knowledge systems are classified according to the three categories. Finally the article introduces three typical knowledge representation languages in the application of robot task planning and the corresponding task planners. It is still a disadvantage that the domain knowledge of the existing task planners is generated manually, so that the article introduces the method of machine learning to generate the domain knowledge automatically. Our contributions are to facilitate the selection of a knowledge system according to the requirements of robotic application and predict the development trend, hoping to inspire and promote the research in autonomous robotics.	2019	https://doi.org/10.1145/3325730.3325756	https://doi.org/10.1145/3325730.3325756	FALSE
Sun, Zequn and Zhang, Qingheng and Hu, Wei and Wang, Chengming and Chen, Muhao and Akrami, Farahnaz and Li, Chengkai	A Benchmarking Study of Embedding-Based Entity Alignment for Knowledge Graphs	Entity alignment seeks to find entities in different knowledge graphs (KGs) that refer to the same real-world object. Recent advancement in KG embedding impels the advent of embedding-based entity alignment, which encodes entities in a continuous embedding space and measures entity similarities based on the learned embeddings. In this paper, we conduct a comprehensive experimental study of this emerging field. We survey 23 recent embedding-based entity alignment approaches and categorize them based on their techniques and characteristics. We also propose a new KG sampling algorithm, with which we generate a set of dedicated benchmark datasets with various heterogeneity and distributions for a realistic evaluation. We develop an open-source library including 12 representative embedding-based entity alignment approaches, and extensively evaluate these approaches, to understand their strengths and limitations. Additionally, for several directions that have not been explored in current approaches, we perform exploratory experiments and report our preliminary findings for future studies. The benchmark datasets, open-source library and experimental results are all accessible online and will be duly maintained.	2020	https://doi.org/10.14778/3407790.3407828	https://doi.org/10.14778/3407790.3407828	FALSE
Agrawal, Parag and Menon, Tulasi and Kam, Aya and Naim, Michel and Chouragade, Chaikesh and Singh, Gurvinder and Kulkarni, Rohan and Suri, Anshuman and Katakam, Sahithi and Pratik, Vineet and Bansal, Prakul and Kaur, Simerpreet and Duggal, Anand and Chalabi, Achraf and Choudhari, Prashant and Satti, Somi Reddy and Nayak, Niranjan and Rajput, Neha	QnAMaker: Data to Bot in 2 Minutes	Having a bot for seamless conversations is a much-desired feature that products and services today seek for their websites and mobile apps. These bots help reduce traffic received by human support significantly by handling frequent and directly answerable known questions. Many such services have huge reference documents such as FAQ pages, which makes it hard for users to browse through this data. A conversation layer over such raw data can lower traffic to human support by a great margin. We demonstrate QnAMaker, a service that creates a conversational layer over semi-structured data such as FAQ pages, product manuals, and support documents. QnAMaker is the popular choice for Extraction and Question-Answering as a service and is used by over 15,000 bots in production. It is also used by search interfaces and not just bots. 	2020		https://doi.org/10.1145/3366424.3383525	FALSE
Agosti, Maristella and Braschler, Martin and Choukri, Khalid and Ferro, Nicola and Harman, Donna and Peters, Carol and Pianta, Emanuele and de Rijke, Maarten and Smeaton, Alan	CLEF 2010 Conference on Multilingual and Multimodal Information Access Evaluation		2011	https://doi.org/10.1145/1924475.1924477	https://doi.org/10.1145/1924475.1924477	FALSE
	Basic OWL	Enterprises have made amazing advances by taking advantage of data about their business to provide predictions and understanding of their customers, markets, and products. But as the world of business becomes more interconnected and global, enterprise data is no long a monolith; it is just a part of a vast web of data. Managing data on a world-wide scale is a key capability for any business today.The Semantic Web treats data as a distributed resource on the scale of the World Wide Web, and incorporates features to address the challenges of massive data distribution as part of its basic design. The aim of the first two editions was to motivate the Semantic Web technology stack from end-to-end; to describe not only what the Semantic Web standards are and how they work, but also what their goals are and why they were designed as they are. It tells a coherent story from beginning to end of how the standards work to manage a world-wide distributed web of knowledge in a meaningful way.The third edition builds on this foundation to bring Semantic Web practice to enterprise. Fabien Gandon joins Dean Allemang and Jim Hendler, bringing with him years of experience in global linked data, to open up the story to a modern view of global linked data. While the overall story is the same, the examples have been brought up to date and applied in a modern setting, where enterprise and global data come together as a living, linked network of data. Also included with the third edition, all of the data sets and queries are available online for study and experimentation at data.world/swwo.	2020		https://doi.org/10.1145/3382097.3382110	FALSE
Raj, Abhinav	Word Level Language Identification and Back-Transliteration	In this paper, I describe a Rule based and List-Searching system for Word-Level Language Identification and Named Entity recognition in bilingual text. My method uses dictionary search, rules for LI and CRF++, character n-gram for NER. The model does not use any Language specific rules, therefore can easily be replicated on most languages having mixed pair with English. The model also does back-transliteration of words into native language script and recognizes named entity. The model performance is carried on the test sets provided by the shared task on language Identification for English Hindi (En-Hi) Pair, Microsoft Research India. The experimental results show a consistent performance with high precision.	2014	https://doi.org/10.1145/2824864.2824884	https://doi.org/10.1145/2824864.2824884	FALSE
Yang, Di and Hussain, Aftab and Lopes, Cristina Videira	From Query to Usable Code: An Analysis of Stack Overflow Code Snippets	Enriched by natural language texts, Stack Overflow code snippets are an invaluable code-centric knowledge base of small units of source code. Besides being useful for software developers, these annotated snippets can potentially serve as the basis for automated tools that provide working code solutions to specific natural language queries.With the goal of developing automated tools with the Stack Overflow snippets and surrounding text, this paper investigates the following questions: (1) How usable are the Stack Overflow code snippets? and (2) When using text search engines for matching on the natural language questions and answers around the snippets, what percentage of the top results contain usable code snippets?A total of 3M code snippets are analyzed across four languages: C#, Java, JavaScript, and Python. Python and JavaScript proved to be the languages for which the most code snippets are usable. Conversely, Java and C# proved to be the languages with the lowest usability rate. Further qualitative analysis on usable Python snippets shows the characteristics of the answers that solve the original question. Finally, we use Google search to investigate the alignment of usability and the natural language annotations around code snippets, and explore how to make snippets in Stack Overflow an adequate base for future automatic program generation.	2016	https://doi.org/10.1145/2901739.2901767	https://doi.org/10.1145/2901739.2901767	FALSE
Dong, Li	Learning Natural Language Interfaces with Neural Models	Language is the primary and most natural means of communication for humans. The learning curve of interacting with various services (e.g., digital assistants, and smart appliances) would be greatly reduced if we could talk to machines using human language. However, in most cases computers can only interpret and execute formal languages.	2021	https://doi.org/10.1145/3478369.3478375	https://doi.org/10.1145/3478369.3478375	FALSE
Yan, Rui and Song, Yiping and Wu, Hua	Learning to Respond with Deep Neural Networks for Retrieval-Based Human-Computer Conversation System	To establish an automatic conversation system between humans and computers is regarded as one of the most hardcore problems in computer science, which involves interdisciplinary techniques in information retrieval, natural language processing, artificial intelligence, etc. The challenges lie in how to respond so as to maintain a relevant and continuous conversation with humans. Along with the prosperity of Web 2.0, we are now able to collect extremely massive conversational data, which are publicly available. It casts a great opportunity to launch automatic conversation systems. Owing to the diversity of Web resources, a retrieval-based conversation system will be able to find at least some responses from the massive repository for any user inputs. Given a human issued message, i.e., query, our system would provide a reply after adequate training and learning of how to respond. In this paper, we propose a retrieval-based conversation system with the deep learning-to-respond schema through a deep neural network framework driven by web data. The proposed model is general and unified for different conversation scenarios in open domain. We incorporate the impact of multiple data inputs, and formulate various features and factors with optimization into the deep learning framework. In the experiments, we investigate the effectiveness of the proposed deep neural network structures with better combinations of all different evidence. We demonstrate significant performance improvement against a series of standard and state-of-art baselines in terms of p@1, MAP, nDCG, and MRR for conversational purposes.	2016	https://doi.org/10.1145/2911451.2911542	https://doi.org/10.1145/2911451.2911542	FALSE
Zhao, Xinyan and Xiao, Feng and Zhong, Haoming and Yao, Jun and Chen, Huanhuan	Condition Aware and Revise Transformer for Question Answering	The study of question answering has received increasing attention in recent years. This work focuses on providing an answer that compatible with both user intent and conditioning information corresponding to the question, such as delivery status and stock information in e-commerce. However, these conditions may be wrong or incomplete in real-world applications. Although existing question answering systems have considered the external information, such as categorical attributes and triples in knowledge base, they all assume that the external information is correct and complete. To alleviate the effect of defective condition values, this paper proposes condition aware and revise Transformer (CAR-Transformer). CAR-Transformer (1) revises each condition value based on the whole conversation and original conditions values, and (2) it encodes the revised conditions and utilizes the conditions embedding to select an answer. Experimental results on a real-world customer service dataset demonstrate that the CAR-Transformer can still select an appropriate reply when conditions corresponding to the question exist wrong or missing values, and substantially outperforms baseline models on automatic and human evaluations. The proposed CAR-Transformer can be extended to other NLP tasks which need to consider conditioning information.	2020		https://doi.org/10.1145/3366423.3380301	FALSE
Wang, Hua and Nie, Feiping and Huang, Heng	Large-Scale Cross-Language Web Page Classification via Dual Knowledge Transfer Using Fast Nonnegative Matrix Trifactorization	With the rapid growth of modern technologies, Internet has reached almost every corner of the world. As a result, it becomes more and more important to manage and mine information contained in Web pages in different languages. Traditional supervised learning methods usually require a large amount of training data to obtain accurate and robust classification models. However, labeled Web pages did not increase as fast as the growth of Internet. The lack of sufficient training Web pages in many languages, especially for those in uncommonly used languages, makes it a challenge for traditional classification algorithms to achieve satisfactory performance. To address this, we observe that Web pages for a same topic from different languages usually share some common semantic patterns, though in different representation forms. In addition, we also observe that the associations between word clusters and Web page classes are another type of reliable carriers to transfer knowledge across languages. With these recognitions, in this article we propose a novel joint nonnegative matrix trifactorization (NMTF) based Dual Knowledge Transfer (DKT) approach for cross-language Web page classification. Our approach transfers knowledge from the auxiliary language, in which abundant labeled Web pages are available, to the target languages, in which we want to classify Web pages, through two different paths: word cluster approximation and the associations between word clusters and Web page classes. With the reinforcement between these two different knowledge transfer paths, our approach can achieve better classification accuracy. In order to deal with the large-scale real world data, we further develop the proposed DKT approach by constraining the factor matrices of NMTF to be cluster indicator matrices. Due to the nature of cluster indicator matrices, we can decouple the proposed optimization objective and the resulted subproblems are of much smaller sizes involving much less matrix multiplications, which make our new approach much more computationally efficient. We evaluate the proposed approach in extensive experiments using a real world cross-language Web page data set. Promising results have demonstrated the effectiveness of our approach that are consistent with our theoretical analyses.	2015	https://doi.org/10.1145/2710021	https://doi.org/10.1145/2710021	FALSE
Sheth, Amit and Perera, Sujan and Wijeratne, Sanjaya and Thirunarayan, Krishnaprasad	Knowledge Will Propel Machine Understanding of Content: Extrapolating from Current Examples	Machine Learning has been a big success story during the AI resurgence. One particular stand out success relates to learning from a massive amount of data. In spite of early assertions of the unreasonable effectiveness of data, there is increasing recognition for utilizing knowledge whenever it is available or can be created purposefully. In this paper, we discuss the indispensable role of knowledge for deeper understanding of content where (i) large amounts of training data are unavailable, (ii) the objects to be recognized are complex, (e.g., implicit entities and highly subjective content), and (iii) applications need to use complementary or related data in multiple modalities/media. What brings us to the cusp of rapid progress is our ability to (a) create relevant and reliable knowledge and (b) carefully exploit knowledge to enhance ML/NLP techniques. Using diverse examples, we seek to foretell unprecedented progress in our ability for deeper understanding and exploitation of multimodal data and continued incorporation of knowledge in learning techniques.	2017	https://doi.org/10.1145/3106426.3109448	https://doi.org/10.1145/3106426.3109448	FALSE
Minaee, Shervin and Kalchbrenner, Nal and Cambria, Erik and Nikzad, Narjes and Chenaghlu, Meysam and Gao, Jianfeng	Deep Learning--Based Text Classification: A Comprehensive Review	Deep learning--based models have surpassed classical machine learning--based approaches in various text classification tasks, including sentiment analysis, news categorization, question answering, and natural language inference. In this article, we provide a comprehensive review of more than 150 deep learning--based models for text classification developed in recent years, and we discuss their technical contributions, similarities, and strengths. We also provide a summary of more than 40 popular datasets widely used for text classification. Finally, we provide a quantitative analysis of the performance of different deep learning models on popular benchmarks, and we discuss future research directions.	2021	https://doi.org/10.1145/3439726	https://doi.org/10.1145/3439726	FALSE
Chandrasekaran, Dhivya and Mago, Vijay	Evolution of Semantic Similarity—A Survey	Estimating the semantic similarity between text data is one of the challenging and open research problems in the field of Natural Language Processing (NLP). The versatility of natural language makes it difficult to define rule-based methods for determining semantic similarity measures. To address this issue, various semantic similarity methods have been proposed over the years. This survey article traces the evolution of such methods beginning from traditional NLP techniques such as kernel-based methods to the most recent research work on transformer-based models, categorizing them based on their underlying principles as knowledge-based, corpus-based, deep neural network–based methods, and hybrid methods. Discussing the strengths and weaknesses of each method, this survey provides a comprehensive view of existing systems in place for new researchers to experiment and develop innovative ideas to address the issue of semantic similarity.	2021	https://doi.org/10.1145/3440755	https://doi.org/10.1145/3440755	FALSE
Kato, Makoto P. and Liu, Yiqun and Kando, Noriko and Clarke, Charles L. A.	Report on the 15th Round of NII Testbeds and Community for Information Access Research (NTCIR-15)	This is a report on the NTCIR-15 conference held online in December 2020. NTCIR is a sesquiannual research project designed to evaluate various information access technologies, including information retrieval, information recommendation, question answering, natural language processing, etc. 55 active research groups from 22 countriesregions have participated in one or more of the seven tasks in NTCIR-15. This report introduces the highlights of the conference, describes the scope and task designs of the seven tasks organized at NTCIR-15.Date: 8--11 December, 2020.Website: http://research.nii.ac.jp/ntcir/ntcir-15/.	2022	https://doi.org/10.1145/3527546.3527570	https://doi.org/10.1145/3527546.3527570	FALSE
Feng, Xiaomei and Liu, Qingtang and Lao, Chuanyuan and Sun, Dinghui	Design and Implementation of Automatic Question Answering System in Information Retrieval	Q &amp; A is an indispensable part in the teaching process. Developing a dialogue-based automated question answering system can answer students' questions accurately and assist the effective teaching of the course. Information retrieval has become one of the core courses in information technology education in major universities in China. However, there are still some problems in the current information retrieval course. This study takes the course of information retrieval as an example to design and implement a QA system based on Lucene. This system mainly includes four core function modules of data set construction, keyword extraction, problem retrieval and answer extraction. The workflow of the system is as follows: First, the system segments the textual content entered by the user into words and extracts the keywords. Then it would retrieve the keywords in the FAQ. Later, the system would extract the matching question and corresponding answer from the retrieved questions, and finally return the answer to the user. The development of QA system is based on the B/S architecture, which is finally presented to the user in the form of web pages. At the same time, the responsive design allows the system to render well in multiple terminals. Finally, the function test and performance test of the system are also carried out. The result shows that after the user enters the question, the system can return the relevant answer, and the accuracy rate is 0.84, which indicates that the automatic question answering system can basically meet the needs of users in both function and performance.	2018	https://doi.org/10.1145/3208854.3208862	https://doi.org/10.1145/3208854.3208862	FALSE
Suchanek, Fabian M. and Weikum, Gerhard	Knowledge Bases in the Age of Big Data Analytics	This tutorial gives an overview on state-of-the-art methods for the automatic construction of large knowledge bases and harnessing them for data and text analytics. It covers both big-data methods for building knowledge bases and knowledge bases being assets for big-data applications. The tutorial also points out challenges and research opportunities.	2014	https://doi.org/10.14778/2733004.2733069	https://doi.org/10.14778/2733004.2733069	FALSE
Chapman, Adriane and Simperl, Elena and Koesten, Laura and Konstantinidis, George and Ib\'{a}\~{n}ez, Luis-Daniel and Kacprzak, Emilia and Groth, Paul	Dataset Search: A Survey	Generating value from data requires the ability to find, access and make sense of datasets. There are many efforts underway to encourage data sharing and reuse, from scientific publishers asking authors to submit data alongside manuscripts to data marketplaces, open data portals and data communities. Google recently beta-released a search service for datasets, which allows users to discover data stored in various online repositories via keyword queries. These developments foreshadow an emerging research field around dataset search or retrieval that broadly encompasses frameworks, methods and tools that help match a user data need against a collection of datasets. Here, we survey the state of the art of research and commercial systems and discuss what makes dataset search a field in its own right, with unique challenges and open questions. We look at approaches and implementations from related areas dataset search is drawing upon, including information retrieval, databases, entity-centric and tabular search in order to identify possible paths to tackle these questions as well as immediate next steps that will take the field forward.	2020	https://doi.org/10.1007/s00778-019-00564-x	https://doi.org/10.1007/s00778-019-00564-x	FALSE
Raghavi, Khyathi Chandu and Chinnakotla, Manoj Kumar and Shrivastava, Manish	"Answer Ka Type Kya He?": Learning to Classify Questions in Code-Mixed Language	Code-Mixing (CM) is defined as the embedding of linguistic units such as phrases, words, and morphemes of one language into an utterance of another language. CM is a natural phenomenon observed in many multilingual societies. It helps in speeding-up communication and allows wider variety of expression due to which it has become a popular mode of communication in social media forums like Facebook and Twitter. However, current Question Answering (QA) research and systems only support expressing a question in a single language which is an unrealistic and hard proposition especially for certain domains like health and technology. In this paper, we take the first step towards the development of a full-fledged QA system in CM language which is building a Question Classification (QC) system. The QC system analyzes the user question and infers the expected Answer Type (AType). The AType helps in locating and verifying the answer as it imposes certain type-specific constraints. In this paper, we present our initial efforts towards building a full-fledged QA system for CM language. We learn a basic Support Vector Machine (SVM) based QC system for English-Hindi CM questions. Due to the inherent complexities involved in processing CM language and also the unavailability of language processing resources such POS taggers, Chunkers, Parsers, we design our current system using only word-level resources such as language identification, transliteration and lexical translation. To reduce data sparsity and leverage resources available in a resource-rich language, in stead of extracting features directly from the original CM words, we translate them commonly into English and then perform featurization. We created an evaluation dataset for this task and our system achieves an accuracy of 63% and 45% in coarse-grained and fine-grained categories of the question taxanomy. The idea of translating features into English indeed helps in improving accuracy over the unigram baseline.	2015	https://doi.org/10.1145/2740908.2743006	https://doi.org/10.1145/2740908.2743006	FALSE
Madkour, Amgad and Aref, Walid G. and Mokbel, Mohamed and Basalamah, Saleh	Geo-Tagging Non-Spatial Concepts	Concept Geo-tagging is the process of assigning a textual identifier that describes a real-world entity to a physical geographic location. A concept can either be a spatial concept where it possesses a spatial presence or be a non-spatial concept where it has no explicit spatial presence. Geo-tagging locations with non-spatial concepts that have no direct relation is a very useful and important operation but is also very challenging. The reason is that, being a non-spatial concept, e.g., crime, makes it hard to geo-tag it. This paper proposes using the semantic information associated with concepts and locations such as the type as a mean for identifying these relations. The co-occurrence of spatial and non-spatial concepts within the same textual resources, e.g., in the web, can be an indicator of a relationship between these spatial and non-spatial concepts. Techniques are presented for learning and modeling relations among spatial and non-spatial concepts from web textual resources. Co-occurring concepts are extracted and modeled as a graph of relations. This graph is used to infer the location types related to a concept. A location type can be a hospital, restaurant, an educational facility and so forth. Due to the immense number of relations that are generated from the extraction process, a semantically-guided query processing algorithm is introduced to prune the graph to the most relevant set of related concepts. For each concept, a set of most relevant types are matched against the location types. Experiments evaluate the proposed algorithm based on its filtering efficiency and the relevance of the discovered relationships. Performance results illustrate how semantically-guided query processing can outperform the baseline in terms of efficiency and relevancy. The proposed approach achieves an average precision of 74% across three different datasets.	2015	https://doi.org/10.1145/2834126.2834138	https://doi.org/10.1145/2834126.2834138	FALSE
Kaschesky, Michael and Gschwend, Adrian and Bouchard, Guillaume and Furrer, Patrick and Gamard, Stephane and Riedl, Reinhard	Aid to Regional Development Agencies: Finding and Matching Research Funding Opportunities	Regional development agencies are confronted with a plethora of research funding programs that provide opportunities for regional research groups and SMEs. This paper describes the practical benefits and technological building blocks of an online matching service of research profiles with funding opportunities. It gives an overview of the infrastructure for data sourcing and processing based on a scalable cloud computing platform. It then describes how data is consolidated, analyzed, and interlinked so as to facilitate the finding and matching of funding opportunities. Integrating concepts and themes with those available as Linked Open Data (LOD) adds value to finding and matching by interlinking results with publicly available data resources, and to improve search performance for related content on the internet. In order to optimize matching, the paper describes the user-profiling capabilities based on statistical analyses and machine learning. The paper concludes by discussing the lessons learned so far as well as possible extensions into other application areas.	2012	https://doi.org/10.1145/2307729.2307732	https://doi.org/10.1145/2307729.2307732	FALSE
Soffer, Aya and Konopnicki, David and Roitman, Haggai	When Watson Went to Work: Leveraging Cognitive Computing in the Real World		2016	https://doi.org/10.1145/2911451.2926724	https://doi.org/10.1145/2911451.2926724	FALSE
Ma, Longxuan and Li, Mingda and Zhang, Wei-Nan and Li, Jiapeng and Liu, Ting	Unstructured Text Enhanced Open-Domain Dialogue System: A Systematic Survey	Incorporating external knowledge into dialogue generation has been proven to benefit the performance of an open-domain Dialogue System (DS), such as generating informative or stylized responses, controlling conversation topics. In this article, we study the open-domain DS that uses unstructured text as external knowledge sources (Unstructured Text Enhanced Dialogue System (UTEDS)). The existence of unstructured text entails distinctions between UTEDS and traditional data-driven DS and we aim at analyzing these differences. We first give the definition of the UTEDS related concepts, then summarize the recently released datasets and models. We categorize UTEDS into Retrieval and Generative models and introduce them from the perspective of model components. The retrieval models consist of Fusion, Matching, and Ranking modules, while the generative models comprise Dialogue and Knowledge Encoding, Knowledge Selection (KS), and Response Generation modules. We further summarize the evaluation methods utilized in UTEDS and analyze the current models’ performance. At last, we discuss the future development trends of UTEDS, hoping to inspire new research in this field.	2021	https://doi.org/10.1145/3464377	https://doi.org/10.1145/3464377	FALSE
E., Shijia and Xu, Shiyao and Xiang, Yang	Incorporating Statistical Features in Convolutional Neural Networks for Question Answering with Financial Data	The goal of question answering with financial data is selecting sentences as answers from the given documents for a question. The core of the task is computing the similarity score between the question and answer pairs. In this paper, we incorporate statistical features such as the term frequency-inverse document frequency (TF-IDF) and the word overlap in convolutional neural networks to learn optimal vector representations of question-answering pairs. The proposed model does not depend on any external resources and can be easily extended to other domains. Our experiments show that the TF-IDF and the word overlap features can improve the performance of basic neural network models. Also, with our experimental results, we can prove that models based on the margin loss training achieve better performance than the traditional classification models. When the number of candidate answers for each question is 500, our proposed model can achieve 0.622 in Top-1 accuracy (Top-1), 0.654 in mean average precision (MAP), 0.767 in normalized discounted cumulative gain (NDCG), and 0.701 in bilingual evaluation understudy (BLEU). If the number of candidate answers is 30, all the values of the evaluation metrics can reach more than 90%.	2018	https://doi.org/10.1145/3184558.3191826	https://doi.org/10.1145/3184558.3191826	FALSE
Affolter, Katrin and Stockinger, Kurt and Bernstein, Abraham	A Comparative Survey of Recent Natural Language Interfaces for Databases	Over the last few years, natural language interfaces (NLI) for databases have gained significant traction both in academia and industry. These systems use very different approaches as described in recent survey papers. However, these systems have not been systematically compared against a set of benchmark questions in order to rigorously evaluate their functionalities and expressive power. In this paper, we give an overview over 24 recently developed NLIs for databases. Each of the systems is evaluated using a curated list of ten sample questions to show their strengths and weaknesses. We categorize the NLIs into four groups based on the methodology they are using: keyword-, pattern-, parsing- and grammar-based NLI. Overall, we learned that keyword-based systems are enough to answer simple questions. To solve more complex questions involving subqueries, the system needs to apply some sort of parsing to identify structural dependencies. Grammar-based systems are overall the most powerful ones, but are highly dependent on their manually designed rules. In addition to providing a systematic analysis of the major systems, we derive lessons learned that are vital for designing NLIs that can answer a wide range of user questions.	2019	https://doi.org/10.1007/s00778-019-00567-8	https://doi.org/10.1007/s00778-019-00567-8	FALSE
Schlaefer, Nico and Chu-Carroll, Jennifer and Nyberg, Eric and Fan, James and Zadrozny, Wlodek and Ferrucci, David	Statistical Source Expansion for Question Answering	A source expansion algorithm automatically extends a given text corpus with related content from large external sources such as the Web. The expanded corpus is not intended for human consumption but can be used in question answering (QA) and other information retrieval or extraction tasks to find more relevant information and supporting evidence. We propose an algorithm that extends a corpus of seed documents with web content, using a statistical model to select text passages that are both relevant to the topics of the seeds and complement existing information.In an evaluation on 1,500 hand-labeled web pages, our algorithm ranked text passages by relevance with 81% MAP, compared to 43% when relying on web search engine ranks alone and 75% when using a multi-document summarization algorithm. Applied to QA, the proposed method yields consistent and significant performance gains. We evaluated the impact of source expansion on over 6,000 questions from the Jeopardy! quiz show and TREC evaluations using Watson, a state-of-the-art QA system. Accuracy increased from 66% to 71% on Jeopardy! questions and from 59% to 64% on TREC questions.	2011	https://doi.org/10.1145/2063576.2063632	https://doi.org/10.1145/2063576.2063632	FALSE
Chen, Cen and Wang, Chengyu and Qiu, Minghui and Gao, Dehong and Jin, Linbo and Li, Wang	Cross-Domain Knowledge Distillation for Retrieval-Based Question Answering Systems	 Question Answering (QA) systems have been extensively studied in both academia and the research community due to their wide real-world applications. When building such industrial-scale QA applications, we are facing two prominent challenges, i.e., i) lacking a sufficient amount of training data to learn an accurate model and ii) requiring high inference speed for online model serving. There are generally two ways to mitigate the above-mentioned problems. One is to adopt transfer learning to leverage information from other domains; the other is to distill the “dark knowledge” from a large teacher model to small student models. The former usually employs parameter sharing mechanisms for knowledge transfer, but does not utilize the “dark knowledge” of pre-trained large models. The latter usually does not consider the cross-domain information from other domains. We argue that these two types of methods can be complementary to each other. Hence in this work, we provide a new perspective on the potential of the teacher-student paradigm facilitating cross-domain transfer learning, where the teacher and student tasks belong to heterogeneous domains, with the goal to improve the student model’s performance in the target domain. Our framework considers the “dark knowledge” learned from large teacher models and also leverages the adaptive hints to alleviate the domain differences between teacher and student models. Extensive experiments have been conducted on two text matching tasks for retrieval-based QA systems. Results show the proposed method has better performance than the competing methods including the existing state-of-the-art transfer learning methods. We have also deployed our method in an online production system and observed significant improvements compared to the existing approaches in terms of both accuracy and cross-domain robustness.	2021	https://doi.org/10.1145/3442381.3449814	https://doi.org/10.1145/3442381.3449814	FALSE
Xia, Yuan and Wang, Chunyu and Shi, Zhenhui and Zhou, Jingbo and Lu, Chao and Huang, Haifeng and Xiong, Hui	Medical Entity Relation Verification with Large-Scale Machine Reading Comprehension	Medical entity relation verification is a crucial step to build a practical and enterprise medical knowledge graph (MKG) because high-precision medical entity relation is a key requirement for many MKG-based applications. Existing relation verification approaches for general knowledge graphs are not designed for considering medical domain knowledge, although it is central to achieve high-quality entity relation verification for MKG. To this end, in this paper, we introduce a system for medical entity relation verification with large-scale machine reading comprehension. The proposed system is tailored to overcome the unique challenges of medical relation verification including high variants of medical terms, the high difficulty of evidence searching in complex medical documents, and the lack of evidence labels for supervision. To deal with the problem of variants of medical terms, we introduce a synonym-aware retrieve model to retrieve the potential evidence implicitly verifying the given claim. To better utilize the medical domain knowledge, a relation-aware evidence detector and a medical ontology-enhanced aggregator are developed to improve the performance of the relation verification module. Moreover, to overcome the challenge of providing high-quality evidence due to the lack of labels, we introduce an interactive collaborative-training method to iteratively improve the evidence accuracy. Finally, we conduct extensive experiments to demonstrate that the performance of our proposed system is superior to all comparable models. We also demonstrate that our system can significantly reduce the annotation time by medical experts in real-world verification tasks. It can help to improve the efficiency by nearly 300%. In particular, our system has been embedded into the Baidu Clinical Decision Support System.	2021	https://doi.org/10.1145/3447548.3467144	https://doi.org/10.1145/3447548.3467144	FALSE
Liao, Han-Teng and Petzold, Thomas	Geographic and Linguistic Normalization: Towards a Better Understanding of the Geolinguistic Dynamics of Knowledge	This paper proposes a method of geo-linguistic normalization to advance the existing comparative analysis of open collaborative communities, with multilingual Wikipedia projects as the example. Such normalization requires data regarding the potential users and/or resources of a geolinguistic unit.	2014	https://doi.org/10.1145/2641580.2641623	https://doi.org/10.1145/2641580.2641623	FALSE
Li, Rui and Yang, Cheng and Li, Tingwei and Su, Sen	MiDTD: A Simple and Effective Distillation Framework for Distantly Supervised Relation Extraction	Relation extraction (RE), an important information extraction task, faced the great challenge brought by limited annotation data. To this end, distant supervision was proposed to automatically label RE data, and thus largely increased the number of annotated instances. Unfortunately, lots of noise relation annotations brought by automatic labeling become a new obstacle. Some recent studies have shown that the teacher-student framework of knowledge distillation can alleviate the interference of noise relation annotations via label softening. Nevertheless, we find that they still suffer from two problems: propagation of inaccurate dark knowledge and constraint of a unified distillation temperature. In this article, we propose a simple and effective Multi-instance Dynamic Temperature Distillation (MiDTD) framework, which is model-agnostic and mainly involves two modules: multi-instance target fusion (MiTF) and dynamic temperature regulation (DTR). MiTF combines the teacher’s predictions for multiple sentences with the same entity pair to amend the inaccurate dark knowledge in each student’s target. DTR allocates alterable distillation temperatures to different training instances to enable the softness of most student’s targets to be regulated to a moderate range. In experiments, we construct three concrete MiDTD instantiations with BERT, PCNN, and BiLSTM-based RE models, and the distilled students significantly outperform their teachers and the state-of-the-art (SOTA) methods.	2022	https://doi.org/10.1145/3503917	https://doi.org/10.1145/3503917	FALSE
Ge, Congcong and Liu, Xiaoze and Chen, Lu and Gao, Yunjun and Zheng, Baihua	LargeEA: Aligning Entities for Large-Scale Knowledge Graphs	Entity alignment (EA) aims to find equivalent entities in different knowledge graphs (KGs). Current EA approaches suffer from scalability issues, limiting their usage in real-world EA scenarios. To tackle this challenge, we propose LargeEA to align entities between large-scale KGs. LargeEA consists of two channels, i.e., structure channel and name channel. For the structure channel, we present METIS-CPS, a memory-saving mini-batch generation strategy, to partition large KGs into smaller mini-batches. LargeEA, designed as a general tool, can adopt any existing EA approach to learn entities' structural features within each mini-batch independently. For the name channel, we first introduce NFF, a name feature fusion method, to capture rich name features of entities without involving any complex training process; we then exploit a name-based data augmentation to generate seed alignment without any human intervention. Such design fits common real-world scenarios much better, as seed alignment is not always available. Finally, LargeEA derives the EA results by fusing the structural features and name features of entities. Since no widely-acknowledged benchmark is available for large-scale EA evaluation, we also develop a large-scale EA benchmark called DBP1M extracted from real-world KGs. Extensive experiments confirm the superiority of LargeEA against state-of-the-art competitors.	2021	https://doi.org/10.14778/3489496.3489504	https://doi.org/10.14778/3489496.3489504	FALSE
Mohsen, Wa'el and Aref, Mostafa and ElBahnasy, Khaled	Cooperative Domain Ontology Reduction Based on Power Sets	Ontology is widely used in the areas of knowledge engineering, web-based data mining, and others. The process of developing and evolving inter-organizational domain ontologies is easy to get much redundant information. PowerSets can be used to reduce the attributes of ontologies. In this paper, "Rule Finding Uniqueness," RFU is proposed for learning a set of rules in order to refine an ontology. The algorithm's primary goal is to generate unique rules that not only cover the initial set but also enhance reasoning. The claimed technique compresses Ontologies after it is already built or during the evolving process of the inter-organizational cooperative domain ontology. The proposed method can also be used to strengthen automatic and semi-automatic operations to develop and evolve ontologies. We can consider this approach as a maintenance operation that could be done periodically based on the ontology evolution frequency rate.	2020	https://doi.org/10.1145/3404709.3404771	https://doi.org/10.1145/3404709.3404771	FALSE
Yan, Rui and Song, Yiping and Zhou, Xiangyang and Wu, Hua	"Shall I Be Your Chat Companion?": Towards an Online Human-Computer Conversation System	To establish an automatic conversation system between human and computer is regarded as one of the most hardcore problems in computer science. It requires interdisciplinary techniques in information retrieval, natural language processing, and data management, etc. The challenges lie in how to respond like a human, and to maintain a relevant, meaningful, and continuous conversation. The arrival of big data era reveals the feasibility to create such a system empowered by data-driven approaches. We can now organize the conversational data as a chat companion. In this paper, we introduce a chat companion system, which is a practical conversation system between human and computer as a real application. Given the human utterances as queries, our proposed system will respond with corresponding replies retrieved and highly ranked from a massive conversational data repository. Note that 'practical' here indicates effectiveness and efficiency: both issues are important for a real-time system based on a massive data repository. We have two scenarios of single-turn and multi-turn conversations. In our system, we have a base ranking without conversational context information (for single-turn) and a context-aware ranking (for multi-turn). Both rankings can be conducted either by a shallow learning or deep learning paradigm. We combine these two rankings together in optimization. In the experimental setups, we investigate the performance between effectiveness and efficiency for the proposed methods, and we also compare against a series of baselines to demonstrate the advantage of the proposed framework in terms of p@1, MAP, and nDCG. We present a new angle to launch a practical online conversation system between human and computer.	2016	https://doi.org/10.1145/2983323.2983360	https://doi.org/10.1145/2983323.2983360	FALSE
Patil, Charulata and Patwardhan, Manasi	Visual Question Generation: The State of the Art	Visual question generation (VQG) is an interesting problem that has recently received attention. The task of VQG involves generating meaningful questions based on the input image. It is a multi-modal problem involving image understanding and natural language generation, especially using deep learning methods. VQG can be considered as complementary task of visual question answering. In this article, we review the current state of VQG in terms of methods to understand the problem, existing datasets to train the VQG model, evaluation metrics, and algorithms to handle the problem. Finally, we discuss the challenges that need to be conquered and the possible future directions for an effective VQG.	2020	https://doi.org/10.1145/3383465	https://doi.org/10.1145/3383465	FALSE
Waibel, Alexander	Multimodal Dialogue Processing for Machine Translation		2019		https://doi.org/10.1145/3233795.3233811	FALSE
Gupta, Manish and Agrawal, Puneet	Compression of Deep Learning Models for Text: A Survey	In recent years, the fields of natural language processing (NLP) and information retrieval (IR) have made tremendous progress thanks to deep learning models like Recurrent Neural Networks (RNNs), Gated Recurrent Units (GRUs) and Long Short-Term Memory (LSTMs) networks, and Transformer&nbsp;[121] based models like Bidirectional Encoder Representations from Transformers (BERT)&nbsp;[24], Generative Pre-training Transformer (GPT-2)&nbsp;[95], Multi-task Deep Neural Network (MT-DNN)&nbsp;[74], Extra-Long Network (XLNet)&nbsp;[135], Text-to-text transfer transformer (T5)&nbsp;[96], T-NLG&nbsp;[99], and GShard&nbsp;[64]. But these models are humongous in size. On the other hand, real-world applications demand small model size, low response times, and low computational power wattage. In this survey, we discuss six different types of methods (Pruning, Quantization, Knowledge Distillation (KD), Parameter Sharing, Tensor Decomposition, and Sub-quadratic Transformer-based methods) for compression of such models to enable their deployment in real industry NLP projects. Given the critical need of building applications with efficient and small models, and the large amount of recently published work in this area, we believe that this survey organizes the plethora of work done by the “deep learning for NLP” community in the past few years and presents it as a coherent story.	2022	https://doi.org/10.1145/3487045	https://doi.org/10.1145/3487045	FALSE
Gupta, Amulya and Zhang, Zhu	Swings and Roundabouts: Attention-Structure Interaction Effect in Deep Semantic Matching	In the context of deep learning models for semantic matching problems, we propose a novel Multi-View Progressive Attention (MV-PA) mechanism general enough to operate on various linguistic structures of text. More importantly, we study the interaction effect between explicit linguistic structures (e.g., linear, constituency, and dependency) and implicit structures elicited by attention mechanisms. Empirical results on multiple datasets demonstrate salient patterns of substitutability between the two families of structures (explicit and implicit). Our findings not only provide intellectual foundations for the popular use of “linear LSTM + attention” architectures in NLP/QA research, but also have implications in other modalities and domains.	2020	https://doi.org/10.1109/TASLP.2020.3013703	https://doi.org/10.1109/TASLP.2020.3013703	FALSE
Yan, Rui and Zhao, Dongyan	A NeuRetrieval Model for Human-Computer Conversations	To establish an automatic conversation system between human and computer is regarded as one of the most hardcore problems in computer science. It requires interdisciplinary techniques of information retrieval, natural language processing, data management as well as artificial intelligence. The arrival of big data era reveals the feasibility to create a conversation system empowered by data-driven approaches. Now we are able to collect extremely large conversational data on Web, and organize them to launch a human-computer conversation system. Owing to the diversity of Web resources available, a retrieval-based conversation system will be able to find at least some responses from the massive data repository for any user inputs. Given a human issued utterance, i.e., a query, a retrieval-based conversation system will search for appropriate replies, conduct a relevance ranking, and then output the highly relevant one as the response. In this paper, we propose a novel retrieval model named NeuRetrieval for short text understanding, representation and semantic matching. The proposed model is general and unified for both single-turn and multi-turn conversation scenarios in open domain. In the experiments, we investigate the effectiveness of the proposed deep neural network model for human-computer conversations. We demonstrate performance improvement against a series of baseline methods in several evaluation metrics. In contrast with previously proposed methods, NeuRetrieval is tailored for conversation scenarios and demonstrated to be more effective.	2018	https://doi.org/10.1145/3184558.3186341	https://doi.org/10.1145/3184558.3186341	FALSE
Abiteboul, Serge and Dong, Luna and Etzioni, Oren and Srivastava, Divesh and Weikum, Gerhard and Stoyanovich, Julia and Suchanek, Fabian M.	The Elephant in the Room: Getting Value from Big Data		2015	https://doi.org/10.1145/2767109.2770014	https://doi.org/10.1145/2767109.2770014	FALSE
Lachana, Zoi and Avgerinos Loutsaris, Michalis and Alexopoulos, Charalampos and Charalabidis, Yannis	Clustering Legal Artifacts Using Text Mining	The globalization of communication networks and the possibilities offered by the information and communication technologies (ICTs) significantly change the public sector's operation and services. Digital Governance is now integrated into administrations' policies and programs at all levels: local, regional, national, European. At the national level, there is a requirement to provide electronic public services according to citizens' needs while, in the sense of globalization, at the European level, there are many programs (e.g., the Europe 2005 and i2010 program) emphasizing the Digital Governance world (or better Digital Governance community) that indicates rapid changes not only in the sense of the change in the public sector's systems but also in the mentality that the public sector operates. On the other hand, Digital Governance's evolution affects societies intensively, emphasizing the importance of cross-border interaction and information sharing between them. [6]. Concerning the legal informatics domain, this can result in changing governments' operations in many ways [2]. By now, the massive amount of each country's legal information currently remains fragmented across multiple national databases and systems or even better legal databases. Most of these legal databases result from the significant advancements in the “legal informatics” research field that observed since governments have started to promote the development of legal information systems [9]. This research contributes to this purpose by developing an open and automated legal system capable of providing any EU country's legal information based on the existing ontologies.	2021	https://doi.org/10.1145/3494193.3494202	https://doi.org/10.1145/3494193.3494202	FALSE
Yuan, Wei and He, Tieke and Dai, Xinyu	Improving Neural Question Generation Using Deep Linguistic Representation	 Question Generation (QG) is a challenging Natural Language Processing (NLP) task which aims at generating questions with given answers and context. There are many works incorporating linguistic features to improve the performance of QG. However, similar to traditional word embedding, these works normally embed such features with a set of trainable parameters, which results in the linguistic features not fully exploited. In this work, inspired by the recent achievements of text representation, we propose to utilize linguistic information via large pre-trained neural models. First, these models are trained in several specific NLP tasks in order to better represent linguistic features. Then, such feature representation is fused into a seq2seq based QG model to guide question generation. Extensive experiments were conducted on two benchmark Question Generation datasets to evaluate the effectiveness of our approach. The experimental results demonstrate that our approach outperforms the state-of-the-art QG systems, as a result, it significantly improves the baseline by 17.2% and 6.2% under the BLEU-4 metric on these two datasets, respectively.	2021	https://doi.org/10.1145/3442381.3449975	https://doi.org/10.1145/3442381.3449975	FALSE
Boteanu, Adrian and Dutile, Emily and Kiezun, Adam and Artzi, Shay	Subjective Search Intent Predictions Using Customer Reviews	Query intent prediction is a component of information retrieval which improves result relevance through an understanding of latent user intents in addition to explicit query keywords. We target context-of-use intents, such as the activity for which a product is used and the target audience for a product, which are subjective and not usually indexed as product attributes in the catalog. We describe a method to predict latent query intents: we extract intents from product reviews on amazon.com and, using behavioral purchase signals that associate queries with the reviewed products, train query classifiers that label queries with the intents extracted from reviews. For example, we predict the activity "running" for the query "adidas mens pants." We show that our method can predict latent intents not indexed directly in the product catalog.	2020		https://doi.org/10.1145/3343413.3377987	FALSE
Alexander, D. and Arvola, P. and Beckers, T. and Bellot, P. and Chappell, T. and DeVries, C. M. and Doucet, A. and Fuhr, N. and Geva, S. and Kamps, J. and Kazai, G. and Koolen, M. and Kutty, S. and Landoni, M. and Moriceau, V. and Nayak, R. and Nordlie, R. and Pharo, N. and SanJuan, E. and Schenkel, R. and Tagarelli, A. and Tannier, X. and Thom, J. A. and Trotman, A. and Vainio, J. and Wang, Q. and Wu, C.	Report on INEX 2010	INEX investigates focused retrieval from structured documents by providing large test collections of structured documents, uniform evaluation measures, and a forum for organizations to compare their results. This paper reports on the INEX 2010 evaluation campaign, which consisted of a wide range of tracks: Ad Hoc, Book, Data Centric, Interactive, QA, Link the Wiki, Relevance Feedback, Web Service Discovery and XML Mining.	2011	https://doi.org/10.1145/1988852.1988854	https://doi.org/10.1145/1988852.1988854	FALSE
Jouis, Christophe and Or\'{u}s-Lacort, Mercedes and Durglishvili, Nino and Or\'{u}s, Rom\'{a}n	Management of Big Textual Data in Qualitative Research: Organizing the Relationships in a Typology Based on Logical Properties	In structured indexes, classification systems, thesauri, conceptual structures or semantic networks, relationships are too often vague. For instance, in terminology, the relationships between concepts are often reduced to the distinction established by standard between hierarchical relationships (genus-species relationships and part/whole relationships) and non-hierarchical relationships ("time, space, causal relationships, etc."). The semantics of relationships are vague because the principal users of these relationships are industrial actors (translators of technical handbooks, terminologists, data-processing specialists, etc.). Nevertheless, the consistency of the models built must always be guaranteed... One possible approach to this problem consists in organizing the relationships in a typology based on logical properties. For instance, we typically use only the general relation "Is-a". It is too vague. We assume that general relation "Is-a" is characterized by asymmetry. This asymmetry is specified in: (1) the belonging of one individualizable entity to a distributive class, (2) Inclusion among distributive classes and (3) relation part of (or "composition"). With the view to better designing the knowledge structures underlying the concepts of a field, and more specifically, the indexing of documents and/or information retrieval, we propose a structured set of relationships, based on a linguistic model, the Applicative and Cognitive Grammar (ACG) of Jean-Pierre Descles [7], [8]. This model was extended to terminology by Christophe Jouis [32], and applied by Widad Mustafa and Christophe Jouis, [33], [34], [35] and [36].	2019	https://doi.org/10.1145/3297662.3365825	https://doi.org/10.1145/3297662.3365825	FALSE
Syme, Don	The Early History of F#	This paper describes the genesis and early history of the F# programming language. I start with the origins of strongly-typed functional programming (FP) in the 1970s, 80s and 90s. During the same period, Microsoft was founded and grew to dominate the software industry. In 1997, as a response to Java, Microsoft initiated internal projects which eventually became the .NET programming framework and the C# language. From 1997 the worlds of academic functional programming and industry combined at Microsoft Research, Cambridge. The researchers engaged with the company through Project 7, the initial effort to bring multiple languages to .NET, leading to the initiation of .NET Generics in 1998 and F# in 2002. F# was one of several responses by advocates of strongly-typed functional programming to the "object-oriented tidal wave" of the mid-1990s. The development of the core features of F# 1.0 happened from 2004-2007, and I describe the decision-making process that led to the "productization" of F# by Microsoft in 2007-10 and the release of F# 2.0. The origins of F#'s characteristic features are covered: object programming, quotations, statically resolved type parameters, active patterns, computation expressions, async, units-of-measure and type providers. I describe key developments in F# since 2010, including F# 3.0-4.5, and its evolution as an open source, cross-platform language with multiple delivery channels. I conclude by examining some uses of F# and the influence F# has had on other languages so far.	2020	https://doi.org/10.1145/3386325	https://doi.org/10.1145/3386325	FALSE
Gon\c{c}alves, Rodrigo and Dorneles, Carina Friedrich	Automated Expertise Retrieval: A Taxonomy-Based Survey and Open Issues	Understanding people’s expertise is not a trivial task since it is time-consuming when manually executed. Automated approaches have become a topic of research in recent years in various scientific fields, such as information retrieval, databases, and machine learning. This article carries out a survey on automated expertise retrieval, i.e., finding data linked to a person that describes the person’s expertise, which allows tasks such as profiling or finding people with a certain expertise. A faceted taxonomy is introduced that covers many of the existing approaches and classifies them on the basis of features chosen from studying the state-of-the-art. A list of open issues, with suggestions for future research topics, is introduced as well. It is hoped that our taxonomy and review of related works on expertise retrieval will be useful when analyzing different proposals and will allow a better understanding of existing work and a systematic classification of future work on the topic.	2019	https://doi.org/10.1145/3331000	https://doi.org/10.1145/3331000	FALSE
Rahman, Rifat and Rahman, Md. Rishadur and Tripto, Nafis Irtiza and Ali, Mohammed Eunus and Apon, Sajid Hasan and Shahriyar, Rifat	AdolescentBot: Understanding Opportunities for Chatbots in Combating Adolescent Sexual and Reproductive Health Problems in Bangladesh	 Traditional face-to-face health consultation-based systems largely failed to attract teenagers to get reproductive and sexual health supports from doctors and practitioners in Bangladesh as ‘sex’ or ‘adolescent’ related issues are considered social taboos and are rarely discussed openly with anyone. This has damaging implications for the physiological and mental well-being of a large group of people. In this paper, we study chatbot’s effectiveness to assist adolescents in seeking reproductive and sexual health supports by analyzing the responses from 256 participants, including adolescents and medical personnel from six different regions of Bangladesh. We prototype an interactive chatbot, namely AdolescentBot, and analyzed users’ communication patterns, feelings, and contexts of use as the first point of support for getting adolescence related health advice. Our analysis finds that a chatbot can satisfy most of the users’ queries, and the majority of the queries are associated with wrong-beliefs. Finally, we discuss ethical and societal issues with chatbot usage and recommend a set of design propositions for the AdolescentBot.	2021	https://doi.org/10.1145/3411764.3445694	https://doi.org/10.1145/3411764.3445694	FALSE
Majeed, Asim	Technology Diffusion and Virtualisation of Virtual Communities	In an undeniably virtual society, various processes and research areas that have customarily been directed by means of physical systems are now being led virtually. This wonder of virtualisation is conducted in numerous ways such as teaching, shopping (by means of electronic business) and social media. It also envisages that a few processes are more amiable to virtualization than others. For instance, replete development of virtual societies, exchange of academic information as well as online learning, appears to be preferable for some disciplines over others. The electronic trade has functioned admirably for some virtual communities, however, not for others. These perceptions spur the focal inquiry postured in this paper: how technology diffusion impacts the virtualisation of virtual communities? This inquiry will become progressively vital as advances in data technology make the potential for society to virtualise more processes. To give a general hypothetical premise to exploring this question, this paper proposes "process virtualisation hypothesis," which incorporates four primary areas (technology diffusion, virtualisation, virtual communities and identification, and control requirements) which influence whether a procedure is agreeable or impervious to being directed. Perceiving that processes can be virtualized with or without the utilisation of data technology, this paper expresses the hypothetical centrality of data technology in virtualisation by talking about the direct impacts of representation of various virtual communities in alliance to monitoring capability and their access. This clarifies how bolstering data technology is empowering another era of virtualisation of virtual communities.	2017	https://doi.org/10.1145/3026480.3026494	https://doi.org/10.1145/3026480.3026494	FALSE
Sojka, Petr	Exploiting Semantic Annotations in Math Information Retrieval	This paper describes exploitation of semantic annotations in the design and architecture of MIaS (Math Indexer and Searcher) system for mathematics retrieval. Basing on the claim that navigational and research search are `killer' applications for digital library such as the European Digital Mathematics Library, EuDML, we argue for an approach based on Natural Language Processing techniques as used in corpus management systems such as the Sketch Engine, that will reach web scalability and avoid inference problems. The main ideas are 1) to augment surface texts (including math formulae) with additional linked representations bearing semantic information (expanded formulae as text, canonicalized text and subformulae) for indexing, including support for indexing structural information (expressed as Content MathML or other tree structures) and 2) use semantic user preferences to order found documents.The semantic enhancements of the MIaS system are being implemented as a math-aware search engine based on the state-of-the-art system Apache Lucene, with support for [MathML] tree indexing. Scalability issues have been checked against more than 400,000 arXiv documents.	2012	https://doi.org/10.1145/2390148.2390157	https://doi.org/10.1145/2390148.2390157	FALSE
Cunningham, Sally Jo and Darari, Fariz and Krisnadhi, Adila and Hinze, Annika	Capturing Cultural Heritage in East Asia and Oceania		2020	https://doi.org/10.1145/3378548	https://doi.org/10.1145/3378548	FALSE
Qin, Yujia and Qi, Fanchao and Ouyang, Sicong and Liu, Zhiyuan and Yang, Cheng and Wang, Yasheng and Liu, Qun and Sun, Maosong	Improving Sequence Modeling Ability of Recurrent Neural Networks via Sememes	Sememes, the minimum semantic units of human languages, have been successfully utilized in various natural language processing applications. However, most existing studies exploit sememes in specific tasks and few efforts are made to utilize sememes more fundamentally. In this paper, we propose to incorporate sememes into recurrent neural networks (RNNs) to improve their sequence modeling ability, which is beneficial to all kinds of downstream tasks. We design three different sememe incorporation methods and employ them in typical RNNs including LSTM, GRU and their bidirectional variants. In evaluation, we use several benchmark datasets involving PTB and WikiText-2 for language modeling, SNLI for natural language inference and another two datasets for sentiment analysis and paraphrase detection. Experimental results show evident and consistent improvement of our sememe-incorporated models compared with vanilla RNNs, which proves the effectiveness of our sememe incorporation methods. Moreover, we find the sememe-incorporated models have higher robustness and outperform adversarial training in defending adversarial attack. All the code and data of this work can be obtained at https://github.com/thunlp/SememeRNN.	2020	https://doi.org/10.1109/TASLP.2020.3012060	https://doi.org/10.1109/TASLP.2020.3012060	FALSE
Gupta, Dhruv and Berberich, Klaus	GYANI: An Indexing Infrastructure for Knowledge-Centric Tasks	In this work, we describe GYANI (gyan stands for knowledge in Hindi), an indexing infrastructure for search and analysis of large semantically annotated document collections. To facilitate the search for sentences or text regions for many knowledge-centric tasks such as information extraction, question answering, and relationship extraction, it is required that one can query large annotated document collections interactively. However, currently such an indexing infrastructure that scales to millions of documents and provides fast query execution times does not exist. To alleviate this problem, we describe how we can effectively index layers of annotations (e.g., part-of-speech, named entities, temporal expressions, and numerical values) that can be attached to sequences of words. Furthermore, we describe a query language that provides the ability to express regular expressions between word sequences and semantic annotations to ease search for sentences and text regions for enabling knowledge acquisition at scale. We build our infrastructure on a state-of-the-art distributed extensible record store. We extensively evaluate GYANI over two large news archives and the entire Wikipedia amounting to more than fifteen million documents. We observe that using GYANI we can achieve significant speed ups of more than 95x in information extraction, 53x on extracting answer candidates for questions, and 12x on relationship extraction task.	2018	https://doi.org/10.1145/3269206.3271745	https://doi.org/10.1145/3269206.3271745	FALSE
Lam, Khang Nhut and Le, Nam Nhat and Kalita, Jugal	Building a Chatbot on a Closed Domain Using RASA	In this study, we build a chatbot system in a closed domain with the RASA framework, using several models such as SVM for classifying intents, CRF for extracting entities and LSTM for predicting action. To improve responses from the bot, the kNN algorithm is used to transform false entities extracted into true entities. The knowledge domain of our chatbot is about the College of Information and Communication Technology of Can Tho University, Vietnam. We manually construct a chatbot corpus with 19 intents, 441 sentence patterns of intents, 253 entities and 133 stories. Experiment results show that the bot responds well to relevant questions.	2020	https://doi.org/10.1145/3443279.3443308	https://doi.org/10.1145/3443279.3443308	FALSE
Sai, Ananya B. and Mohankumar, Akash Kumar and Khapra, Mitesh M.	A Survey of Evaluation Metrics Used for NLG Systems	In the last few years, a large number of automatic evaluation metrics have been proposed for evaluating Natural Language Generation (NLG) systems. The rapid development and adoption of such automatic evaluation metrics in a relatively short time has created the need for a survey of these metrics. In this survey, we (i) highlight the challenges in automatically evaluating NLG systems, (ii) propose a coherent taxonomy for organising existing evaluation metrics, (iii) briefly describe different existing metrics, and finally (iv) discuss studies criticising the use of automatic evaluation metrics. We then conclude the article highlighting promising future directions of research.	2022	https://doi.org/10.1145/3485766	https://doi.org/10.1145/3485766	FALSE
Agarwal, Swati	Open Source Social Media Intelligence for Enabling Government Applications: Extended Abstract	Open-source social media intelligence (OSSMInt) is a field comprising of techniques and applications to analyze and mine open-source social media data for extracting actionable information and useful insights. The focus of the work presented in this paper is on novel applications and techniques of OSSMInt in the government sector. I propose and develop several novel use-cases and applications around OSSMInt for government and broadly divide them into three categories: identification, prediction, and response applications. In particular, I present solutions, tools and techniques for analyzing data from micro-blogging website to analyze citizen complaints and grievances in the public sector [response]. The research presented in this paper also describes my work on analyzing data from Twitter micro-blogging website to early forecast a civil unrest and protest [prediction]. Furthermore, I build various applications around identification and detection that are useful for the government and security analysts. I demonstrate the application of OSSMInt for identifying religious conflicts within society by mining public opinions on Tumblr website and fill the gaps of offline surveys. The study presented in this paper propose solutions for enabling law enforcement agencies to detect, prevent and combat online radicalization and extremism (content, users, and communities) by mining data from Tumblr, Twitter and YouTube websites [identification]. Furthermore, I also propose and build an application for detecting secret message exchanged in an adversarial communication and capture the obfuscated terms in messages. The central component of my proposed solution approach is the application of information retrieval and machine learning based techniques and algorithms. The study consists of experimenting with a diverse range of machine learning algorithms such as unsupervised, semi-supervised and supervised learning (k-NN, SVM, naive Bayes, random forest and decision tree) based algorithms. Based on the experimental results, I observe that there are several untapped opportunities as well as technical challenges in exploiting open-source social media data by the government for intelligence gathering. Furthermore, social media is a rich source of information for building security informatics based applications by mining the content, users, links and relationships on social media platforms.	2017	https://doi.org/10.1145/3110394.3110397	https://doi.org/10.1145/3110394.3110397	FALSE
Feng, Fuli and Luo, Cheng and He, Xiangnan and Liu, Yiqun and Chua, Tat-Seng	FinIR 2020: The First Workshop on Information Retrieval in Finance	This half-day workshop explores challenges and potential research directions about Information Retrieval (IR) in finance. The focus will be on stimulating discussions around the accessing, searching, filtering, and analyzing financial documents in banking, insurance, and investment, such as the financial statements, analyst reports, filling forms, and news articles. We welcome theoretical, experimental, and methodological studies that aim to advance techniques of managing and understanding financial documents, as well as emphasize the applicability in practical applications. The workshop aims to bring together a diverse set of researchers and practitioners interested in investigating relevant topics. Besides, to facilitate developing and testing some relevant techniques, we hold a data challenge on quantifying analyst reports and news articles for the prediction of commodity prices.	2020		https://doi.org/10.1145/3397271.3401462	FALSE
Gelernter, Judith and Ganesh, Gautam and Krishnakumar, Hamsini and Zhang, Wei	Automatic Gazetteer Enrichment with User-Geocoded Data	Geographical knowledge resources or gazetteers that are enriched with local information have the potential to add geographic precision to information retrieval. We have identified sources of novel local gazetteer entries in crowd-sourced OpenStreetMap and Wikimapia geotags that include geo-coordinates. We created a fuzzy match algorithm using machine learning (SVM) that checks both for approximate spelling and approximate geocoding in order to find duplicates between the crowd-sourced tags and the gazetteer in effort to absorb those tags that are novel. For each crowd-sourced tag, our algorithm generates candidate matches from the gazetteer and then ranks those candidates based on word form or geographical relations between each tag and gazetteer candidate. We compared a baseline of edit distance for candidate ranking to an SVM-trained candidate ranking model on a city level location tag match task. Experiment results show that the SVM greatly outperforms the baseline.	2013	https://doi.org/10.1145/2534732.2534736	https://doi.org/10.1145/2534732.2534736	FALSE
Mansour, Essam and Srinivas, Kavitha and Hose, Katja	Federated Data Science to Break Down Silos [Vision]	Similar to Open Data initiatives, data science as a community has launched initiatives for sharing not only data but entire pipelines, derivatives, artifacts, etc. (Open Data Science). However, the few efforts that exist focus on the technical part on how to facilitate sharing, conversion, etc. This vision paper goes a step further and proposes KEK, an open federated data science platform that does not only allow for sharing data science pipelines and their (meta)data but also provides methods for efficient search and, in the ideal case, even allows for combining and defining pipelines across platforms in a federated manner. In doing so, KEK addresses the so far neglected challenge of actually finding artifacts that are semantically related and that can be combined to achieve a certain goal.	2022	https://doi.org/10.1145/3516431.3516435	https://doi.org/10.1145/3516431.3516435	FALSE
Maier, David and Tekle, K. Tuncay and Kifer, Michael and Warren, David S.	Datalog: Concepts, History, and Outlook	This chapter is a survey of the history and the main concepts of Datalog.We begin with an introduction to the language and its use for database definition and querying. We then look back at the threads from logic languages, databases, artificial intelligence, and expert systems that led to the emergence of Datalog and reminiscence about the origin of the name. We consider the interaction of recursion with other common data language features, such as negation and aggregation, and look at other extensions, such as constraints, updates, and object-oriented features.We provide an overview of the main approaches to Datalog evaluation and their variants, then recount some early implementations of Datalog and of similar deductive database systems.We speculate on the reasons for the decline in the interest in the language in the 1990s and the causes for its later resurgence in a number of application areas.We conclude with several examples of current systems based on or supporting Datalog and briefly examine the performance of some of them.	2018		https://doi.org/10.1145/3191315.3191317	FALSE
Hogan, Aidan and Blomqvist, Eva and Cochez, Michael and D’amato, Claudia and Melo, Gerard De and Gutierrez, Claudio and Kirrane, Sabrina and Gayo, Jos\'{e} Emilio Labra and Navigli, Roberto and Neumaier, Sebastian and Ngomo, Axel-Cyrille Ngonga and Polleres, Axel and Rashid, Sabbir M. and Rula, Anisa and Schmelzeisen, Lukas and Sequeda, Juan and Staab, Steffen and Zimmermann, Antoine	Knowledge Graphs	In this article, we provide a comprehensive introduction to knowledge graphs, which have recently garnered significant attention from both industry and academia in scenarios that require exploiting diverse, dynamic, large-scale collections of data. After some opening remarks, we motivate and contrast various graph-based data models, as well as languages used to query and validate knowledge graphs. We explain how knowledge can be represented and extracted using a combination of deductive and inductive techniques. We conclude with high-level future research directions for knowledge graphs.	2021	https://doi.org/10.1145/3447772	https://doi.org/10.1145/3447772	FALSE
Cai, Li and Zhou, Guangyou and Liu, Kang and Zhao, Jun	Large-Scale Question Classification in CQA by Leveraging Wikipedia Semantic Knowledge	With the flourishing of community-based question answering (cQA) services like Yahoo! Answers, more and more web users seek their information need from these sites. Understanding user's information need expressed through their search questions is crucial to information providers. Question classification in cQA is studied for this purpose. However, there are two main difficulties in applying traditional methods (question classification in TREC QA and text classification) to cQA: (1) Traditional methods confine themselves to classify a text or question into two or a few predefined categories. While in cQA, the number of categories is much larger, such as Yahoo! Answers, there contains 1,263 categories. Our empirical results show that with the increasing of the number of categories to moderate size, the performance of the classification accuracy dramatically decreases. (2) Unlike the normal texts, questions in cQA are very short, which cannot provide sufficient word co-occurrence or shared information for a good similarity measure due to the data sparseness. In this paper, we propose a two-stage approach for question classification in cQA that can tackle the difficulties of the traditional methods. In the first stage, we preform a search process to prune the large-scale categories to focus our classification effort on a small subset. In the second stage, we enrich questions by leveraging Wikipedia semantic knowledge to tackle the data sparseness. As a result, the classification model is trained on the enriched small subset. We demonstrate the performance of our proposed method on Yahoo! Answers with 1,263 categories. The experimental results show that our proposed method significantly outperforms the baseline method (with error reductions of 23.21%).	2011	https://doi.org/10.1145/2063576.2063768	https://doi.org/10.1145/2063576.2063768	FALSE
Maroengsit, Wari and Piyakulpinyo, Thanarath and Phonyiam, Korawat and Pongnumkul, Suporn and Chaovalit, Pimwadee and Theeramunkong, Thanaruk	A Survey on Evaluation Methods for Chatbots	Nowadays chatbots have been widely adopted in many industries to automatically answer users' questions and requests via chat interfaces. While it has become much easier to develop a chatbot system, the system itself is a complex system in nature. It is a challenge to evaluate and compare various chatbot systems in terms of effectiveness, efficiency, goal achievability, and the ability to satisfy users. This paper presents a survey, starting from literature review, chatbot architecture, evaluation methods/criteria, and comparison of evaluation methods. Focused on the three subprocesses in the chatbot architecture: text processing, semantic understanding, and response generation. Moreover, the survey is conducted with classification of chatbot evaluation methods and their analysis according to chatbot types and three main evaluation schemes; content evaluation, user satisfaction, and chat function.	2019	https://doi.org/10.1145/3323771.3323824	https://doi.org/10.1145/3323771.3323824	FALSE
Yang, Christopher C.	ACM SIGHIT International Health Informatics Symposium Report	This report summarizes the events and activities of the 2nd ACM SIGHIT International Health Informatics Symposium (IHI 2012), which was held in Miami, Florida, January 28-30, 2012. IHI 2012 is the flagship conference of ACM SIGHIT concerned with the application of computer science principles, information science principles, information technology, and communication technology to address problems in healthcare, public health, everyday wellness as well as the related social and ethical issues. There were 321 submissions. 48 and 52 papers were accepted for oral presentations and poster presentations, respectively. There were also 2 keynote speeches, 4 panels, 3 tutorials, 1 extended poster session, and a doctoral consortium. There were over 220 attendees.	2012	https://doi.org/10.1145/2384556.2384562	https://doi.org/10.1145/2384556.2384562	FALSE
Li, Tao and Xie, Ning and Zeng, Chunqiu and Zhou, Wubai and Zheng, Li and Jiang, Yexi and Yang, Yimin and Ha, Hsin-Yu and Xue, Wei and Huang, Yue and Chen, Shu-Ching and Navlakha, Jainendra and Iyengar, S. S.	Data-Driven Techniques in Disaster Information Management	Improving disaster management and recovery techniques is one of national priorities given the huge toll caused by man-made and nature calamities. Data-driven disaster management aims at applying advanced data collection and analysis technologies to achieve more effective and responsive disaster management, and has undergone considerable progress in the last decade. However, to the best of our knowledge, there is currently no work that both summarizes recent progress and suggests future directions for this emerging research area. To remedy this situation, we provide a systematic treatment of the recent developments in data-driven disaster management. Specifically, we first present a general overview of the requirements and system architectures of disaster management systems and then summarize state-of-the-art data-driven techniques that have been applied on improving situation awareness as well as in addressing users’ information needs in disaster management. We also discuss and categorize general data-mining and machine-learning techniques in disaster management. Finally, we recommend several research directions for further investigations.	2017	https://doi.org/10.1145/3017678	https://doi.org/10.1145/3017678	FALSE
Furtado, Antonio L. and Ziviani, Nivio	Information and Data Management at PUC-Rio and UFMG	This article presents a summary of the main activities of the Database &amp; Information Systems Research Group at Pontif\'{\i}cia Universidade Cat\'{o}lica do Rio de Janeiro (PUC-Rio) and the Information Management Research Group at Universidade Federal de Minas Gerais (UFMG). These two groups played a pioneering role in the development of the information and data management research area in Brazil. The survey covers about four decades of research work, aiming at theoretical and practical results, with increasing participation of other groups that they helped to initiate.	2018	https://doi.org/10.14778/3229863.3240490	https://doi.org/10.14778/3229863.3240490	FALSE
Papantoniou, Katerina and Tzitzikas, Yannis	NLP for the Greek Language: A Brief Survey	 There is a plethora of methods, tools and resources for processing text in the English language, however this is not the case for other languages, like Greek. Due to the increasing interest in NLP, and since there is a noteworthy number of works related to the processing of the Greek language, in this paper we survey the work related to the processing of Greek language. In particular, we list and briefly discuss related works, resources and tools, categorized according to various processing layers and contexts. This survey can be useful for researchers and students interested in NLP tasks, Information Retrieval and Knowledge Management for the Greek language.	2020	https://doi.org/10.1145/3411408.3411410	https://doi.org/10.1145/3411408.3411410	FALSE
Carpineto, Claudio and Romano, Giovanni	A Survey of Automatic Query Expansion in Information Retrieval	The relative ineffectiveness of information retrieval systems is largely caused by the inaccuracy with which a query formed by a few keywords models the actual user information need. One well known method to overcome this limitation is automatic query expansion (AQE), whereby the user’s original query is augmented by new features with a similar meaning. AQE has a long history in the information retrieval community but it is only in the last years that it has reached a level of scientific and experimental maturity, especially in laboratory settings such as TREC. This survey presents a unified view of a large number of recent approaches to AQE that leverage various data sources and employ very different principles and techniques. The following questions are addressed. Why is query expansion so important to improve search effectiveness? What are the main steps involved in the design and implementation of an AQE component? What approaches to AQE are available and how do they compare? Which issues must still be resolved before AQE becomes a standard component of large operational information retrieval systems (e.g., search engines)?	2012	https://doi.org/10.1145/2071389.2071390	https://doi.org/10.1145/2071389.2071390	FALSE
Pereira, Ivan and Sousa, J\'{e}ssica and Costa, Polyana B. and Barbosa, Simone D. J. and Colcher, S\'{e}rgio	SucupiraBot: An Interactive Question-Answering System for the Sucupira Platform	Conversational user interfaces have been an increasingly popular way to obtain quick information in natural language, especially with advances in the area of natural language processing (NLP) fueled by the surge of deep neural networks. Real-world cases include IBM Watson and Azure Microsoft chatbots, among others. Such examples, however, are most present in the commercial spectrum, with few being actively used by the scientific community. In this work, we develop an Interactive Question Answering System (IQA) for the Sucupira platform, Brazil's biggest open platform of postgraduate content, deployed as conversational interfaces for two popular messaging platforms: Telegram and Discord. We also propose the use of multilingual embeddings for ordering answers by a language-independent measuer of semantic similarity. Our IQA was evaluated with an study with 16 participants. The results were mostly positive, indicating that the use of conversational user interfaces, along with NLP techniques, may be a valid alternative for retrieving scientific information from the Sucupira plataform.	2020	https://doi.org/10.1145/3428658.3431752	https://doi.org/10.1145/3428658.3431752	FALSE
Fiorelli, Manuel and Pazienza, Maria Teresa and Stellato, Armando	Semantic Turkey Goes SKOS Managing Knowledge Organization Systems	In this paper we describe a novel SKOS editor built on top of the web browser Mozilla Firefox. Our tool is targeted towards KOS developers and KOS consumers as well. Indeed, the ability to surf the Web with a standards compliant browser proves useful for both: the former may prove the soundness of a concept by associating it with a concrete set of web resources, whereas the latter may exploit a given KOS to effectively organize information collected from the Web. The editor has been designed as an extension of the knowledge management and acquisition tool Semantic Turkey. The proposed SKOS editor creates a dedicated perspective within an OWL compliant environment, which eases dealing with KOSs. By relying on such rich environment, the editor allows the user to exploit the subtle relationship between SKOS and OWL, thus opening it up to more elaborated modelling solutions, in contrast to other tools which are built on top of the SKOS direct semantics.	2012	https://doi.org/10.1145/2362499.2362509	https://doi.org/10.1145/2362499.2362509	FALSE
Cappellato, Linda and Clough, Paul and Ferro, Nicola and Hall, Mark and Halvey, Martin and Hanbury, Allan and Kanoulas, Evangelos and Kraaij, Wessel and Lupu, Mihai and Sanderson, Mark and Toms, Elaine and Villa, Robert	CLEF 2014: Information Access Evaluation Meets Multilinguality, Multimodality, and Interaction		2014	https://doi.org/10.1145/2701583.2701589	https://doi.org/10.1145/2701583.2701589	FALSE
Qu, Chen and Ji, Feng and Qiu, Minghui and Yang, Liu and Min, Zhiyu and Chen, Haiqing and Huang, Jun and Croft, W. Bruce	Learning to Selectively Transfer: Reinforced Transfer Learning for Deep Text Matching	Deep text matching approaches have been widely studied for many applications including question answering and information retrieval systems. To deal with a domain that has insufficient labeled data, these approaches can be used in a Transfer Learning (TL) setting to leverage labeled data from a resource-rich source domain. To achieve better performance, source domain data selection is essential in this process to prevent the "negative transfer" problem. However, the emerging deep transfer models do not fit well with most existing data selection methods, because the data selection policy and the transfer learning model are not jointly trained, leading to sub-optimal training efficiency. In this paper, we propose a novel reinforced data selector to select high-quality source domain data to help the TL model. Specifically, the data selector "acts" on the source domain data to find a subset for optimization of the TL model, and the performance of the TL model can provide "rewards" in turn to update the selector. We build the reinforced data selector based on the actor-critic framework and integrate it to a DNN based transfer learning model, resulting in a Reinforced Transfer Learning (RTL) method. We perform a thorough experimental evaluation on two major tasks for text matching, namely, paraphrase identification and natural language inference. Experimental results show the proposed RTL can significantly improve the performance of the TL model. We further investigate different settings of states, rewards, and policy optimization methods to examine the robustness of our method. Last, we conduct a case study on the selected data and find our method is able to select source domain data whose Wasserstein distance is close to the target domain data. This is reasonable and intuitive as such source domain data can provide more transferability power to the model.	2019	https://doi.org/10.1145/3289600.3290978	https://doi.org/10.1145/3289600.3290978	FALSE
Langhans, Philipp and Wieser, Christoph and Bry, Fran\c{c}ois	Crowdsourcing MapReduce: JSMapReduce	JSMapReduce is an implementation of MapReduce which exploits the computing power available in the computers of the users of a web platform by giving tasks to the JavaScript engines of their web browsers. This article describes the implementation of JSMapReduce exploiting HTML 5 features, the heuristics it uses for distributing tasks to workers, and reports on an experimental evaluation of JSMapReduce.	2013	https://doi.org/10.1145/2487788.2487915	https://doi.org/10.1145/2487788.2487915	FALSE
Pasetto, Davide and Franke, Hubertus and Qian, Weihong and Guo, Zhili and Guo, Honglei and Duan, Dongxu and Ni, Yuan and Pan, Yingxin and Bao, Shenghua and Cao, Feng and Su, Zhong	RTS - an Integrated Analytic Solution for Managing Regulation Changes and Their Impact on Business Compliance	Governance, Risk Management and Compliance are key success factors for corporations. Every company worldwide must ensure a proper compliance level with current and future laws and regulations, but managing the dynamic nature of the regulatory environment is a challenge, for both small and medium business as well as large corporations. Specifically the challenge is knowing and interpreting which regulations impact a particular business. Governments and standard bodies keep producing new, revised legislation, and businesses today rely on employees and consultants for tracking and understanding impact on their operations.This paper introduces a novel prototype solution that addresses these concerns through the use of advanced text analytics. In particular the system is able to discover sources of regulatory content on the world wide web, track the changes to these regulations, extract metadata and semantic information and use these to provide a semantically guided comparison of regulation versions. Moreover, by leveraging the IBM DeepQA architecture, the solution is able to cross reference business objectives with the regulatory database and provide insights about the impact of new and revised laws on a company's business.	2013	https://doi.org/10.1145/2482767.2482798	https://doi.org/10.1145/2482767.2482798	FALSE
Vogel, Adam and Jurafsky, Dan	He Said, She Said: Gender in the ACL Anthology	Studies of gender balance in academic computer science are typically based on statistics on enrollment and graduation. Going beyond these coarse measures of gender participation, we conduct a fine-grained study of gender in the field of Natural Language Processing. We use topic models (Latent Dirichlet Allocation) to explore the research topics of men and women in the ACL Anthology Network. We find that women publish more on dialog, discourse, and sentiment, while men publish more than women in parsing, formal semantics, and finite state models. To conduct our study we labeled the gender of authors in the ACL Anthology mostly manually, creating a useful resource for other gender studies. Finally, our study of historical patterns in female participation shows that the proportion of women authors in computational linguistics has been continuously increasing, with approximately a 50% increase in the three decades since 1980.	2012			FALSE
Chen, Zhiyu and Zhang, Shuo and Davison, Brian D.	WTR: A Test Collection for Web Table Retrieval	We describe the development, characteristics and availability of a test collection for the task of Web table retrieval, which uses a large-scale Web Table Corpora extracted from the Common Crawl. Since a Web table usually has rich context information such as the page title and surrounding paragraphs, we not only provide relevance judgments of query-table pairs, but also the relevance judgments of query-table context pairs with respect to a query, which are ignored by previous test collections. To facilitate future research with this benchmark, we provide details about how the dataset is pre-processed and also baseline results from both traditional and recently proposed table retrieval methods. Our experimental results show that proper usage of context labels can benefit previous table retrieval methods.	2021	https://doi.org/10.1145/3404835.3463260	https://doi.org/10.1145/3404835.3463260	FALSE
Dieng, Youssou and Diop, Ibrahima and Faye, Youssou and Malack, Camir A.	A Sub-Regional Information System for Monitoring and Managing PLHIV in Cross-Border Areas between Gambia, Senegal and Guinea Bissau: Information System for Managing PLHIV in Cross-Border Areas	Since aids appeared, it is of common knowledge that geographical spread of HIV is linked to human mobility [1]. However, this relationship between mobility and Aids is both complex and relatively unknown. The FEVE project (Frontiers and Vulnerability to HIV in West Africa), is implemented by ENDA to underline the causal process in order to achieve the UNAIDS 90-90-90 target [2]: by 2020, 90% of the people living with HIV know their HIV status, 90% of the people who know their HIV-positive status are accessing antiretroviral therapy and 90% of the people receiving antiretroviral therapy will have suppressed viral loads. However, as in most African countries, in Senegal health actors generate a large amount of information every day, such as consultation, hospitalization, monitoring infectious diseases, deaths, etc that is recorded in registers. That make difficult their exploitation. This difficulty is compounded when we interested in data related to infectious diseases such as HIV in cross-border areas. The high mobility of the population in these areas poses a great deal of problems in terms of treatment adherence as well as the search for those lost to follow-up. To overcome this problem, we propose a transboundary platform for the monitoring of People Living with HIV (PVVIH). This platform is a web and application which offers not only a sub-regional system of PLHIV management but also a system of communication and capacity building between actors.	2019	https://doi.org/10.1145/3361570.3361576	https://doi.org/10.1145/3361570.3361576	FALSE
Allan, James and Croft, Bruce and Moffat, Alistair and Sanderson, Mark	Frontiers, Challenges, and Opportunities for Information Retrieval	During a three-day workshop in February 2012, 45 Information Retrieval researchers met to discuss long-range challenges and opportunities within the field. The result of the workshop is a diverse set of research directions, project ideas, and challenge areas. This report describes the workshop format, provides summaries of broad themes that emerged, includes brief descriptions of all the ideas, and provides detailed discussion of six proposals that were voted "most interesting" by the participants. Key themes include the need to: move beyond ranked lists of documents to support richer dialog and presentation, represent the context of search and searchers, provide richer support for information seeking, enable retrieval of a wide range of structured and unstructured content, and develop new evaluation methodologies.	2012			FALSE
Allan, James and Croft, Bruce and Moffat, Alistair and Sanderson, Mark	Frontiers, Challenges, and Opportunities for Information Retrieval: Report from SWIRL 2012 the Second Strategic Workshop on Information Retrieval in Lorne	During a three-day workshop in February 2012, 45 Information Retrieval researchers met to discuss long-range challenges and opportunities within the field. The result of the workshop is a diverse set of research directions, project ideas, and challenge areas. This report describes the workshop format, provides summaries of broad themes that emerged, includes brief descriptions of all the ideas, and provides detailed discussion of six proposals that were voted "most interesting" by the participants. Key themes include the need to: move beyond ranked lists of documents to support richer dialog and presentation, represent the context of search and searchers, provide richer support for information seeking, enable retrieval of a wide range of structured and unstructured content, and develop new evaluation methodologies.	2012	https://doi.org/10.1145/2215676.2215678	https://doi.org/10.1145/2215676.2215678	FALSE
Li, Zhenyu and He, Bin and Yu, Xinguo and Hu, Rong	Speech Interaction of Educational Robot Based on Ekho and Sphinx	Speech interaction is a prominent interaction technology in educational robot, and educational robot with speech interaction can have a more harmonic and natural way of interaction. In some specific application scenarios such as education, the performance of speech recognition is not satisfactory due to the wide range of vocabularies and poor network. This paper trains the language model with professional vocabulary in the specific subject and establishes the corresponding phonetic dictionary in speech recognition based on Sphinx, and combines with speech synthesis based on Ekho to implement the speech interaction of educational robot in the offline state, and designs the relevant application of speech interaction about the teaching scenario of Chinese ancient poetry. The experiment demonstrates that the system designed and implemented can meet the demand of practical application of speech interaction in specific subject, and achieve the purpose of enhancing the student's learning experience and improving the learning effect.	2017	https://doi.org/10.1145/3124116.3124119	https://doi.org/10.1145/3124116.3124119	FALSE
Sonntag, Daniel	Medical and Health Systems		2019		https://doi.org/10.1145/3233795.3233808	FALSE
Cappellato, Linda and Ferro, Nicola and Jones, Gareth J.F. and Kamps, Jaap and Mothe, Josiane and Pinel-Sauvagnat, Karen and San Juan, Eric and Savoy, Jacques	Report on CLEF 2015: Experimental IR Meets Multilinguality, Multimodality, and Interaction	This is a report on the sixth edition of the Conference and Labs of the Evaluation Forum (CLEF 2015), held in early September 2015, in Toulouse, France. CLEF was a four day event combining a Conference and an Evaluation Forum. The focus of the Conference is "Experimental IR" as carried out in the CLEF Labs and other evaluation forums, it featured keynotes by Greg Greffenstette, Mounia Lalmas, and Doug Oard, and 43 papers have been presented, covering a wide range of topics. There were a total of eight Labs: eHealth, ImageCLEF, LifeCLEF, Living Labs for IR, NEWSREEL, PAN, QA, and Social Book Search, addressing a wide range of tasks, media, languages, and ways to go beyond standard test collections.	2016	https://doi.org/10.1145/2888422.2888428	https://doi.org/10.1145/2888422.2888428	FALSE
Allen, James and Andr\'{e}, Elisabeth and Cohen, Philip R. and Hakkani-T\"{u}r, Dilek and Kaplan, Ronald and Lemon, Oliver and Traum, David	Challenge Discussion: Advancing Multimodal Dialogue		2019		https://doi.org/10.1145/3233795.3233802	FALSE
Sun, Fu and Li, Feng-Lin and Wang, Ruize and Chen, Qianglong and Cheng, Xingyi and Zhang, Ji	K-AID: Enhancing Pre-Trained Language Models with Domain Knowledge for Question Answering	Knowledge enhanced pre-trained language models (K-PLMs) are shown to be effective for many public tasks in the literature, but few of them have been successfully applied in practice. To address this problem, we propose K-AID, a systematic approach that includes a low-cost knowledge acquisition process for acquiring domain knowledge, an effective knowledge infusion module for improving model performance, and a knowledge distillation component for reducing the model size and deploying K-PLMs on resource-restricted devices (e.g., CPU) for real-world application. Importantly, instead of capturing entity knowledge like the majority of existing K-PLMs, our approach captures relational knowledge, which contributes to better improving sentence-level text classification and text matching tasks that play a key role in question answering (QA). We conducted a set of experiments on five text classification tasks and three text matching tasks from three domains, namely E-commerce, Government, and Film&amp;TV, and performed online A/B tests in E-commerce. Experimental results show that our approach is able to achieve substantial improvement on sentence-level question answering tasks and bring beneficial business value in industrial settings.	2021		https://doi.org/10.1145/3459637.3481930	FALSE
	The Handbook of Multimodal-Multisensor Interfaces: Language Processing, Software, Commercialization, and Emerging Directions	The Handbook of Multimodal-Multisensor Interfaces provides the first authoritative resource on what has become the dominant paradigm for new computer interfaces---user input involving new media (speech, multi-touch, hand and body gestures, facial expressions, writing) embedded in multimodal-multisensor interfaces.This three-volume handbook is written by international experts and pioneers in the field. It provides a textbook, reference, and technology roadmap for professionals working in this and related areas.This third volume focuses on state-of-the-art multimodal language and dialogue processing, including semantic integration of modalities. The development of increasingly expressive embodied agents and robots has become an active test-bed for coordinating multimodal dialogue input and output, including processing of language and nonverbal communication. In addition, major application areas are featured for commercializing multimodal-multisensor systems, including automotive, robotic, manufacturing, machine translation, banking, communications, and others. These systems rely heavily on software tools, data resources, and international standards to facilitate their development. For insights into the future, emerging multimodal-multisensor technology trends are highlighted for medicine, robotics, interaction with smart spaces, and similar topics. Finally, this volume discusses the societal impact of more widespread adoption of these systems, such as privacy risks and how to mitigate them. The handbook chapters provide a number of walk-through examples of system design and processing, information on practical resources for developing and evaluating new systems, and terminology and tutorial support for mastering this emerging field. In the final section of this volume, experts exchange views on a timely and controversial challenge topic, and how they believe multimodal-multisensor interfaces need to be equipped to most effectively advance human performance during the next decade.	2019			FALSE
Antoniadis, Panteleimon and Tambouris, Efthimios	PassBot: A Chatbot for Providing Information on Getting a Greek Passport	An important goal of eGovernment initiatives worldwide is to provide citizens with accurate, personalized and accessible information on Public Services (PS). Chatbots is a promising technology that can be used for that purpose. The aim of this paper is to develop and evaluate a chatbot (namely PassBot) for providing personalized information on the PS “Get a Greek Passport”. For this purpose, the PS was analyzed and different chatbot platforms were evaluated. Based on that, PassBot was developed to enable users to easily and reliably obtain personalized information on getting a Greek passport. The description of the public service was developed according to the European Core Public Service Vocabulary Application Profile (CPSV-AP), which is an extensible and machine-readable data model aiming to harmonise PS descriptions across the EU member states. As CPSV-AP does not inherently support complex PS, it was extended to accommodate personalized PS information provision. The chatbot was used and evaluated with promising results.	2021	https://doi.org/10.1145/3494193.3494233	https://doi.org/10.1145/3494193.3494233	FALSE
Pulido-Prieto, Oscar and Ju\'{a}rez-Mart\'{\i}nez, Ulises	A Survey of Naturalistic Programming Technologies	Mainly focused on solving abstraction problems, programming paradigms limit language expressiveness, thus leaving unexplored natural language descriptions that are implicitly expressive. Several authors have developed tools for programming with a natural language subset limited to specific domains to deal with the ambiguity occurring with artificial intelligence technique use. This article presents a review of tools and languages with naturalistic features and highlights the problems that authors have resolved and those they have not addressed, going on to discuss the fact that a “naturalistic” language based on a well-defined model is not reported.	2017	https://doi.org/10.1145/3109481	https://doi.org/10.1145/3109481	FALSE
Efron, Miles	Query Representation for Cross-Temporal Information Retrieval	This paper addresses the problem of long-term language change in information retrieval (IR) systems. IR research has often ignored lexical drift. But in the emerging domain of massive digitized book collections, the risk of vocabulary mismatch due to language change is high. Collections such as Google Books and the Hathi Trust contain text written in the vernaculars of many centuries. With respect to IR, changes in vocabulary and orthography make 14th-Century English qualitatively different from 21st-Century English. This challenges retrieval models that rely on keyword matching. With this challenge in mind, we ask: given a query written in contemporary English, how can we retrieve relevant documents that were written in early English? We argue that search in historically diverse corpora is similar to cross-language retrieval (CLIR). By considering "modern" English and "archaic" English as distinct languages, CLIR techniques can improve what we call cross-temporal IR (CTIR). We focus on ways to combine evidence to improve CTIR effectiveness, proposing and testing several ways to handle language change during book search. We find that a principled combination of three sources of evidence during relevance feedback yields strong CTIR performance.	2013	https://doi.org/10.1145/2484028.2484054	https://doi.org/10.1145/2484028.2484054	FALSE
Lam, Monica S. and Campagna, Giovanni and Xu, Silei and Fischer, Michael and Moradshahi, Mehrad	Protecting Privacy and Open Competition with Almond: An Open-Source Virtual Assistant	Will Alexa and Google Assistant become the duopoly platforms on which consumers reach web services and IoTs verbally? With open and collaborative research, we can build the best open-source virtual assistant to ensure choice, privacy, and open competition.	2019	https://doi.org/10.1145/3355757	https://doi.org/10.1145/3355757	FALSE
Blanke, Tobias and Bryant, Michael and Frankl, Michal and Kristel, Conny and Speck, Reto and Daelen, Veerle Vanden and Horik, Ren\'{e} Van	The European Holocaust Research Infrastructure Portal	Over the course of the past century, there have been significant changes in the practices of archives driven by the massive increase in the volume of records for archiving, a larger and more diverse user base, and the digital turn. This paper analyses work undertaken by the European Holocaust Research Infrastructure project (EHRI) to develop heritage archives into research infrastructures by connecting their knowledge and making it relevant for research. In the article, we focus on EHRI’s work on an integrated collection portal, acting as a central gateway to the rich information on Holocaust-related sources. At the time of writing, the portal contains over 150,000 descriptions of over 1,850 institutions that hold Holocaust-related archival material in 51 countries. In addition, it hosts concise reports that provide in-depth per-country information about the Holocaust history and archival situation in 47 countries, topic-focused research guides, and a range of other services. The article presents how the EHRI portal work connects to the state of the art of heritage portals and the novel solutions we had to develop to align the portal with the requirements of a research infrastructure.	2017	https://doi.org/10.1145/3004457	https://doi.org/10.1145/3004457	FALSE
Luchev, Detelin and Paneva-Marinova, Desislava and Pavlova-Draganova, Lilia and Pavlov, Radoslav	New Digital Fashion World	The contemporary information space provides a special place for the fashion art exhibitions. The fashion art reflects not only the choice of the society for the clothes, but it is a valuable cultural heritage - a long-term commitment for dress designers. Preserving and presenting this treasure is not new task, but the digital world gives a new vision on fashion creation and performing. In this paper we presented the first attempt in Bulgaria to be developed a multimedia digital library, keeping fashion objects, created by students of the National Art Academy. During the process on fashion study of young people it is often discovered new lines, talented creators, innovative fashion tendencies and new performing ideas that have to be advertised and popularized.	2013	https://doi.org/10.1145/2516775.2516803	https://doi.org/10.1145/2516775.2516803	FALSE
Carmel, David and Chang, Ming-Wei and Gabrilovich, Evgeniy and Hsu, Bo-June (Paul) and Wang, Kuansan	ERD'14: Entity Recognition and Disambiguation Challenge	In this paper we overview the 2014 Entity Recognition and Disambiguation Challenge (ERD'14), which took place from March to June 2014 and was summarized in a dedicated workshop at SIGIR 2014. The main goal of the ERD challenge was to promote research in recognition and disambiguation of entities in unstructured text. Unlike many past entity linking challenges, no mention segmentations were given to the participating systems for a given document. Participants were asked to implement a web service for their system to minimize human involvement during evaluation and to enable measuring the processing times. The challenge has attracted a lot of interest (over 100 teams registered, and 27 of those submitted final results).In this paper we cover the task definition, issues encountered during annotation, and provide a detailed analysis of all the participating systems. Specifically, we show how we adapted the pooling technique to address the difficulties of gathering annotations for the entity linking task. We also summarize the ERD workshop that followed the challenge, including the oral and poster presentations as well as the invited talks.	2014	https://doi.org/10.1145/2701583.2701591	https://doi.org/10.1145/2701583.2701591	FALSE
Yang, Zi and Garduno, Elmer and Fang, Yan and Maiberg, Avner and McCormack, Collin and Nyberg, Eric	Building Optimal Information Systems Automatically: Configuration Space Exploration for Biomedical Information Systems	Software frameworks which support integration and scaling of text analysis algorithms make it possible to build complex, high performance information systems for information extraction, information retrieval, and question answering; IBM's Watson is a prominent example. As the complexity and scaling of information systems become ever greater, it is much more challenging to effectively and efficiently determine which toolkits, algorithms, knowledge bases or other resources should be integrated into an information system in order to achieve a desired or optimal level of performance on a given task. This paper presents a formal representation of the space of possible system configurations, given a set of information processing components and their parameters (configuration space) and discusses algorithmic approaches to determine the optimal configuration within a given configuration space (configuration space exploration or CSE). We introduce the CSE framework, an extension to the UIMA framework which provides a general distributed solution for building and exploring configuration spaces for information systems. The CSE framework was used to implement biomedical information systems in case studies involving over a trillion different configuration combinations of components and parameter values operating on question answering tasks from the TREC Genomics. The framework automatically and efficiently evaluated different system configurations, and identified configurations that achieved better results than prior published results.	2013	https://doi.org/10.1145/2505515.2505692	https://doi.org/10.1145/2505515.2505692	FALSE
Agarwal, Prabal and Str\"{o}tgen, Jannik	Tiwiki: Searching Wikipedia with Temporal Constraints	Temporal information retrieval received a lot of attention during the last years and it is, in the meantime, widely accepted in the IR community that temporal information needs are important to tackle. A particular type of temporal queries are those with explicit temporal constraints, which make almost 15% of today's Web search queries. Although several approaches to allow textual search combined with temporal constraints regarding the content of the documents have been suggested, there are no publicly available search engines allowing for a time-centric search experience.In this paper, we suggest TIWIKI, a time-aware search engine for Wikipedia. Relying on steadily updated Wikipedia dumps annotated with temporal expressions, queries with textual and temporal components can be formulated and are served by ranking the search results based on aggregated values of temporal and textual relevance. As the search results directly link to the original Wikipedia pages, the Tiwiki search engine can be considered as slightly delayed, yet timely access to Wikipedia.	2017	https://doi.org/10.1145/3041021.3051112	https://doi.org/10.1145/3041021.3051112	FALSE
Yin, Zi and Chang, Keng-hao and Zhang, Ruofei	DeepProbe: Information Directed Sequence Understanding and Chatbot Design via Recurrent Neural Networks	Information extraction and user intention identification is a central topic in modern query understanding and recommendation systems. In this paper, we propose DeepProbe, a generic information-directed interaction framework which is built around an attention-based sequence to sequence (seq2seq) recurrent neural network. DeepProbe can rephrase, evaluate, and even actively ask questions, leveraging the generative ability and likelihood estimation made possible by seq2seq models. DeepProbe makes decisions based on a derived uncertainty (entropy) measure conditioned on user inputs, possibly with multiple rounds of interactions. Three applications, namely a rewritter, a relevance scorer and a chatbot for ad recommendation, were built around DeepProbe, with the first two serving as precursory building blocks for the third. We first use the seq2seq model in DeepProbe to rewrite a user query into one of standard query form, which is submitted to an ordinary recommendation system. Secondly, we evaluate DeepProbe's seq2seq model-based relevance scoring. Finally, we build a chatbot prototype capable of making active user interactions, which can ask questions that maximize information gain, allowing for a more efficient user intention idenfication process. We evaluate first two applications by 1) comparing with baselines by BLEU and AUC, and 2) human judge evaluation. Both demonstrate significant improvements compared with current state-of-the-art systems, proving their values as useful tools on their own, and at the same time laying a good foundation for the ongoing chatbot application.	2017	https://doi.org/10.1145/3097983.3098148	https://doi.org/10.1145/3097983.3098148	FALSE
Das, Dipankar	Analysis and Tracking of Emotions in English and Bengali Texts: A Computational Approach	The present discussion highlights the aspects of an ongoing doctoral thesis grounded on the analysis and tracking of emotions from English and Bengali texts. Development of lexical resources and corpora meets the preliminary urgencies. The research spectrum aims to identify the evaluative emotional expressions at word, phrase, sentence, and document level granularities along with their associated holders and topics. Tracking of emotions based on topic or event was carried out by employing sense based affect scoring techniques. The labeled emotion corpora are being prepared from unlabeled examples to cope with the scarcity of emotional resources, especially for the resource constraint language like Bengali. Different unsupervised, supervised and semi-supervised strategies, adopted for coloring each outline of the research spectrum produce satisfactory outcomes	2011	https://doi.org/10.1145/1963192.1963341	https://doi.org/10.1145/1963192.1963341	FALSE
Keuning, Hieke and Jeuring, Johan and Heeren, Bastiaan	A Systematic Literature Review of Automated Feedback Generation for Programming Exercises	Formative feedback, aimed at helping students to improve their work, is an important factor in learning. Many tools that offer programming exercises provide automated feedback on student solutions. We have performed a systematic literature review to find out what kind of feedback is provided, which techniques are used to generate the feedback, how adaptable the feedback is, and how these tools are evaluated. We have designed a labelling to classify the tools, and use Narciss’ feedback content categories to classify feedback messages. We report on the results of coding a total of 101 tools. We have found that feedback mostly focuses on identifying mistakes and less on fixing problems and taking a next step. Furthermore, teachers cannot easily adapt tools to their own needs. However, the diversity of feedback types has increased over the past decades and new techniques are being applied to generate feedback that is increasingly helpful for students.	2018	https://doi.org/10.1145/3231711	https://doi.org/10.1145/3231711	FALSE
Kuhn, Tobias	A Survey and Classification of Controlled Natural Languages	What is here called controlled natural language CNL has traditionally been given many different names. Especially during the last four decades, a wide variety of such languages have been designed. They are applied to improve communication among humans, to improve translation, or to provide natural and intuitive representations for formal notations. Despite the apparent differences, it seems sensible to put all these languages under the same umbrella. To bring order to the variety of languages, a general classification scheme is presented here. A comprehensive survey of existing English-based CNLs is given, listing and describing 100 languages from 1930 until today. Classification of these languages reveals that they form a single scattered cloud filling the conceptual space between natural languages such as English on the one end and formal languages such as propositional logic on the other. The goal of this article is to provide a common terminology and a common model for CNL, to contribute to the understanding of their general nature, to provide a starting point for researchers interested in the area, and to help developers to make design decisions.	2014	https://doi.org/10.1162/COLI_a_00168	https://doi.org/10.1162/COLI_a_00168	FALSE
Badaro, Gilbert and Hajj, Hazem and Habash, Nizar	A Link Prediction Approach for Accurately Mapping a Large-Scale Arabic Lexical Resource to English WordNet	Success of Natural Language Processing (NLP) models, just like all advanced machine learning models, rely heavily on large -scale lexical resources. For English, English WordNet (EWN) is a leading example of a large-scale resource that has enabled advances in Natural Language Understanding (NLU) tasks such as word sense disambiguation, question answering, sentiment analysis, and emotion recognition. EWN includes sets of cognitive synonyms called synsets, which are interlinked by means of conceptual-semantic and lexical relations and where each synset expresses a distinct concept. However, other languages are still lagging behind in having large-scale and rich lexical resources similar to EWN. In this article, we focus on enabling the development of such resources for Arabic. While there have been efforts in developing an Arabic WordNet (AWN), the current version of AWN has its limitations in size and in lacking transliteration standards, which are important for compatibility with Arabic NLP tools. Previous efforts for extending AWN resulted in a lexicon, called ArSenL, that overcame the size and the transliteration standard limitation but was limited in accuracy due to the heuristic approach that only considered surface matching between the English definitions from the Standard Arabic Morphological Analyzer (SAMA) and EWN synset terms, and that resulted in inaccurate mapping of Arabic lemmas to EWN’s synsets. Furthermore, there has been limited exploration of other expansion methods due to expensive manual validation needed. To address these limitations of simultaneously having large-scale size with high accuracy and standard representations, the mapping problem is formulated as a link prediction problem between a large-scale Arabic lexicon and EWN, where a word in one lexicon is linked to a word in another lexicon if the two words are semantically related. We use a semi-supervised approach to create a training dataset by finding common terms in the large-scale Arabic resource and AWN. This set of data becomes implicitly linked to EWN and can be used for training and evaluating prediction models. We propose the use of a two-step Boosting method, where the first step aims at linking English translations of SAMA’s terms to EWN’s synsets. The second step uses surface similarity between SAMA’s glosses and EWN’s synsets. The method results in a new large-scale Arabic lexicon that we call ArSenL 2.0 as a sequel to the previously developed sentiment lexicon ArSenL. A comprehensive study covering both intrinsic and extrinsic evaluations shows the superiority of the method compared to several baseline and state-of-the-art link prediction methods. Compared to previously developed ArSenL, ArSenL 2.0 included a larger set of sentimentally charged adjectives and verbs. It also showed higher linking accuracy on the ground truth data compared to previous ArSenL. For extrinsic evaluation, ArSenL 2.0 was used for sentiment analysis and showed, here, too, higher accuracy compared to previous ArSenL.	2020	https://doi.org/10.1145/3404854	https://doi.org/10.1145/3404854	FALSE
Siqueira, Sean Wolfgand Matsui and Silva, Marcel Ferrante	Session Details: Special Track - Applications and Tools		2015			FALSE
Wu, Guan-Long and Su, Yu-Chuan and Chiu, Tzu-Hsuan and Hsieh, Liang-Chi and Hsu, Winston H.	Scalable Mobile Video Question-Answering System with Locally Aggregated Descriptors and Random Projection	We present a scalable mobile video Question-Answering system with locally aggregated descriptors and random projection using user-generated videos all around the world for ACM Multimedia 2011 Technicolor challenge: "precise event recognition and description from video excerpts." Our proposed system takes a video excerpt as a query and explores its canonical semantics of that. We collect a public events video dataset containing 7 topics with 1963 YouTube videos to evaluate our proposed system. The experiment results show that our proposed video feature representation outperforms a state-of-the-art near-duplicate retrieval based on color histogram. The signature generated by random projection not only ensures real-time efficiency but also achieves a competitive MAP performance with original feature.	2011	https://doi.org/10.1145/2072298.2072409	https://doi.org/10.1145/2072298.2072409	FALSE
Vinogradov, Andrei and Kurshev, Evgeny and Vlasova, Natalia and Podobryaev, Alexey	Information Extraction Tasks in Public Administration Domain: ISIDA-T Natural Language Processing System	This article represents approaches of the artificial intelligence methods, used in public administration. An overview of various technologies of artificial intelligence applications in the field of public administration and related fields is given. All of these research directions are particularly relevant to the task of digital technologies (including artificial intelligence) growth to create an efficient and competitive digital economics in Russia. Among the modern intellectual technologies that allow solving the widest range of tasks, an important role plays technologies related to natural language text processing - nonstructured NL-texts are essential segment of data, used for analysis and decision-making tasks (in terms of data volume, of course, video data have a leading position, however, they are usually suitable for solving tactical but not strategic management tasks). The article provides an overview of the existing methods of natural language processing and their practical application to the tasks of public administration. An integrated approach to the natural language processing tools using for solving practical problems in the field of public administration is considered on the example of the ISIDA-T system for extracting information from natural language texts, developed at the Artificial Intelligence Research Center PSI RAS. The system under consideration is distinguished by a modular approach to the pre-processing of unstructured text and the possibility of manual adjustment for a specific extraction task. This technological solution gives the necessary flexibility and ease of use. The system consists of configurable text preprocessing, linguistic analysis, target information retrieval and output of the results in user-friendly form modules. One of the important components of the system is an integrated knowledge resource that allows quickly and efficiently adjust the system to the specifics of the relevant subject area. An approach to the application of the ISIDA-T system to the task of fact information extraction (as the most important for analyzing the situation and subsequent management decisions) is proposed using the example of information extraction about resignations and appointments from news feeds.	2019	https://doi.org/10.1145/3373722.3373782	https://doi.org/10.1145/3373722.3373782	FALSE
Bohus, Dan and Horvitz, Eric	Situated Interaction		2019		https://doi.org/10.1145/3233795.3233800	FALSE
Poesio, Massimo and Chamberlain, Jon and Kruschwitz, Udo and Robaldo, Livio and Ducceschi, Luca	Phrase Detectives: Utilizing Collective Intelligence for Internet-Scale Language Resource Creation	We are witnessing a paradigm shift in Human Language Technology (HLT) that may well have an impact on the field comparable to the statistical revolution: acquiring large-scale resources by exploiting collective intelligence. An illustration of this new approach is Phrase Detectives, an interactive online game with a purpose for creating anaphorically annotated resources that makes use of a highly distributed population of contributors with different levels of expertise.The purpose of this article is to first of all give an overview of all aspects of Phrase Detectives, from the design of the game and the HLT methods we used to the results we have obtained so far. It furthermore summarizes the lessons that we have learned in developing this game which should help other researchers to design and implement similar games.	2013	https://doi.org/10.1145/2448116.2448119	https://doi.org/10.1145/2448116.2448119	FALSE
Steedman, Mark	The Lost Combinator		2018	https://doi.org/10.1162/coli_a_00328	https://doi.org/10.1162/coli_a_00328	FALSE
Li, Tongliang and Fang, Lei and Lou, Jian-Guang and Li, Zhoujun and Zhang, Dongmei	AnaSearch: Extract, Retrieve and Visualize Structured Results from Unstructured Text for Analytical Queries	Modern search engines retrieve results mainly based on the keyword matching techniques, and thus fail to answer analytical queries like "apps with more than 1 billion monthly active users" or "population growth of the US from 2015 to 2019", which requires numerical reasoning or aggregating results from multiple web pages. Such analytical queries are very common in the data analysis area, the expected results would be structured tables or charts. In most cases, these structured results are not available or accessible, they scatter in various text sources. In this work, we build AnaSearch, a search system to support analytical queries, and return structured results that can be visualized in the form of tables or charts. We collect and build structured quantitative data from the unstructured text on the web automatically. With AnaSearch, data analysts could easily derive insights for decision making with keyword or natural language queries. Specifically, we build AnaSearch under the COVID-19 news data, which makes it easy to compare with manually collected structured data.	2021	https://doi.org/10.1145/3437963.3441694	https://doi.org/10.1145/3437963.3441694	FALSE
	Declarative Logic Programming: Theory, Systems, and Applications	Logic Programming (LP) is at the nexus of knowledge representation, AI, mathematical logic, databases, and programming languages. It allows programming to be more declarative, by specifying “what” to do instead of “how” to do it. This field is fascinating and intellectually stimulating due to the fundamental interplay among theory, systems, and applications brought about by logic. Several books cover the basics of LP but they focus mostly on the Prolog language. There is generally a lack of accessible collections of articles covering the key aspects of LP, such as the well-founded vs. stable semantics for negation, constraints, object-oriented LP, updates, probabilistic LP, and implementation methods, including top-down vs. bottom-up evaluation and tabling.For systems, the situation is even less satisfactory, lacking expositions of LP inference machinery that supports tabling and other state-of-the-art implementation techniques. There is also a dearth of articles about systems that support truly declarative languages, especially those that tie into first-order logic, mathematical programming, and constraint programming. Also rare are surveys of challenging application areas of LP, such as bioinformatics, natural language processing, verification, and planning, as well as analysis of LP applications based on language abstractions and implementations methods.The goal of this book is to help fill in the void in the literature with state-of-the-art surveys on key aspects of LP. Much attention was paid to making these surveys accessible to researchers, practitioners, and graduate students alike. 	2018			FALSE
My-Thanh Nguyen, Thi and Hai Diep, Thanh and Bien Ngo, Bac and Bich Le, Ngoc and Quy Dao, Xuan	Design of Online Learning Platform with Vietnamese Virtual Assistant	This paper proposes an online learning platform with a Vietnamese Virtual Assistant (VVA) that is utilized to help instructors in presenting lessons as well as testing and assessing learners. Currently, in online learning platforms, the lesson content is recorded in videos. In the proposed approach, the lesson content is delivered through slides (PDF format) combined with the instructor's voice and face that are synthesized from the text (TTS: Text-to-Speech and SDF: Speech-driven-Face). In addition, our approach ensures the highest quality in displaying the lesson content and allows us to edit the lesson content directly from text without recording video again as the current way.	2021	https://doi.org/10.1145/3460179.3460188	https://doi.org/10.1145/3460179.3460188	FALSE
Feld, Michael and Neβelrath, Robert and Schwartz, Tim	Software Platforms and Toolkits for Building Multimodal Systems and Applications		2019		https://doi.org/10.1145/3233795.3233801	FALSE
Kir\'{a}ly, P\'{e}ter	Towards an Extensible Measurement of Metadata Quality	This paper describes the structure of an extensible metadata quality assessment framework, which supports multiple metadata schemas, and is flexible enough to work with new schemas. The software has to be scalable to be able to process huge amount of metadata records within a reasonable time. Fundamental requirements that need to be considered during the design of such a software are i) the abstraction of the metadata schema (in the context of the measurement process), ii) how to address distinct parts within metadata records, iii) the workflow of the measurement, iv) a common and powerful interface for the individual metrics, and v) interoperability with Java and REST APIs.	2017	https://doi.org/10.1145/3078081.3078109	https://doi.org/10.1145/3078081.3078109	FALSE
	Preface		2019		https://doi.org/10.1145/3233795.3233796	FALSE
Skantze, Gabriel and Gustafson, Joakim and Beskow, Jonas	Multimodal Conversational Interaction with Robots		2019		https://doi.org/10.1145/3233795.3233799	FALSE
	Introduction: Toward the Design, Construction, and Deployment of Multimodal-Multisensor Interfaces		2019		https://doi.org/10.1145/3233795.3233797	FALSE
Hossain, MD. Zakir and Sohel, Ferdous and Shiratuddin, Mohd Fairuz and Laga, Hamid	A Comprehensive Survey of Deep Learning for Image Captioning	Generating a description of an image is called image captioning. Image captioning requires recognizing the important objects, their attributes, and their relationships in an image. It also needs to generate syntactically and semantically correct sentences. Deep-learning-based techniques are capable of handling the complexities and challenges of image captioning. In this survey article, we aim to present a comprehensive review of existing deep-learning-based image captioning techniques. We discuss the foundation of the techniques to analyze their performances, strengths, and limitations. We also discuss the datasets and the evaluation metrics popularly used in deep-learning-based automatic image captioning.	2019	https://doi.org/10.1145/3295748	https://doi.org/10.1145/3295748	FALSE
	Index		2019		https://doi.org/10.1145/3233795.3233814	FALSE
Guo, Jiafeng and Cai, Yinqiong and Fan, Yixing and Sun, Fei and Zhang, Ruqing and Cheng, Xueqi	Semantic Models for the First-Stage Retrieval: A Comprehensive Review	Multi-stage ranking pipelines have been a practical solution in modern search systems, where the first-stage retrieval is to return a subset of candidate documents and latter stages attempt to re-rank those candidates. Unlike re-ranking stages going through quick technique shifts over the past decades, the first-stage retrieval has long been dominated by classical term-based models. Unfortunately, these models suffer from the vocabulary mismatch problem, which may block re-ranking stages from relevant documents at the very beginning. Therefore, it has been a long-term desire to build semantic models for the first-stage retrieval that can achieve high recall efficiently. Recently, we have witnessed an explosive growth of research interests on the first-stage semantic retrieval models. We believe it is the right time to survey current status, learn from existing methods, and gain some insights for future development. In this article, we describe the current landscape of the first-stage retrieval models under a unified framework to clarify the connection between classical term-based retrieval methods, early semantic retrieval methods, and neural semantic retrieval methods. Moreover, we identify some open challenges and envision some future directions, with the hope of inspiring more research on these important yet less investigated topics.	2022	https://doi.org/10.1145/3486250	https://doi.org/10.1145/3486250	FALSE
Cohen, Philip R. and Tumuluri, Raj	Commercialization of Multimodal Systems		2019		https://doi.org/10.1145/3233795.3233812	FALSE
Tumuluri, Raj and Dahl, Deborah and Patern\`{o}, Fabio and Zancanaro, Massimo	Standardized Representations and Markup Languages for Multimodal Interaction		2019		https://doi.org/10.1145/3233795.3233806	FALSE
Saha, Diptikalyan and Floratou, Avrilia and Sankaranarayanan, Karthik and Minhas, Umar Farooq and Mittal, Ashish R. and \"{O}zcan, Fatma	ATHENA: An Ontology-Driven System for Natural Language Querying over Relational Data Stores	In this paper, we present ATHENA, an ontology-driven system for natural language querying of complex relational databases. Natural language interfaces to databases enable users easy access to data, without the need to learn a complex query language, such as SQL. ATHENA uses domain specific ontologies, which describe the semantic entities, and their relationships in a domain. We propose a unique two-stage approach, where the input natural language query (NLQ) is first translated into an intermediate query language over the ontology, called OQL, and subsequently translated into SQL. Our two-stage approach allows us to decouple the physical layout of the data in the relational store from the semantics of the query, providing physical independence. Moreover, ontologies provide richer semantic information, such as inheritance and membership relations, that are lost in a relational schema. By reasoning over the ontologies, our NLQ engine is able to accurately capture the user intent. We study the effectiveness of our approach using three different workloads on top of geographical (GEO), academic (MAS) and financial (FIN) data. ATHENA achieves 100% precision on the GEO and MAS workloads, and 99% precision on the FIN workload which operates on a complex financial ontology. Moreover, ATHENA attains 87.2%, 88.3%, and 88.9% recall on the GEO, MAS, and FIN workloads, respectively.	2016	https://doi.org/10.14778/2994509.2994536	https://doi.org/10.14778/2994509.2994536	FALSE
LIU, Yiqun and Kato, Makoto P. and Clarke, Charles L.A. and Kando, Noriko and Sakai, Tetsuya	Report on NTCIR-13: The Thirteenth Round of NII Testbeds and Community for Information Access Research	This is a report on the NTCIR-13 conference held in December 2017, in Tokyo, Japan. NTCIR is a series of parallel and collective evaluation efforts designed to enhance research on diverse information access technologies, including, but not limited to, cross-language and multimedia information access, question-answering, text mining and summarization, with an emphasis on East Asian languages such as Chinese, Korean, and Japanese, as well as English. 105 different research groups from 20 countries/regions participated in one or more of the nine different tasks in NTCIR-13, to compete and collaborate on a common ground and thereby advance the state of the art. This report introduces the highlights of the conference, describes the scope and task designs of nine tasks organized at NTCIR-13, and provides a brief introduction to NTCIR-14, which started from January 2018 and will be closed in June 2019, which will be the 20th anniversary since the first NTCIR Conference in the summer of 1999.	2018	https://doi.org/10.1145/3274784.3274791	https://doi.org/10.1145/3274784.3274791	FALSE
Qazvinian, Vahed and Radev, Dragomir R. and Mohammad, Saif M. and Dorr, Bonnie and Zajic, David and Whidby, Michael and Moon, Taesun	Generating Extractive Summaries of Scientific Paradigms	Researchers and scientists increasingly find themselves in the position of having to quickly understand large amounts of technical material. Our goal is to effectively serve this need by using bibliometric text mining and summarization techniques to generate summaries of scientific literature. We show how we can use citations to produce automatically generated, readily consumable, technical extractive summaries. We first propose C-LexRank, a model for summarizing single scientific articles based on citations, which employs community detection and extracts salient information-rich sentences. Next, we further extend our experiments to summarize a set of papers, which cover the same scienti fic topic. We generate extractive summaries of a set of Question Answering (QA) and Dependency Parsing (DP) papers, their abstracts, and their citation sentences and show that citations have unique information amenable to creating a summary.	2013			FALSE
Mukherjee, Subhabrata and Ajmera, Jitendra and Joshi, Sachindra	Domain Cartridge: Unsupervised Framework for Shallow Domain Ontology Construction from Corpus	In this work we propose an unsupervised framework to construct a shallow domain ontology from corpus. It is essential for Information Retrieval systems, Question-Answering systems, Dialogue etc. to identify important concepts in the domain and the relationship between them. We identify important domain terms of which multi-words form an important component. We show that the incorporation of multi-words improves parser performance, resulting in better parser output, which improves the performance of an existing Question-Answering system by upto 7%. On manually annotated smartphone dataset, the proposed system identifies 40:87% of the domain terms, compared to 22% recall obtained using WordNet, 43:77% by Yago and 53:74% by BabelNet respectively. However, it does not use any manually annotated resource like the compared systems. Thereafter, we propose a framework to construct a shallow ontology from the discovered domain terms by identifying four domain relations namely, Synonyms ('similar-to'), Type-Of ('is-a'), Action-On ('methods') and Feature-Of ('attributes'), where we achieve significant performance improvement over WordNet, BabelNet and Yago without using any mode of supervision or manual annotation.	2014	https://doi.org/10.1145/2661829.2662087	https://doi.org/10.1145/2661829.2662087	FALSE
Banerjee, Somnath and Kuila, Alapan and Roy, Aniruddha and Naskar, Sudip Kumar and Rosso, Paolo and Bandyopadhyay, Sivaji	A Hybrid Approach for Transliterated Word-Level Language Identification: CRF with Post-Processing Heuristics	In this paper, we describe a hybrid approach for word-level language (WLL) identification of Bangla words written in Roman script and mixed with English words as part of our participation in the shared task on transliterated search at Forum for Information Retrieval Evaluation (FIRE) in 2014. A CRF based machine learning model and post-processing heuristics are employed for the WLL identification task. In addition to language identification, two transliteration systems were built to transliterate detected Bangla words written in Roman script into native Bangla script. The system demonstrated an overall token level language identification accuracy of 0.905. The token level Bangla and English language identification F-scores are 0.899, 0.920 respectively. The two transliteration systems achieved accuracies of 0.062 and 0.037. The word-level language identification system presented in this paper resulted in the best scores across almost all metrics among all the participating systems for the Bangla-English language pair.	2014	https://doi.org/10.1145/2824864.2824876	https://doi.org/10.1145/2824864.2824876	FALSE
Neal, Tempestt and Sundararajan, Kalaivani and Fatima, Aneez and Yan, Yiming and Xiang, Yingfei and Woodard, Damon	Surveying Stylometry Techniques and Applications	The analysis of authorial style, termed stylometry, assumes that style is quantifiably measurable for evaluation of distinctive qualities. Stylometry research has yielded several methods and tools over the past 200 years to handle a variety of challenging cases. This survey reviews several articles within five prominent subtasks: authorship attribution, authorship verification, authorship profiling, stylochronometry, and adversarial stylometry. Discussions on datasets, features, experimental techniques, and recent approaches are provided. Further, a current research challenge lies in the inability of authorship analysis techniques to scale to a large number of authors with few text samples. Here, we perform an extensive performance analysis on a corpus of 1,000 authors to investigate authorship attribution, verification, and clustering using 14 algorithms from the literature. Finally, several remaining research challenges are discussed, along with descriptions of various open-source and commercial software that may be useful for stylometry subtasks.	2017	https://doi.org/10.1145/3132039	https://doi.org/10.1145/3132039	FALSE
Hornung, Rachel and Chen, Nutan and van der Smagt, Patrick	Early Integration for Movement Modeling in Latent Spaces		2019		https://doi.org/10.1145/3233795.3233805	FALSE
Schnelle-Walka, Dirk and Radomski, Stefan	Automotive Multimodal Human-Machine Interface		2019		https://doi.org/10.1145/3233795.3233809	FALSE
Heloir, Alexis and Nunnari, Fabrizio and Bachynskyi, Myroslav	Ergonomics for the Design of Multimodal Interfaces		2019		https://doi.org/10.1145/3233795.3233804	FALSE
Snider, Sharon and Scott II, Willie L. and Trewin, Shari	Accessibility Information Needs in the Enterprise	We describe the questions asked about accessibility, both through information searches and direct queries, within a large multinational corporation over a period of two years, finding an emphasis on topics covering enterprise requirements for testing, recording, and reporting compliance. Our analysis finds that up to 66% of these questions may be answerable by an accessibility ontology, but only 26% of the terms in the questions are concepts found in existing available accessibility ontologies. To fill this gap, we introduce the Enterprise Accessibility Conformance Ontology, which extends previous ontologies to include the relevant concepts. We demonstrate the use of the ontology to provide a unifying model of the accessibility domain that contributed to a 22% performance improvement for a question-answering accessibility conformance chatbot.	2020	https://doi.org/10.1145/3368620	https://doi.org/10.1145/3368620	FALSE
Valstar, Michel	Multimodal Databases		2019		https://doi.org/10.1145/3233795.3233807	FALSE
Kalra, Sumit and Prabhakar, T. V.	Ontology-Based Framework for Internal-External Quality Trade-Offs and Tenant Management in Multi-Tenant Applications	Software Quality Attributes (QAs) can be categorized as either internal to the system as experienced by the developers or external to the system perceived by the end users. These QA categories have trade-off among them - an emphasis on internal QA may result in a compromise of an external QA. For example, there is a trade-off between maintainability and performance. Model-driven development approaches manage this trade-off and increase the degree of internal QA maintainability. In this work, we propose an ontology-based communication mechanism among software components to handle the trade-off. The approach increases the degree of internal QAs such as modifiability, maintainability, testability during the design and development phases without compromising the external QAs for the end users during the operation phase. We also evaluate a prototype system to validate the proposed approach using Software Architecture Analysis Method (SAAM). It is also easier to integrate into the software development lifecycle as compared to existing model-driven approaches. The internal quality attributes become more significant in a multi-tenant scenario than conventional software. It requires managing dynamic requirements of tenants continuously. The proposed approach also useful in such scenario to reduce the maintenance overhead without compromising the degree of multi-tenancy.	2018	https://doi.org/10.1145/3183628.3183632	https://doi.org/10.1145/3183628.3183632	FALSE
Liu, Xin and Pan, Haojie and He, Mutian and Song, Yangqiu and Jiang, Xin and Shang, Lifeng	Neural Subgraph Isomorphism Counting	In this paper, we study a new graph learning problem: learning to count subgraph isomorphisms. Different from other traditional graph learning problems such as node classification and link prediction, subgraph isomorphism counting is NP-complete and requires more global inference to oversee the whole graph. To make it scalable for large-scale graphs and patterns, we propose a learning framework that augments different representation learning architectures and iteratively attends pattern and target data graphs to memorize intermediate states of subgraph isomorphism searching for global counting. We develop both small graphs (&lt;= 1,024 subgraph isomorphisms in each) and large graphs (&lt;= 4,096 subgraph isomorphisms in each) sets to evaluate different representation and interaction modules. A mutagenic compound dataset, MUTAG, is also used to evaluate neural models and demonstrate the success of transfer learning. While the learning based approach is inexact, we are able to generalize to count large patterns and data graphs in linear time compared to the exponential time of the original NP-complete problem. Experimental results show that learning based subgraph isomorphism counting can speed up the traditional algorithm, VF2, 10-1,000 times with acceptable errors. Domain adaptation based on fine-tuning also shows the usefulness of our approach in real-world applications.	2020	https://doi.org/10.1145/3394486.3403247	https://doi.org/10.1145/3394486.3403247	FALSE
Jiang, Zhuoren and Gao, Liangcai and Yuan, Ke and Gao, Zheng and Tang, Zhi and Liu, Xiaozhong	Mathematics Content Understanding for Cyberlearning via Formula Evolution Map	Although the scientific digital library is growing at a rapid pace, scholars/students often find reading Science, Technology, Engineering, and Mathematics (STEM) literature daunting, especially for the math-content/formula. In this paper, we propose a novel problem, "mathematics content understanding", for cyberlearning and cyberreading. To address this problem, we create a Formula Evolution Map (FEM) offline and implement a novel online learning/reading environment, PDF Reader with Math-Assistant (PRMA), which incorporates innovative math-scaffolding methods. The proposed algorithm/system can auto-characterize student emerging math-information need while reading a paper and enable students to readily explore the formula evolution trajectory in FEM. Based on a math-information need, PRMA utilizes innovative joint embedding, formula evolution mining, and heterogeneous graph mining algorithms to recommend high quality Open Educational Resources (OERs), e.g., video, Wikipedia page, or slides, to help students better understand the math-content in the paper. Evaluation and exit surveys show that the PRMA system and the proposed formula understanding algorithm can effectively assist master and PhD students better understand the complex math-content in the class readings.	2018	https://doi.org/10.1145/3269206.3271694	https://doi.org/10.1145/3269206.3271694	FALSE
Cafaro, Angelo and Pelachaud, Catherine and Marsella, Stacy C.	Nonverbal Behavior in Multimodal Performances		2019		https://doi.org/10.1145/3233795.3233803	FALSE
Friedland, Gerald and Tschantz, Michael Carl	Privacy Concerns of Multimodal Sensor Systems		2019		https://doi.org/10.1145/3233795.3233813	FALSE
Arampatzis, Avi and Cappellato, Linda and Eickhoff, Carsten and Ferro, Nicola and Joho, Hideo and Kanoulas, Evangelos and Lioma, Christina and N\'{e}v\'{e}ol, Aur\'{e}lie and Tsikrika, Theodora and Vrochidis, Stefanos	Report on CLEF 2020	This is a report on the tenth edition of the Conference and Labs of the Evaluation Forum (CLEF 2020), (virtually) held from September 22--25, 2020, in Thessaloniki, Greece.CLEF was a four day event combining a Conference and an Evaluation Forum. The Conference featured keynotes by Ellen Voorhees and Yiannis Kompasiaris, and presentation of peer reviewed research papers covering a wide range of topics in addition to many posters. The Evaluation Forum consisted to twelve Labs: ARQMath, BioASQ, CheckThat!, ChEMU, CLEF eHealth, eRisk, HIPE, ImageCLEF, LifeCLEF, LiLAS, PAN, and Touch\'{e}, addressing a wide range of tasks, media, languages, and ways to go beyond standard test collections.	2021	https://doi.org/10.1145/3483382.3483396	https://doi.org/10.1145/3483382.3483396	FALSE
Kirchner, Elsa A. and Fairclough, Stephen H. and Kirchner, Frank	Embedded Multimodal Interfaces in Robotics: Applications, Future Trends, and Societal Implications		2019		https://doi.org/10.1145/3233795.3233810	FALSE
Johnston, Michael	Multimodal Integration for Interactive Conversational Systems		2019		https://doi.org/10.1145/3233795.3233798	FALSE
Poesio, Massimo and Barbu, Eduard and Stemle, Egon W. and Girardi, Christian	Structure-Preserving Pipelines for Digital Libraries	Most existing HLT pipelines assume the input is pure text or, at most, HTML and either ignore (logical) document structure or remove it. We argue that identifying the structure of documents is essential in digital library and other types of applications, and show that it is relatively straightforward to extend existing pipelines to achieve ones in which the structure of a document is preserved.	2011			FALSE
Fei, Hongliang and Tan, Shulong and Li, Ping	Hierarchical Multi-Task Word Embedding Learning for Synonym Prediction	Automatic synonym recognition is of great importance for entity-centric text mining and interpretation. Due to the high language use variability in real-life, manual construction of semantic resources to cover all synonyms is prohibitively expensive and may also result in limited coverage. Although there are public knowledge bases, they only have limited coverage for languages other than English. In this paper, we focus on medical domain and propose an automatic way to accelerate the process of medical synonymy resource development for Chinese, including both formal entities from healthcare professionals and noisy descriptions from end-users. Motivated by the success of distributed word representations, we design a multi-task model with hierarchical task relationship to learn more representative entity/term embeddings and apply them to synonym prediction. In our model, we extend the classical skip-gram word embedding model by introducing an auxiliary task "neighboring word semantic type prediction'' and hierarchically organize them based on the task complexity. Meanwhile, we incorporate existing medical term-term synonymous knowledge into our word embedding learning framework. We demonstrate that the embeddings trained from our proposed multi-task model yield significant improvement for entity semantic relatedness evaluation, neighboring word semantic type prediction and synonym prediction compared with baselines. Furthermore, we create a large medical text corpus in Chinese that includes annotations for entities, descriptions and synonymous pairs for future research in this direction.	2019	https://doi.org/10.1145/3292500.3330914	https://doi.org/10.1145/3292500.3330914	FALSE
Lim, Ee-Peng and Chen, Hsinchun and Chen, Guoqing	Business Intelligence and Analytics: Research Directions	Business intelligence and analytics (BIA) is about the development of technologies, systems, practices, and applications to analyze critical business data so as to gain new insights about business and markets. The new insights can be used for improving products and services, achieving better operational efficiency, and fostering customer relationships. In this article, we will categorize BIA research activities into three broad research directions: (a) big data analytics, (b) text analytics, and (c) network analytics. The article aims to review the state-of-the-art techniques and models and to summarize their use in BIA applications. For each research direction, we will also determine a few important questions to be addressed in future research.	2013	https://doi.org/10.1145/2407740.2407741	https://doi.org/10.1145/2407740.2407741	FALSE
Wang, Li and Zhu, Wei and Jiang, Sihang and Zhang, Sheng and Wang, Keqiang and Ni, Yuan and Xie, Guotong and Xiao, Yanghua	Mining Infrequent High-Quality Phrases from Domain-Specific Corpora	Phrase mining is a fundamental task for text analysis and has various downstream applications such as named entity recognition, topic modeling, and relation extraction. In this paper, we focus on mining high-quality phrases from domain-specific corpora with special consideration of infrequent ones. Previous methods might miss infrequent high-quality phrases in the candidate selection stage. And these methods rely on explicit features to mine phrases while rarely considering the implicit features. In addition, completeness is rarely explicitly considered in the evaluation of a high-quality phrase. In this paper, we propose a novel approach that exploits a sequence labeling model to capture infrequent phrases. And we employ implicit semantic features and contextual POS tag statistics to measure meaningfulness and completeness, respectively. Experiments over four real-world corpora demonstrate that our method achieves significant improvements over previous state-of-the-art methods across different domains and languages.	2020	https://doi.org/10.1145/3340531.3412029	https://doi.org/10.1145/3340531.3412029	FALSE
Grappy, Arnaud and Grau, Brigitte and Rosset, Sophie	Methods Combination and ML-Based Re-Ranking of Multiple Hypothesis for Question-Answering Systems	Question answering systems answer correctly to different questions because they are based on different strategies. In order to increase the number of questions which can be answered by a single process, we propose solutions to combine two question answering systems, QAVAL and RITEL. QAVAL proceeds by selecting short passages, annotates them by question terms, and then extracts from them answers which are ordered by a machine learning validation process. RITEL develops a multi-level analysis of questions and documents. Answers are extracted and ordered according to two strategies: by exploiting the redundancy of candidates and a Bayesian model. In order to merge the system results, we developed different methods either by merging passages before answer ordering, or by merging end-results. The fusion of end-results is realized by voting, merging, and by a machine learning process on answer characteristics, which lead to an improvement of the best system results of 19 %.	2012			FALSE
	The Handbook of Multimodal-Multisensor Interfaces: Signal Processing, Architectures, and Detection of Emotion and Cognition - Volume 2	The Handbook of Multimodal-Multisensor Interfaces provides the first authoritative resource on what has become the dominant paradigm for new computer interfaces: user input involving new media (speech, multi-touch, hand and body gestures, facial expressions, writing) embedded in multimodal-multisensor interfaces that often include biosignals.This edited collection is written by international experts and pioneers in the field. It provides a textbook, reference, and technology roadmap for professionals working in this and related areas.This second volume of the handbook begins with multimodal signal processing, architectures, and machine learning. It includes recent deep-learning approaches for processing multisensorial and multimodal user data and interaction, as well as context-sensitivity. A further highlight is processing of information about users' states and traits, an exciting emerging capability in next-generation user interfaces. These chapters discuss real-time multimodal analysis of emotion and social signals from various modalities and perception of affective expression by users. Further chapters discuss multimodal processing of cognitive state using behavioral and physiological signals to detect cognitive load, domain expertise, deception, and depression. This collection of chapters provides walk-through examples of system design and processing, information on tools and practical resources for developing and evaluating new systems, and terminology, and tutorial support for mastering this rapidly expanding field. In the final section of this volume, experts exchange views on the timely and controversial challenge topic of multimodal deep learning. The discussion focuses on how multimodal-multisensor interfaces are most likely to advance human performance during the next decade.	2018			FALSE
	Making Databases Work: The Pragmatic Wisdom of Michael Stonebraker	At the ACM Awards banquet in June 2017, during the 50th anniversary celebration of the A.M. Turing Award, ACM announced the launch of the ACM A.M. Turing Book Series, a sub-series of ACM Books, to celebrate the winners of the A.M. Turing Award, computing's highest honor, the "Nobel Prize" for computing. This series aims to highlight the accomplishments of awardees, explaining their major contributions of lasting importance in computing."Making Databases Work: The Pragmatic Wisdom of Michael Stonebraker," the first book in the series, celebrates Mike's contributions and impact. What accomplishments warranted computing's highest honor? How did Stonebraker do it? Who is Mike Stonebraker---researcher, professor, CTO, lecturer, innovative product developer, serial entrepreneur, and decades-long leader, and research evangelist for the database community. This book describes Mike's many contributions and evaluates them in light of the Turing Award.The book describes, in 36 chapters, the unique nature, significance, and impact of Mike's achievements in advancing modern database systems over more than 40 years. The stories involve technical concepts, projects, people, prototype systems, failures, lucky accidents, crazy risks, startups, products, venture capital, and lots of applications that drove Mike Stonebraker's achievements and career. Even if you have no interest in databases at all, you'll gain insights into the birth and evolution of Turing Award-worthy achievements from the perspectives of 39 remarkable computer scientists and professionals.Today, data is considered the world's most valuable resource ("The Economist," May 6, 2017), whether it is in the tens of millions of databases used to manage the world's businesses and governments, in the billions of databases in our smartphones and watches, or residing elsewhere, as yet unmanaged, awaiting the elusive next generation of database systems. Every one of the millions or billions of databases includes features that are celebrated by the 2014 A.M. Turing Award and are described in this book.	2018			FALSE
G\"{u}nther, Michael and Sikorski, Paul and Thiele, Maik and Lehner, Wolfgang	FacetE: Exploiting Web Tables for Domain-Specific Word Embedding Evaluation	Today's natural language processing and information retrieval systems heavily depend on word embedding techniques to represent text values. However, given a specific task deciding for a word embedding dataset is not trivial. Current word embedding evaluation methods mostly provide only a one-dimensional quality measure, which does not express how knowledge from different domains is represented in the word embedding models. To overcome this limitation, we provide a new evaluation data set called FacetE derived from 125M Web tables, enabling domain-sensitive evaluation. We show that FacetE can effectively be used to evaluate word embedding models. The evaluation of common general-purpose word embedding models suggests that there is currently no best word embedding for every domain.	2020	https://doi.org/10.1145/3395032.3395325	https://doi.org/10.1145/3395032.3395325	FALSE
Shutova, Ekaterina and Teufel, Simone and Korhonen, Anna	Statistical Metaphor Processing	Metaphor is highly frequent in language, which makes its computational processing indispensable for real-world NLP applications addressing semantic tasks. Previous approaches to metaphor modeling rely on task-specific hand-coded knowledge and operate on a limited domain or a subset of phenomena. We present the first integrated open-domain statistical model of metaphor processing in unrestricted text. Our method first identifies metaphorical expressions in running text and then paraphrases them with their literal paraphrases. Such a text-to-text model of metaphor interpretation is compatible with other NLP applications that can benefit from metaphor resolution. Our approach is minimally supervised, relies on the state-of-the-art parsing and lexical acquisition technologies distributional clustering and selectional preference induction, and operates with a high accuracy.	2013	https://doi.org/10.1162/COLI_a_00124	https://doi.org/10.1162/COLI_a_00124	FALSE
Or\u{a}san, Constantin	Book Review: Interactive Multi-Modal Question-Answering Antal van Den Bosch and Gosse Bouma (Editors) (*Tilburg University and University of Groningen) Berlin: Springer (Theory and Applications of Natural Language Processing Series, Edited by Eduard Hovy), 2011, Xii+279 Pp; Hardbound, Isbn 978-3-642-17524-4, $124.00; e-Book, Isbn 978-3-642-17525-1; Paperbound, $24.95 or £24.95 to Members of Subscribing Institutions		2012	https://doi.org/10.1162/COLI_r_00104	https://doi.org/10.1162/COLI_r_00104	FALSE
Alkwai, Lulwah M. and Nelson, Michael L. and Weigle, Michele C.	Comparing the Archival Rate of Arabic, English, Danish, and Korean Language Web Pages	It has long been suspected that web archives and search engines favor Western and English language webpages. In this article, we quantitatively explore how well indexed and archived Arabic language webpages are as compared to those from other languages. We began by sampling 15,092 unique URIs from three different website directories: DMOZ (multilingual), Raddadi, and Star28 (the last two primarily Arabic language). Using language identification tools, we eliminated pages not in the Arabic language (e.g., English-language versions of Aljazeera pages) and culled the collection to 7,976 Arabic language webpages. We then used these 7,976 pages and crawled the live web and web archives to produce a collection of 300,646 Arabic language pages. We compared the analysis of Arabic language pages with that of English, Danish, and Korean language pages. First, for each language, we sampled unique URIs from DMOZ; then, using language identification tools, we kept only pages in the desired language. Finally, we crawled the archived and live web to collect a larger sample of pages in English, Danish, or Korean. In total for the four languages, we analyzed over 500,000 webpages. We discovered: (1) English has a higher archiving rate than Arabic, with 72.04% archived. However, Arabic has a higher archiving rate than Danish and Korean, with 53.36% of Arabic URIs archived, followed by Danish and Korean with 35.89% and 32.81% archived, respectively. (2) Most Arabic and English language pages are located in the United States; only 14.84% of the Arabic URIs had an Arabic country code top-level domain (e.g., sa) and only 10.53% had a GeoIP in an Arabic country. Most Danish-language pages were located in Denmark, and most Korean-language pages were located in South Korea. (3) The presence of a webpage in a directory positively impacts indexing and presence in the DMOZ directory, specifically, positively impacts archiving in all four languages. In this work, we show that web archives and search engines favor English pages. However, it is not universally true for all Western-language webpages because, in this work, we show that Arabic webpages have a higher archival rate than Danish language webpages.	2017	https://doi.org/10.1145/3041656	https://doi.org/10.1145/3041656	FALSE
Carisimo, Esteban and Gamero-Garrido, Alexander and Snoeren, Alex C. and Dainotti, Alberto	Identifying ASes of State-Owned Internet Operators	In this paper we present and apply a methodology to accurately identify state-owned Internet operators worldwide and their Autonomous System Numbers (ASNs). Obtaining an accurate dataset of ASNs of state-owned Internet operators enables studies where state ownership is an important dimension, including research related to Internet censorship and surveillance, cyber-warfare and international relations, ICT development and digital divide, critical infrastructure protection, and public policy. Our approach is based on a multi-stage, in-depth manual analysis of datasets that are highly diverse in nature. We find that each of these datasets contributes in different ways to the classification process and we identify limitations and shortcomings of these data sources. We obtain the first data set of this type, make it available to the research community together with the several lessons we learned in the process, and perform a preliminary analysis based on our data. We find that 53% (i.e., 123) of the world's countries are majority owners of Internet operators, highlighting that this is a widespread phenomenon. We also find and document the existence of subsidiaries of state-owned governments operating in foreign countries, an aspect that touches every continent and particularly affects Africa. We hope that this work and the associated data set will inspire and enable a broad set of Internet measurement studies and interdisciplinary research.	2021	https://doi.org/10.1145/3487552.3487822	https://doi.org/10.1145/3487552.3487822	FALSE
	ICMI '16: Proceedings of the 18th ACM International Conference on Multimodal Interaction		2016			FALSE
Uma, Alexandra N. and Fornaciari, Tommaso and Hovy, Dirk and Paun, Silviu and Plank, Barbara and Poesio, Massimo	Learning from Disagreement: A Survey	Many tasks in Natural Language Processing (NLP) and Computer Vision (CV) offer evidence that humans disagree, from objective tasks such as part-of-speech tagging to more subjective tasks such as classifying an image or deciding whether a proposition follows from certain premises. While most learning in artificial intelligence (AI) still relies on the assumption that a single (gold) interpretation exists for each item, a growing body of research aims to develop learning methods that do not rely on this assumption. In this survey, we review the evidence for disagreements on NLP and CV tasks, focusing on tasks for which substantial datasets containing this information have been created. We discuss the most popular approaches to training models from datasets containing multiple judgments potentially in disagreement. We systematically compare these different approaches by training them with each of the available datasets, considering several ways to evaluate the resulting models. Finally, we discuss the results in depth, focusing on four key research questions, and assess how the type of evaluation and the characteristics of a dataset determine the answers to these questions. Our results suggest, first of all, that even if we abandon the assumption of a gold standard, it is still essential to reach a consensus on how to evaluate models. This is because the relative performance of the various training methods is critically affected by the chosen form of evaluation. Secondly, we observed a strong dataset effect. With substantial datasets, providing many judgments by high-quality coders for each item, training directly with soft labels achieved better results than training from aggregated or even gold labels. This result holds for both hard and soft evaluation. But when the above conditions do not hold, leveraging both gold and soft labels generally achieved the best results in the hard evaluation. All datasets and models employed in this paper are freely available as supplementary materials.	2022	https://doi.org/10.1613/jair.1.12752	https://doi.org/10.1613/jair.1.12752	FALSE
	CSCW '17: Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing	Welcome to CSCW 2017, the ACM 2017 Conference on Computer Supported Cooperative Work and Social Computing! We are excited to welcome the CSCW community back to Portland, Oregon, where the second CSCW conference was held in 1988. Both Portland and CSCW have matured a great deal during the intervening 29 years. We hope that you will find that Portland provides a stimulating environment for our conference.CSCW is the premier venue for presenting research in the design and use of technologies that affect groups, organizations, communities, and networks. Bringing together top researchers and practitioners from academia and industry, CSCW explores the technical, social, material, and theoretical challenges of designing technology to support collaborative work and life activities. CSCW welcomes a diverse range of topics and research methodologies. Studies often involve the development and application of novel technologies and/or ethnographic studies that inform design practice or theory. The mission of the conference is to share research that advances the state of human knowledge and improves both the design of systems and the ways they are used. The diversity of work in our conference program reflects the diversity of technology use in people's work, social, and civic lives as well as the geographic and cultural diversity of contributors.As many of you know, CSCW follows a rigorous "revise and resubmit" review process that uses peer review to improve submitted papers while maintaining a high-quality threshold for final acceptance. We also help prepare the next generation of reviewers with a mentorship program in which students review papers under the guidance of an experienced reviewer. This year we have the largest CSCW program ever. We had 530 submitted papers and 183 were accepted for presentation at the conference. The program also includes 4 papers published in ACM Transactions on Human- Computer Interaction (TOCHI). In addition, we will feature 14 workshops, 56 posters, 12 demos, and 3 panels.Lili Cheng of Microsoft Research will open the conference, speaking on "Conversational AI &amp; Lessons Learned." Our closing plenary will feature Jorge Cham, the creator of PhD Comics, who will talk about, "The Science Gap." We also welcome Paul Luff and Christian Heath from King's College as the recipients of this year's CSCW Lasting Impact award for their influential 1998 paper, "Mobility in Collaboration."	2017			FALSE
Roberts, Richard and Goldschlag, Yaelle and Walter, Rachel and Chung, Taejoong and Mislove, Alan and Levin, Dave	You Are Who You Appear to Be: A Longitudinal Study of Domain Impersonation in TLS Certificates	The public key infrastructure (PKI) provides the fundamental property of authentication: the means by which users can know with whom they are communicating online. The PKI ensures end-to-end authenticity insofar as it verifies a chain of certificates, but the true final step in end-to-end authentication comes when the user verifies that the website is what they expect. To this end, users are expected to evaluate domain names, but various "domain impersonation" attacks threaten their ability to do so. Indeed, if a user could be easily tricked into believing that amazon.com-offers.com is actually amazon.com, then, coupled with security indicators like a lock icon, users could believe that they have a secure connection to Amazon.We study this threat to end-to-end authentication: (1) We introduce a new classification of an impersonation attack that we call target embedding. This embeds an entire target domain, unmodified, using one or more subdomains of the actual domain. (2) We perform a user study with the specific goal of understanding whether users fall for target embedding, and how its efficacy compares to other popular impersonation attacks (typosquatting, combosquatting, and homographs). We find that target embedding is the most effective against modern browsers. (3) Using all HTTPS certificates collected by Censys, we perform a longitudinal analysis of how target-embedding impersonation has evolved, who is responsible for issuing impersonating certificates, who hosts the domains, where the economic choke-points are, and more. We close with a discussion of counter-measures against this growing threat.	2019	https://doi.org/10.1145/3319535.3363188	https://doi.org/10.1145/3319535.3363188	FALSE
	ASE 2016: Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering		2016			FALSE
Bar-Haim, Roy and Dagan, Ido and Berant, Jonathan	Knowledge-Based Textual Inference via Parse-Tree Transformations	Textual inference is an important component in many applications for understanding natural language. Classical approaches to textual inference rely on logical representations for meaning, which may be regarded as "external" to the natural language itself. However, practical applications usually adopt shallower lexical or lexical-syntactic representations, which correspond closely to language structure. In many cases, such approaches lack a principled meaning representation and inference framework. We describe an inference formalism that operates directly on language-based structures, particularly syntactic parse trees. New trees are generated by applying inference rules, which provide a unified representation for varying types of inferences. We use manual and automatic methods to generate these rules, which cover generic linguistic structures as well as specific lexical-based inferences. We also present a novel packed data-structure and a corresponding inference algorithm that allows efficient implementation of this formalism. We proved the correctness of the new algorithm and established its efficiency analytically and empirically. The utility of our approach was illustrated on two tasks: unsupervised relation extraction from a large corpus, and the Recognizing Textual Entailment (RTE) benchmarks.	2015			FALSE
	SA '14: SIGGRAPH Asia 2014 Courses		2014			FALSE
Halvorson, Michael J.	Code Nation: Personal Computing and the Learn to Program Movement in America	Code Nation explores the rise of software development as a social, cultural, and technical phenomenon in American history. The movement germinated in government and university labs during the 1950s, gained momentum through corporate and counterculture experiments in the 1960s and 1970s, and became a broad-based computer literacy movement in the 1980s. As personal computing came to the fore, learning to program was transformed by a groundswell of popular enthusiasm, exciting new platforms, and an array of commercial practices that have been further amplified by distributed computing and the Internet. The resulting society can be depicted as a “Code Nation”—a globally-connected world that is saturated with computer technology and enchanted by software and its creation.Code Nation is a new history of personal computing that emphasizes the technical and business challenges that software developers faced when building applications for CP/M, MS-DOS, UNIX, Microsoft Windows, the Apple Macintosh, and other emerging platforms. It is a popular history of computing that explores the experiences of novice computer users, tinkerers, hackers, and power users, as well as the ideals and aspirations of leading computer scientists, engineers, educators, and entrepreneurs. Computer book and magazine publishers also played important, if overlooked, roles in the diffusion of new technical skills, and this book highlights their creative work and influence.Code Nation offers a “behind-the-scenes” look at application and operating-system programming practices, the diversity of historic computer languages, the rise of user communities, early attempts to market PC software, and the origins of “enterprise” computing systems. Code samples and over 80 historic photographs support the text. The book concludes with an assessment of contemporary efforts to teach computational thinking to young people.	2020			FALSE
Calegari, Patrice and Levrier, Marc and Balczy\'{n}ski, Pawe\l{}	Web Portals for High-Performance Computing: A Survey	This article addresses web interfaces for High-performance Computing (HPC) simulation software. First, it presents a brief history, starting in the 1990s with Java applets, of web interfaces used for accessing and making best possible use of remote HPC resources. It introduces HPC web-based portal use cases. Then it identifies and discusses the key features, among functional and non-functional requirements, that characterize such portals. A brief state of the art is then presented. The design and development of Bull extreme factory Computing Studio v3 (XCS3) is chosen as a common thread for showing how the identified key features can all be implemented in one software: multi-tenancy, multi-scheduler compatibility, complete control through an HTTP RESTful API, customizable user interface with Responsive Web Design, HPC application template framework, remote visualization, and access through the Authentication, Authorization, and Accounting security framework with the Role-Based Access Control permission model. Non-functional requirements (security, usability, performance, reliability) are discussed, and the article concludes by giving perspective for future work.	2019	https://doi.org/10.1145/3197385	https://doi.org/10.1145/3197385	FALSE
	Probabilistic and Causal Inference: The Works of Judea Pearl	Professor Judea Pearl won the 2011 Turing Award “for fundamental contributions to artificial intelligence through the development of a calculus for probabilistic and causal reasoning.” This book contains the original articles that led to the award, as well as other seminal works, divided into four parts: heuristic search, probabilistic reasoning, causality, first period (1988–2001), and causality, recent period (2002–2020). Each of these parts starts with an introduction written by Judea Pearl. The volume also contains original, contributed articles by leading researchers that analyze, extend, or assess the influence of Pearl’s work in different fields: from AI, Machine Learning, and Statistics to Cognitive Science, Philosophy, and the Social Sciences. The first part of the volume includes a biography, a transcript of his Turing Award Lecture, two interviews, and a selected bibliography annotated by him.	2022			FALSE
	POPL '16: Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages		2016			FALSE
	CHI EA '21: Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems		2021			FALSE
Al Mahmud, Tamim and Hussain, Md Gulzar and Kabir, Sumaiya and Ahmad, Hasnain and Sobhan, Mahmudus	A Keyword Based Technique to Evaluate Broad Question Answer Script	Evaluation is the method of assessing and determining the educational system through various techniques such as verbal or viva-voice test, subjective or objective written test. This paper presents an efficient solution to evaluate the subjective answer script electronically. In this paper, we proposed and implemented an integrated system that examines and evaluates the written answer script. This article focuses on finding the keywords from the answer script and then compares them with the keywords that have been parsed from both open and closed domain. The system also checks the grammatical and spelling errors in the answer script. Our proposed system tested with answer scripts of 100 students and gives precision score 0.91.	2020	https://doi.org/10.1145/3384544.3384604	https://doi.org/10.1145/3384544.3384604	FALSE
Cheniki, Nasredine and Belkhir, Abdelkader and Atif, Yacine	Supporting Multilingual Semantic Web Services Discovery by Consuming Data from DBpedia Knowledge Base	The Web is becoming a truly multilingual hub in which speakers from different languages and cultures are producing, interacting and consuming information. Web services are an essential part of the Web which nowadays attract more and more people around the world. So, supporting multilingual Web services discovery is a crucial goal to achieve, so that providers and users publish or consume Web services independently from their culture and native language. In this paper, we propose to overcome language barrier by supporting multilingual Web services discovery using DBpedia, which is a cross-domain multilingual knowledge base. DBpedia is used to annotate services with semantic entities called resources as well as their categories and types. We take advantage of semantic and multilingual information provided by DBpedia to enable cross-language semantic Web services discovery. Implementation shows that DBpedia offers a valuable information source to achieve our goal.	2015	https://doi.org/10.1145/2816839.2816862	https://doi.org/10.1145/2816839.2816862	FALSE
Liu, Xueqing and Wang, Chi and Leng, Yue and Zhai, ChengXiang	LinkSO: A Dataset for Learning to Retrieve Similar Question Answer Pairs on Software Development Forums	We present LinkSO, a dataset for learning to rank similar questions on Stack Overflow. Stack Overflow contains a massive amount of crowd-sourced question links of high quality, which provides a great opportunity for evaluating retrieval algorithms for community-based question answer (cQA) archives and for learning to rank such archives. However, due to the existence of missing links, one question is whether question links can be readily used as the relevance judgment for evaluation. We study this question by measuring the closeness between question links and the relevance judgment, and we find their agreement rates range from 80% to 88%. We conduct an empirical study on the performance of existing work on LinkSO. While existing work focuses on non-learning approaches, our study results reveal that learning-based approaches has great potential to further improve the retrieval performance.	2018	https://doi.org/10.1145/3283812.3283815	https://doi.org/10.1145/3283812.3283815	FALSE
Liu, Qian and Geng, Xiubo and Lu, Jie and Jiang, Daxin	Pivot-Based Candidate Retrieval for Cross-Lingual Entity Linking	 Entity candidate retrieval plays a critical role in cross-lingual entity linking (XEL). In XEL, entity candidate retrieval needs to retrieve a list of plausible candidate entities from a large knowledge graph in a target language given a piece of text in a sentence or question, namely a mention, in a source language. Existing works mainly fall into two categories: lexicon-based and semantic-based approaches. The lexicon-based approach usually creates cross-lingual and mention-entity lexicons, which is effective but relies heavily on bilingual resources (e.g. inter-language links in Wikipedia). The semantic-based approach maps mentions and entities in different languages to a unified embedding space, which reduces dependence on large-scale bilingual dictionaries. However, its effectiveness is limited by the representation capacity of fixed-length vectors. In this paper, we propose a pivot-based approach which inherits the advantages of the aforementioned two approaches while avoiding their limitations. It takes an intermediary set of plausible target-language mentions as pivots to bridge the two types of gaps: cross-lingual gap and mention-entity gap. Specifically, it first converts mentions in the source language into an intermediary set of plausible mentions in the target language by cross-lingual semantic retrieval and a selective mechanism, and then retrieves candidate entities based on the generated mentions by lexical retrieval. The proposed approach only relies on a small bilingual word dictionary, and fully exploits the benefits of both lexical and semantic matching. Experimental results on two challenging cross-lingual entity linking datasets spanning over 11 languages show that the pivot-based approach outperforms both the lexicon-based and semantic-based approach by a large margin.	2021	https://doi.org/10.1145/3442381.3449852	https://doi.org/10.1145/3442381.3449852	FALSE
Yu, Puxuan and Fei, Hongliang and Li, Ping	Cross-Lingual Language Model Pretraining for Retrieval	 Existing research on cross-lingual retrieval cannot take good advantage of large-scale pretrained language models such as multilingual BERT and XLM. We hypothesize that the absence of cross-lingual passage-level relevance data for finetuning and the lack of query-document style pretraining are key factors of this issue. In this paper, we introduce two novel retrieval-oriented pretraining tasks to further pretrain cross-lingual language models for downstream retrieval tasks such as cross-lingual ad-hoc retrieval (CLIR) and cross-lingual question answering (CLQA). We construct distant supervision data from multilingual Wikipedia using section alignment to support retrieval-oriented language model pretraining. We also propose to directly finetune language models on part of the evaluation collection by making Transformers capable of accepting longer sequences. Experiments on multiple benchmark datasets show that our proposed model can significantly improve upon general multilingual language models in both the cross-lingual retrieval setting and the cross-lingual transfer setting.	2021	https://doi.org/10.1145/3442381.3449830	https://doi.org/10.1145/3442381.3449830	FALSE
R\"{u}ckl\'{e}, Andreas and Swarnkar, Krishnkant and Gurevych, Iryna	Improved Cross-Lingual Question Retrieval for Community Question Answering	We perform cross-lingual question retrieval in community question answering (cQA), i.e., we retrieve similar questions for queries that are given in another language. The standard approach to cross-lingual information retrieval, which is to automatically translate the query to the target language and continue with a monolingual retrieval model, typically falls short in cQA due to translation errors. This is even more the case for specialized domains such as in technical cQA, which we explore in this work. To remedy, we propose two extensions to this approach that improve cross-lingual question retrieval: (1) we enhance an NMT model with monolingual cQA data to improve the translation quality, and (2) we improve the robustness of a state-of-the-art neural question retrieval model to common translation errors by adding back-translations during training. Our results show that we achieve substantial improvements over the baseline approach and considerably close the gap to a setup where we have access to an external commercial machine translation service (i.e., Google Translate), which is often not the case in many practical scenarios. Our source code and data is publicly available.1	2019	https://doi.org/10.1145/3308558.3313502	https://doi.org/10.1145/3308558.3313502	FALSE
Mountantonakis, Michalis and Tzitzikas, Yannis	Large-Scale Semantic Integration of Linked Data: A Survey	A large number of published datasets (or sources) that follow Linked Data principles is currently available and this number grows rapidly. However, the major target of Linked Data, i.e., linking and integration, is not easy to achieve. In general, information integration is difficult, because (a) datasets are produced, kept, or managed by different organizations using different models, schemas, or formats, (b) the same real-world entities or relationships are referred with different URIs or names and in different natural languages,<!--?brk?-->(c) datasets usually contain complementary information, (d) datasets can contain data that are erroneous, out-of-date, or conflicting, (e) datasets even about the same domain may follow different conceptualizations of the domain, (f) everything can change (e.g., schemas, data) as time passes. This article surveys the work that has been done in the area of Linked Data integration, it identifies the main actors and use cases, it analyzes and factorizes the integration process according to various dimensions, and it discusses the methods that are used in each step. Emphasis is given on methods that can be used for integrating several datasets. Based on this analysis, the article concludes with directions that are worth further research.	2019	https://doi.org/10.1145/3345551	https://doi.org/10.1145/3345551	FALSE
Allemang, Dean and Hendler, Jim and Gandon, Fabien	Semantic Web for the Working Ontologist: Effective Modeling for Linked Data, RDFS, and OWL	Enterprises have made amazing advances by taking advantage of data about their business to provide predictions and understanding of their customers, markets, and products. But as the world of business becomes more interconnected and global, enterprise data is no long a monolith; it is just a part of a vast web of data. Managing data on a world-wide scale is a key capability for any business today.The Semantic Web treats data as a distributed resource on the scale of the World Wide Web, and incorporates features to address the challenges of massive data distribution as part of its basic design. The aim of the first two editions was to motivate the Semantic Web technology stack from end-to-end; to describe not only what the Semantic Web standards are and how they work, but also what their goals are and why they were designed as they are. It tells a coherent story from beginning to end of how the standards work to manage a world-wide distributed web of knowledge in a meaningful way.The third edition builds on this foundation to bring Semantic Web practice to enterprise. Fabien Gandon joins Dean Allemang and Jim Hendler, bringing with him years of experience in global linked data, to open up the story to a modern view of global linked data. While the overall story is the same, the examples have been brought up to date and applied in a modern setting, where enterprise and global data come together as a living, linked network of data. Also included with the third edition, all of the data sets and queries are available online for study and experimentation at data.world/swwo.	2020			FALSE
Snover, Matthew and Li, Xiang and Lin, Wen-Pin and Chen, Zheng and Tamang, Suzanne and Ge, Mingmin and Lee, Adam and Li, Qi and Li, Hao and Anzaroot, Sam and Ji, Heng	Cross-Lingual Slot Filling from Comparable Corpora	This paper introduces a new task of crosslingual slot filling which aims to discover attributes for entity queries from crosslingual comparable corpora and then present answers in a desired language. It is a very challenging task which suffers from both information extraction and machine translation errors. In this paper we analyze the types of errors produced by five different baseline approaches, and present a novel supervised rescoring based validation approach to incorporate global evidence from very large bilingual comparable corpora. Without using any additional labeled data this new approach obtained 38.5% relative improvement in Precision and 86.7% relative improvement in Recall over several state-of-the-art approaches. The ultimate system outperformed monolingual slot filling pipelines built on much larger monolingual corpora.	2011			FALSE
Amanqui, Flor K. and Serique, Kleberson J. and Cardoso, Silvio D. and Santos, Jos\'{e} L. Dos and Albuquerque, Andrea and Moreira, Dilvan A.	Improving Biodiversity Data Retrieval through Semantic Search and Ontologies	Due to the increased amount of available biodiversity data, many biodiversity research institutions are now making their databases openly available on the web. Researchers in the field use this databases to extract new knowledge and also share their own discoveries. However, when these researchers need to find relevant information in the data, they still rely on the traditional search approach, based on text matching, that is not appropriate to be used in these large amounts of heterogeneous biodiversity's data, leading to search results with low precision and recall. We present a new architecture that tackle this problem using a semantic search system for biodiversity data. Semantic search aims to improve search accuracy by using ontologies to understand user objectives and the contextual meaning of terms used in the search to generate more relevant results. Biodiversity data is mapped to terms from relevant ontologies, such as Darwin Core, DBpedia, Ontobio and Catalogue of Life, stored using semantic web formats and queried using semantic web tools (such as triple stores). A prototype semantic search tool was successfully implemented and evaluated by users from the National Research Institute for the Amazon (INPA). Our results show that the semantic search approach has a better precision (28% improvement) and recall (25% improvement) when compared to keyword based search, when used in a big set of representative biodiversity data (206,000 records) from INPA and the Emilio Gueldi Museum in Par\'{a} (MPEG). We also show that, because the biodiversity data is now in semantic web format and mapped to ontology terms, it is easy to enhance it with information from other sources, an example using deforestation data (from the National Institute of Space Research - INPE) to enrich collection data is shown.	2014	https://doi.org/10.1109/WI-IAT.2014.44	https://doi.org/10.1109/WI-IAT.2014.44	FALSE
Mitra, Rajarshee and Jain, Rhea and Veerubhotla, Aditya Srikanth and Gupta, Manish	Zero-Shot Multi-Lingual Interrogative Question Generation for "People Also Ask" at Bing	Multi-lingual question generation (QG) is the task of generating natural language questions for single answer passage in any given language. In this paper, we design a system for supporting multi-lingual QG in the "People Also Ask" (PAA) module for Bing. For zero shot setting, the primary challenge is to transfer the knowledge from trained QG model in the pivot language to other languages without further addition of training data in these languages. Compared to other zero-shot tasks, the differentiating and challenging aspect in QG is to preserve the question structure so that the resulting output is interrogative. Existing models for similar tasks tend to generate natural language queries or copy sub-span of the passage, failing to preserve the question structure. In our work, we demonstrate how knowledge transfer in multi-lingual IQG (Interrogative QG) can be significantly improved using auxiliary tasks either in multi-task or pre-training task setting. We explore two kinds of tasks - cross-lingual translation and multi-lingual denoising auto-encoding of questions, especially when using translate-train. Using data for 13 languages from Bing PAA as well as online A/B tests, we show that both of these tasks significantly improve the quality of zero-shot IQG on non-trained languages.	2021	https://doi.org/10.1145/3447548.3469403	https://doi.org/10.1145/3447548.3469403	FALSE
Fern\'{a}ndez, Alejandro Moreo and Esuli, Andrea and Sebastiani, Fabrizio	Distributional Correspondence Indexing for Cross-Lingual and Cross-Domain Sentiment Classification	Domain Adaptation (DA) techniques aim at enabling machine learning methods learn effective classifiers for a "target" domain when the only available training data belongs to a different "source" domain. In this paper we present the Distributional Correspondence Indexing (DCI) method for domain adaptation in sentiment classification. DCI derives term representations in a vector space common to both domains where each dimension re ects its distributional correspondence to a pivot, i.e., to a highly predictive term that behaves similarly across domains. Term correspondence is quantified by means of a distributional correspondence function (DCF). We propose a number of efficient DCFs that are motivated by the distributional hypothesis, i.e., the hypothesis according to which terms with similar meaning tend to have similar distributions in text. Experiments show that DCI obtains better performance than current state-of-the-art techniques for cross-lingual and cross-domain sentiment classification. DCI also brings about a significantly reduced computational cost, and requires a smaller amount of human intervention. As a final contribution, we discuss a more challenging formulation of the domain adaptation problem, in which both the cross-domain and cross-lingual dimensions are tackled simultaneously.	2016			FALSE
Rudnik, Charlotte and Ehrhart, Thibault and Ferret, Olivier and Teyssou, Denis and Troncy, Raphael and Tannier, Xavier	Searching News Articles Using an Event Knowledge Graph Leveraged by Wikidata	News agencies produce thousands of multimedia stories describing events happening in the world that are either scheduled such as sports competitions, political summits and elections, or breaking events such as military conflicts, terrorist attacks, natural disasters, etc. When writing up those stories, journalists refer to contextual background and to compare with past similar events. However, searching for precise facts described in stories is hard. In this paper, we propose a general method that leverages the Wikidata knowledge base to produce semantic annotations of news articles. Next, we describe a semantic search engine that supports both keyword based search in news articles and structured data search providing filters for properties belonging to specific event schemas that are automatically inferred.	2019	https://doi.org/10.1145/3308560.3316761	https://doi.org/10.1145/3308560.3316761	FALSE
Dalton, Jeffrey and Dietz, Laura and Allan, James	Entity Query Feature Expansion Using Knowledge Base Links	Recent advances in automatic entity linking and knowledge base construction have resulted in entity annotations for document and query collections. For example, annotations of entities from large general purpose knowledge bases, such as Freebase and the Google Knowledge Graph. Understanding how to leverage these entity annotations of text to improve ad hoc document retrieval is an open research area. Query expansion is a commonly used technique to improve retrieval effectiveness. Most previous query expansion approaches focus on text, mainly using unigram concepts. In this paper, we propose a new technique, called entity query feature expansion (EQFE) which enriches the query with features from entities and their links to knowledge bases, including structured attributes and text. We experiment using both explicit query entity annotations and latent entities. We evaluate our technique on TREC text collections automatically annotated with knowledge base entity links, including the Google Freebase Annotations (FACC1) data. We find that entity-based feature expansion results in significant improvements in retrieval effectiveness over state-of-the-art text expansion approaches.	2014	https://doi.org/10.1145/2600428.2609628	https://doi.org/10.1145/2600428.2609628	FALSE
Lehmann, Jens and Auer, S\"{o}ren and Capadisli, Sarven and Janowicz, Krzysztof and Bizer, Christian and Heath, Tom and Hogan, Aidan and Berners-Lee, Tim	LDOW2017: 10th Workshop on Linked Data on the Web	The 10th Linked Data on the Web workshop (LDOW2017) was held in Perth, Western Australia on April 3, 2017, co-located with the 26th International World Wide Web Conference (WWW2017). In its 10th anniversary edition, the LDOW workshop aims to stimulate discussion and further research into the challenges of publishing, consuming, and integrating structured data on the Web as well as mining knowledge from said data.	2017	https://doi.org/10.1145/3041021.3055510	https://doi.org/10.1145/3041021.3055510	FALSE
Lee, Jieh-Sheng	Measuring and Controlling Text Generation by Semantic Search	Our motivation in this work is to measure patent text generation by semantic search, particularly by textual similarity in high dimensional space for neural network models. The objective is to control patent text generation by semantic search. Conceptually it is an attempt to integrate two subfields in NLP: text generation and semantic search. In our previous milestone of the PatentTransformer project, a prototype based on GPT-2 is capable of generating fluent patent title, abstract, independent claim, and dependent claim. However, beneath the surface form, the quality issue in the generated patent text was less explored. How to control text generation is also a hard problem in NLP field. We would like to address these issues in this work and experiment with different approaches. On the measurement side, this work will address the quality measurement issue from the perspective of textual similarity. Based on that, the approaches we propose include two embedding spaces, span-based textual similarity, and language model for patent claim spans. One the control side, we propose a knob-turning approach for controlling text generation based on measuring a range of textual similarity. In this way, we can search for a Goldilocks zone in which the similarity of generated patent text is close to but not too far from prior patents. We hypothesize that patent novelty may exist in such a zone.	2020		https://doi.org/10.1145/3366424.3382086	FALSE
Garc\'{\i}a-Silva, Andr\'{e}s and Garc\'{\i}a-Castro, Leyla Jael and Garc\'{\i}a, Alexander and Corcho, Oscar	Social Tags and Linked Data for Ontology Development: A Case Study in the Financial Domain	We describe a domain ontology development approach that extracts domain terms from folksonomies and enrich them with data and vocabularies from the Linked Open Data cloud. As a result, we obtain lightweight domain ontologies that combine the emergent knowledge of social tagging systems with formal knowledge from Ontologies. In order to illustrate the feasibility of our approach, we have produced an ontology in the financial domain from tags available in Delicious, using DBpedia, OpenCyc and UMBEL as additional knowledge sources.	2014	https://doi.org/10.1145/2611040.2611075	https://doi.org/10.1145/2611040.2611075	FALSE
Yan, Rui and Liao, Weiheng and Cui, Jianwei and Zhang, Hailei and Hu, Yichuan and Zhao, Dongyan	Multilingual COVID-QA: Learning towards Global Information Sharing via Web Question Answering in Multiple Languages	 Since late December 2019, it has been reported an outbreak of atypical pneumonia, now known as COVID-19 caused by the novel coronavirus. Cases have spread to more than 200 countries and regions internationally. World Health Organization (WHO) officially declares the coronavirus outbreak a pandemic and the public health emergency has caused world-wide impact to daily lives: people are advised to keep social distance, in-person events have been moved online, and some function facilitates have been locked-down. Alternatively, the Web becomes an active venue for people to share information. With respect to the on-going topic, people continuously post questions online and seek for answers. Yet, sharing global information conveyed in different languages is challenging because the language barrier is intrinsically unfriendly to monolingual speakers. In this paper, we propose a multilingual COVID-QA model to answer people’s questions in their own languages while the model is able to absorb knowledge from other languages. Another challenge is that in most cases, the information to share does not have parallel data in multiple languages. To this end, we propose a novel framework which incorporates (unsupervised) translation alignment to learn as pseudo-parallel data. Then we train multilingual question-answering mapping and generation. We demonstrate the effectiveness of our proposed approach compared against a series of competitive baselines. In this way, we make it easier to share global information across the language barriers, and hopefully we contribute to the battle against COVID-19.	2021	https://doi.org/10.1145/3442381.3449991	https://doi.org/10.1145/3442381.3449991	FALSE
Hu, Anming and Chen, Huie	Data Visualization Analysis of Knowledge Graph Application	In recent years, the application of knowledge graph has gradually become an important field of computer research. In this paper, a visual data analysis tool is used to conduct data analysis on the literature samples. The analysis focuses on solving the following problems :(1) Knowledge Graphic provides a selection mechanism for academic research and corporate practice in the application of artificial intelligence;(2) The development process, technical framework and co-citation network of application fields of knowledge graph were analyzed.	2021	https://doi.org/10.1145/3469213.3472783	https://doi.org/10.1145/3469213.3472783	FALSE
Ma, Lin and Ma, Yuchun	Automatic Question Generation Based on MOOC Video Subtitles and Knowledge Graph	With the popularity of MOOCs (Massive Open Online Courses), videos have gradually replaced textbooks as the most common educational resources. Quiz is an important teaching tool in online learning process [1], and manually constructing quiz questions is time-consuming and labor-intensive, so there is a pressing need for automatic question generation from video subtitles. The challenge of this problem is that video subtitles are different from textbooks considering their characteristics such as redundancy, colloquialism, and no sentence division. In this paper, we firstly use external wiki knowledge graph (WIKIDATA [2]) to extract facts of interest from video subtitles, and then propose a novel template-based method to generate quiz questions from knowledge graph. Compared to traditional template-based method [4], we make improvements on the accuracy and comprehensibility of the question. Final experiment on SimpleQuestions dataset [3] shows that the proposed method outperforms other competitive methods by +6% in terms of BLEU on the task of question generations from knowledge graph [4].	2019	https://doi.org/10.1145/3323771.3323820	https://doi.org/10.1145/3323771.3323820	FALSE
Linjordet, Trond	Neural (Knowledge Graph) Question Answering Using Synthetic Training Data	Deep learning requires volume, quality, and variety of training data. In neural question answering, a trade-off between quality and volume comes from the need to either manually curate or construct realistic question answering data, which is costly, or else augmenting, weakly labeling or generating training data from smaller datasets, leading to low variety and sometimes low quality. What can be done to make the best of this necessary trade-off? What can be understood from the endeavor to seek such solutions?	2020	https://doi.org/10.1145/3340531.3418505	https://doi.org/10.1145/3340531.3418505	FALSE
Song, Qiurong and Wang, Xinyue and Wang, Rui	Knowledge Network and Visual Analysis of Knowledge Graph Research		2021	https://doi.org/10.1145/3463914.3463924	https://doi.org/10.1145/3463914.3463924	FALSE
Embley, David W. and Liddle, Stephen W. and Lonsdale, Deryle W. and Tijerino, Yuri	Multilingual Ontologies for Cross-Language Information Extraction and Semantic Search	Valuable local information is often available on the web, but encoded in a foreign language that non-local users do not understand. Can we create a system to allow a user to query in language L1 for facts in a web page written in language L2? We propose a suite of multilingual extraction ontologies as a solution to this problem. We ground extraction ontologies in each language of interest, and we map both the data and the metadata among the language-specific extraction ontologies. The mappings are through a central, language-agnostic ontology that allows new languages to be added by only having to provide one mapping rather than one for each language pair. Results from an implemented early prototype demonstrate the feasibility of cross-language information extraction and semantic search. Further, results from an experimental evaluation of ontology-based query translation and extraction accuracy are remarkably good given the complexity of the problem and the complications of its implementation.	2011			FALSE
Demartini, Gianluca and Difallah, Djellel Eddine and Cudr\'{e}-Mauroux, Philippe	Large-Scale Linked Data Integration Using Probabilistic Reasoning and Crowdsourcing	We tackle the problems of semiautomatically matching linked data sets and of linking large collections of Web pages to linked data. Our system, ZenCrowd, (1) uses a three-stage blocking technique in order to obtain the best possible instance matches while minimizing both computational complexity and latency, and (2) identifies entities from natural language text using state-of-the-art techniques and automatically connects them to the linked open data cloud. First, we use structured inverted indices to quickly find potential candidate results from entities that have been indexed in our system. Our system then analyzes the candidate matches and refines them whenever deemed necessary using computationally more expensive queries on a graph database. Finally, we resort to human computation by dynamically generating crowdsourcing tasks in case the algorithmic components fail to come up with convincing results. We integrate all results from the inverted indices, from the graph database and from the crowd using a probabilistic framework in order to make sensible decisions about candidate matches and to identify unreliable human workers. In the following, we give an overview of the architecture of our system and describe in detail our novel three-stage blocking technique and our probabilistic decision framework. We also report on a series of experimental results on a standard data set, showing that our system can achieve a 95 % average accuracy on instance matching (as compared to the initial 88 % average accuracy of the purely automatic baseline) while drastically limiting the amount of work performed by the crowd. The experimental evaluation of our system on the entity linking task shows an average relative improvement of 14 % over our best automatic approach.	2013	https://doi.org/10.1007/s00778-013-0324-z	https://doi.org/10.1007/s00778-013-0324-z	FALSE
Marie, Nicolas and Gandon, Fabien and Ribi\`{e}re, Myriam and Rodio, Florentin	Discovery Hub: On-the-Fly Linked Data Exploratory Search	Exploratory search systems help users learn or investigate a topic. The richness of the linked open data can be used to assist this task. We present a method that selects and ranks linked data resources that are semantically related to the user's interest. The objective is to focus the user's attention on a meaningful subset of highly informative resources. We extended spreading activation to typed graphs and coupled it with a graph sampling technique. The results selection and ranking is performed on--the-fly and doesn't require pre-processing. This allows addressing remote SPARQL endpoints. We describe first implementation on top of DBpedia. It is used by the Discovery Hub exploratory search system to select interesting resources, to support faceted browsing of the results, to provide explanations and to offer redirections to third-party services. Results of a user evaluation conclude the article.	2013	https://doi.org/10.1145/2506182.2506185	https://doi.org/10.1145/2506182.2506185	FALSE
Takhom, Akkharawoot and Utasri, Tharathon and Leenoi, Dhanon and Soomjinda, Pitchaya and Boonkwan, Prachya and Supnithi, Thepchai	Knowledge Graph Enhanced Community Consensus: A Scenario-Based Knowledge Construction on Buddha Images		2021	https://doi.org/10.1145/3502223.3502744	https://doi.org/10.1145/3502223.3502744	FALSE
Do, Phuc and Phan, Truong H. V. and Gupta, Brij B.	Developing a Vietnamese Tourism Question Answering System Using Knowledge Graph and Deep Learning	In recent years, Question Answering (QA) systems have increasingly become very popular in many sectors. This study aims to use a knowledge graph and deep learning to develop a QA system for tourism in Vietnam. First, the QA system replies to a user's question about a place in Vietnam. Then, the QA describes it in detail such as when the place was discovered, why the place's name was called like that, and so on. Finally, the system recommends some related tourist attractions to users. Meanwhile, deep learning is used to solve a simple natural language answer, and a knowledge graph is used to infer a natural language answering list related to entities in the question. The study experiments on a manual dataset collected from Vietnamese tourism websites. As a result, the QA system combining the two above approaches provides more information than other systems have done before. Besides that, the system gets 0.83 F1, 0.87 precision on the test set.	2021	https://doi.org/10.1145/3453651	https://doi.org/10.1145/3453651	FALSE
Stolee, Kathryn T.	Finding Suitable Programs: Semantic Search with Incomplete and Lightweight Specifications	 Finding suitable code for reuse is a common task for programmers. Two general approaches dominate the code search literature: syntactic and semantic. While queries for syntactic search are easy to compose, the results are often vague or irrelevant. On the other hand, a semantic search may return relevant results, but current techniques require developers to write specifications by hand, are costly as potentially matching code need to be executed to verify congruence with the specifications, or only return exact matches. In this work, we propose an approach for semantic search in which programmers specify lightweight, incomplete specifications and an SMT solver automatically identifies programs from a repository, encoded as constraints, that match the specifications. The repository of programs is automatically encoded offline so the search for matching programs is efficient. The program encodings cover various levels of abstraction to enable partial matches when no or few exact matches exists. We instantiate this approach on a subset of the Yahoo! Pipes mashup language, and plan to extend our techniques to more traditional programming languages as the research progresses. 	2012			FALSE
Assaf, Ahmad and Louw, Eldad and Senart, Aline and Follenfant, Corentin and Troncy, Rapha\"{e}l and Trastour, David	RUBIX: A Framework for Improving Data Integration with Linked Data	With today's public data sets containing billions of data items, more and more companies are looking to integrate external data with their traditional enterprise data to improve business intelligence analysis. These distributed data sources however exhibit heterogeneous data formats and terminologies and may contain noisy data. In this paper, we present RUBIX, a novel framework that enables business users to semi-automatically perform data integration on potentially noisy tabular data. This framework offers an extension to Google Refine with novel schema matching algorithms leveraging Freebase rich types. First experiments show that using Linked Data to map cell values with instances and column headers with types improves significantly the quality of the matching results and therefore should lead to more informed decisions.	2012	https://doi.org/10.1145/2422604.2422607	https://doi.org/10.1145/2422604.2422607	FALSE
Thoma, Steffen and Thalhammer, Andreas and Harth, Andreas and Studer, Rudi	FusE: Entity-Centric Data Fusion on Linked Data	Many current web pages include structured data which can directly be processed and used. Search engines, in particular, gather that structured data and provide question answering capabilities over the integrated data with an entity-centric presentation of the results. Due to the decentralized nature of the web, multiple structured data sources can provide similar information about an entity. But data from different sources may involve different vocabularies and modeling granularities, which makes integration difficult. We present FusE, an approach that identifies similar entity-specific data across sources, independent of the vocabulary and data modeling choices. We apply our method along the scenario of a trustable knowledge panel, conduct experiments in which we identify and process entity data from web sources, and compare the output to a competing system. The results underline the advantages of the presented entity-centric data fusion approach.	2019	https://doi.org/10.1145/3306128	https://doi.org/10.1145/3306128	FALSE
Hoffart, Johannes and Suchanek, Fabian M. and Berberich, Klaus and Lewis-Kelham, Edwin and de Melo, Gerard and Weikum, Gerhard	YAGO2: Exploring and Querying World Knowledge in Time, Space, Context, and Many Languages	We present YAGO2, an extension of the YAGO knowledge base with focus on temporal and spatial knowledge. It is automatically built from Wikipedia, GeoNames, and WordNet, and contains nearly 10 million entities and events, as well as 80 million facts representing general world knowledge. An enhanced data representation introduces time and location as first-class citizens. The wealth of spatio-temporal information in YAGO can be explored either graphically or through a special time- and space-aware query language.	2011	https://doi.org/10.1145/1963192.1963296	https://doi.org/10.1145/1963192.1963296	FALSE
Jindal, Shivani and Dua, Mohit and Virk, Zorawar Singh	KHIK: Karaka Based Hindi Language Interface to Knowledge Base	With the increased use of computer applications, database is gaining prime importance in every field for information retrieval. Large number of people with non technical background is dealing with these applications. However, database access is difficult for such users. The paper discussed a prototype, developed to retrieve the data from the database. NLIDB architecture is one which converts Hindi language query into database query language. The sentence is processed syntactically using karaka or post positions and language syntax, followed by semantic analysis. The formed SQL query is executed on the database and result is displayed to the user in Hindi language. Testing is done on Employee database. The system eliminates the need to have a fixed pattern and allows the user to write the same Hindi language query in different pattern.	2014	https://doi.org/10.1145/2677855.2677869	https://doi.org/10.1145/2677855.2677869	FALSE
Cojan, Julien and Cabrio, Elena and Gandon, Fabien	Filling the Gaps among DBpedia Multilingual Chapters for Question Answering	To publish information extracted from multilingual pages of Wikipedia in a structured way, the Semantic Web community has started an effort of internationalization of DBpedia. Multilingual chapters of DBpedia can in fact contain different information with respect to the English version, in particular they provide more specificity on certain topics, or fill information gaps. DBpedia multilingual chapters are well connected through instance interlinking, extracted from Wikipedia. An alignment between properties is also carried out by DBpedia contributors as a mapping from the terms used in Wikipedia to a common ontology, enabling the exploitation of information coming from the multilingual chapters of DBpedia. However, the mapping process is currently incomplete, it is time consuming since it is manually performed, and may lead to the introduction of redundant terms in the ontology, as it becomes difficult to navigate through the existing vocabulary. In this paper we propose an approach to automatically extend the existing alignments, and we integrate it in a question answering system over linked data. We report on experiments carried out applying the QAKiS (Question Answering wiKiframework-based) system on the English and French DBpedia chapters, and we show that the use of such approach broadens its coverage.	2013	https://doi.org/10.1145/2464464.2464500	https://doi.org/10.1145/2464464.2464500	FALSE
Singh, Kuldeep and Radhakrishna, Arun Sethupat and Both, Andreas and Shekarpour, Saeedeh and Lytra, Ioanna and Usbeck, Ricardo and Vyas, Akhilesh and Khikmatullaev, Akmal and Punjani, Dharmen and Lange, Christoph and Vidal, Maria Esther and Lehmann, Jens and Auer, S\"{o}ren	Why Reinvent the Wheel: Let's Build Question Answering Systems Together	Modern question answering (QA) systems need to flexibly integrate a number of components specialised to fulfil specific tasks in a QA pipeline. Key QA tasks include Named Entity Recognition and Disambiguation, Relation Extraction, and Query Building. Since a number of different software components exist that implement different strategies for each of these tasks, it is a major challenge to select and combine the most suitable components into a QA system, given the characteristics of a question. We study this optimisation problem and train classifiers, which take features of a question as input and have the goal of optimising the selection of QA components based on those features. We then devise a greedy algorithm to identify the pipelines that include the suitable components and can effectively answer the given question. We implement this model within Frankenstein, a QA framework able to select QA components and compose QA pipelines. We evaluate the effectiveness of the pipelines generated by Frankenstein using the QALD and LC-QuAD benchmarks. These results not only suggest that Frankenstein precisely solves the QA optimisation problem but also enables the automatic composition of optimised QA pipelines, which outperform the static Baseline QA pipeline. Thanks to this flexible and fully automated pipeline generation process, new QA components can be easily included in Frankenstein, thus improving the performance of the generated pipelines.	2018	https://doi.org/10.1145/3178876.3186023	https://doi.org/10.1145/3178876.3186023	FALSE
Ceccarelli, Diego and Gordea, Sergiu and Lucchese, Claudio and Nardini, Franco Maria and Perego, Raffale	When Entities Meet Query Recommender Systems: Semantic Search Shortcuts	The Web of Data is growing in popularity and dimension, and entities are gaining importance in many research fields. In this paper, we explore the use of entities that can be extracted from a query log to enhance query recommendation. In particular, we use a large query log recorded by the Europeana portal, a central access point to the descriptions of more than 20 million cultural heritage objects, and we extend a state-of-the-art query recommendation algorithm to take into account the semantic information associated with the submitted queries. Our novel method generates highly related and diversified suggestions. We assess it by means of a new evaluation technique. The manually annotated dataset used for performance comparisons has been made available to the research community to favor the repeatability of the experiments.	2013	https://doi.org/10.1145/2480362.2480540	https://doi.org/10.1145/2480362.2480540	FALSE
Mar\'{\i}n Arraiza, Paloma and Strobel, Sven	The TIB|AV Portal as a Future Linked Media Ecosystem	Various techniques for video analysis, concept mapping, semantic search and metadata management are part of the current features of the TIB|AV Portal as described in this demo. The segment identification and ontology annotation make the portal a good platform to support the Linked Data and Media. Weaving into a machine-readable metadata format will complete this task.	2015	https://doi.org/10.1145/2740908.2742912	https://doi.org/10.1145/2740908.2742912	FALSE
Annane, Amina and Emonet, Vincent and Azouaou, Fai\c{c}al and Jonquet, Clement	Multilingual Mapping Reconciliation between English-French Biomedical Ontologies	Even if multilingual ontologies are now more common, for historical reasons, in the biomedical domain, many ontologies or terminologies have been translated from one natural language to another resulting in two potentially aligned ontologies but with their own specificity (e.g., format, developers, and versions). Most often, there is no formal representation of the translation links between translated ontologies and original ones and those mappings are not formally available as linked data. However, these mappings are very important for the interoperability and the integration of multilingual biomedical data. In this paper, we propose an approach to represent translation mappings between ontologies based on the NCBO BioPortal format. We have reconciled more than 228K mappings between ten English ontologies hosted on NCBO BioPortal and their French translations. Then, we have stored both the translated ontologies and mappings on a French customized version of the platform, called the SIFR BioPortal, making the whole thing available in RDF. Reconciling the mappings turned more complex than expected because the translations are rarely exactly the same than the original ontologies as discussed in this paper.	2016	https://doi.org/10.1145/2912845.2912847	https://doi.org/10.1145/2912845.2912847	FALSE
Diefenbach, Dennis and Migliatti, Pedro Henrique and Qawasmeh, Omar and Lully, Vincent and Singh, Kamal and Maret, Pierre	QAnswer: A Question Answering Prototype Bridging the Gap between a Considerable Part of the LOD Cloud and End-Users	We present QAnswer, a Question Answering system which queries at the same time 3 core datasets of the Semantic Web, that are relevant for end-users. These datasets are Wikidata with Lexemes, LinkedGeodata and Musicbrainz. Additionally, it is possible to query these datasets in English, German, French, Italian, Spanish, Pourtuguese, Arabic and Chinese. Moreover, QAnswer includes a fallback option to the search engine Qwant when the answer to a question cannot be found in the datasets mentioned above. These features make QAnswer as the first prototype of a Question Answering System over a considerable part of the LOD cloud.	2019	https://doi.org/10.1145/3308558.3314124	https://doi.org/10.1145/3308558.3314124	TRUE
Sima, Ana Claudia and Mendes de Farias, Tarcisio and Anisimova, Maria and Dessimoz, Christophe and Robinson-Rechavi, Marc and Zbinden, Erich and Stockinger, Kurt	Bio-SODA: Enabling Natural Language Question Answering over Knowledge Graphs without Training Data	 The problem of natural language processing over structured data has become a growing research field, both within the relational database and the Semantic Web community, with significant efforts involved in question answering over knowledge graphs (KGQA). However, many of these approaches are either specifically targeted at open-domain question answering using DBpedia, or require large training datasets to translate a natural language question to SPARQL in order to query the knowledge graph. Hence, these approaches often cannot be applied directly to complex scientific datasets where no prior training data is available. In this paper, we focus on the challenges of natural language processing over knowledge graphs of scientific datasets. In particular, we introduce Bio-SODA, a natural language processing engine that does not require training data in the form of question-answer pairs for generating SPARQL queries. Bio-SODA uses a generic graph-based approach for translating user questions to a ranked list of SPARQL candidate queries. Furthermore, Bio-SODA uses a novel ranking algorithm that includes node centrality as a measure of relevance for selecting the best SPARQL candidate query. Our experiments with real-world datasets across several scientific domains, including the official bioinformatics Question Answering over Linked Data (QALD) challenge, as well as the CORDIS dataset of European projects, show that Bio-SODA outperforms publicly available KGQA systems by an F1-score of least 20% and by an even higher factor on more complex bioinformatics datasets. 	2021	https://doi.org/10.1145/3468791.3469119	https://doi.org/10.1145/3468791.3469119	FALSE
Kaffee, Lucie-Aim\'{e}e and Endris, Kemele M. and Simperl, Elena and Vidal, Maria-Esther	Ranking Knowledge Graphs By Capturing Knowledge about Languages and Labels	Capturing knowledge about the mulitilinguality of a knowledge graph is of supreme importance to understand its applicability across multiple languages. Several metrics have been proposed for describing mulitilinguality at the level of a whole knowledge graph. Albeit enabling the understanding of the ecosystem of knowledge graphs in terms of the utilized languages, they are unable to capture a fine-grained description of the languages in which the different entities and properties of the knowledge graph are represented. This lack of representation prevents the comparison of existing knowledge graphs in order to decide which are the most appropriate for a multilingual application.In this work, we approach the problem of ranking knowledge graphs based on their language features and propose LINGVO, a framework able to capture mulitilinguality at different levels of granularity. Grounded in knowledge graph descriptions, LINGVO is, additionally, able to solve the problem of ranking knowledge graphs according to a degree of mulitilinguality of the represented entities. We have empirically studied the effectiveness of LINGVO in a benchmark of queries to be executed against existing knowledge graphs. The observed results provide evidence that LINGVO captures the mulitilinguality of the studied knowledge graphs similarly than a crowd-sourced gold standard.	2019	https://doi.org/10.1145/3360901.3364443	https://doi.org/10.1145/3360901.3364443	FALSE
Souza Costa, Tarc\'{\i}sio and Gottschalk, Simon and Demidova, Elena	Event-QA: A Dataset for Event-Centric Question Answering over Knowledge Graphs	Semantic Question Answering (QA) is a crucial technology to facilitate intuitive user access to semantic information stored in knowledge graphs. Whereas most of the existing QA systems and datasets focus on entity-centric questions, very little is known about these systems' performance in the context of events. As new event-centric knowledge graphs emerge, datasets for such questions gain importance. In this paper, we present the Event-QA dataset for answering event-centric questions over knowledge graphs. Event-QA contains 1000 semantic queries and the corresponding English, German and Portuguese verbalizations for EventKG - an event-centric knowledge graph with more than 970 thousand events.	2020	https://doi.org/10.1145/3340531.3412760	https://doi.org/10.1145/3340531.3412760	TRUE
HongZhou, Duan and Lee, Yongju	Design and Implementation of Location-Aware Semantic Mobile Mashups	Multimedia and communication capabilities of the Smart-phone provide the environment for creating and advanced mashup content that combines with Linked Data. This paper describes the design and implementation of the location-aware semantic mobile mashup system on Smartphones. The Geospatial Semantic Web allows for the use of RDF (Resource Description Framework) to describe locations in an open and distributed manner and to provide the relevant data on locations between data resources. Our system provides mashups of Google Maps and DBpedia based on users' current location, displays surrounding DBpedia objects, and links search for additional semantic information. DBpedia is emerging as the core of LOD (Linked Open Data), which is gathering information based on Wikipedia and using it to develop the large scale of Big Data. Developed location-aware semantic mobile mashups can find where you want to be on your current location through GPS. They can navigate through the data links to other data sources to see contextual information about their surroundings and can easily access the semantic information over Linked Data browsers.	2018	https://doi.org/10.1145/3233740.3233754	https://doi.org/10.1145/3233740.3233754	FALSE
Lakshen, Guma Abdulkhader and Janev, Valentina and Vrane\v{s}, Sanja	Challenges in Quality Assessment of Arabic DBpedia	The development of Semantic Web technology has fueled the creation of a large amount of Linked Open Data. DBpedia is a good example of an open data repository extracted from the crowd sourced knowledge base, Wikipedia. Because of the way Wikipedia and DBpedia were created, the information available there is more vulnerable to grammatical errors, inconsistency, structures, and, data type problems. The introduction of the Arabic chapter of DBpedia created additional problems due to the nature of the language, when it comes to quality assessment issues. In this paper 1, we focus on identifying challenges in quality assessment of Arabic DBpedia, as well as analysis of existing tools and methodologies used for Linked Data quality assessments.	2018	https://doi.org/10.1145/3227609.3227675	https://doi.org/10.1145/3227609.3227675	FALSE
Reda, Roberto and Carbonaro, Antonella	Design and Development of a Linked Open Data-Based Web Portal for Sharing IoT Health and Fitness Datasets	The huge amounts of self-tracked health data collected by Internet of Things (IoT) fitness devices offer important opportunities to the research community. If properly exploited, IoT health and fitness datasets can help to gain valuable insights into the human health in order to provide better healthcare.However, IoT health data come from a variety of different heterogeneous sources and in proprietary formats, which means that they require an integration process, normally manually done by domain experts, in order to be analysed. This task is not only significantly time consuming but in many cases, error prone.In this study, we designed and developed a web platform for collecting and publishing IoT health and fitness datasets according to Linked Data principles. We leveraged the IFO ontology and the Semantic Web technologies to make the IoT health and fitness datasets freely available to the community in a shared, semantically meaningful, easily discoverable, and reusable manner.The system introduced in this article shows that Semantic Web technologies can be a viable and comprehensive solution for describing, integrating and sharing heterogeneous IoT datasets, thus overcoming the issues of data silos that nowadays dominate the IoT landscape.	2018	https://doi.org/10.1145/3284869.3284890	https://doi.org/10.1145/3284869.3284890	FALSE
Torre-Bastida, Ana I. and Berm\'{u}dez, Jes\'{u}s and Illarramendi, Arantza	A Rule-Based Transducer for Querying Incompletely Aligned Datasets	A growing number of Linked Open Data sources (from diverse provenances and about different domains) that can be freely browsed and searched to find and extract useful information have been made available. However, access to them is difficult for different reasons. This study addresses access issues concerning heterogeneity. It is common for datasets to describe the same or overlapping domains while using different vocabularies. Our study presents a transducer that transforms a SPARQL query suitably expressed in terms of the vocabularies used in a source dataset into another SPARQL query suitably expressed for a target dataset involving different vocabularies. The transformation is based on existing alignments between terms in different datasets. Whenever the transducer is unable to produce a semantically equivalent query because of the scarcity of term alignments, the transducer produces a semantic approximation of the query to avoid returning the empty answer to the user. Transformation across datasets is achieved through the management of a wide range of transformation rules. The feasibility of our proposal has been validated with a prototype implementation that processes queries that appear in well-known benchmarks and SPARQL endpoint logs. Results of the experiments show that the system is quite effective in achieving adequate transformations.	2018	https://doi.org/10.1145/3228328	https://doi.org/10.1145/3228328	FALSE
Hasibi, Faegheh and Nikolaev, Fedor and Xiong, Chenyan and Balog, Krisztian and Bratsberg, Svein Erik and Kotov, Alexander and Callan, Jamie	DBpedia-Entity v2: A Test Collection for Entity Search	The DBpedia-entity collection has been used as a standard test collection for entity search in recent years. We develop and release a new version of this test collection, DBpedia-Entity v2, which uses a more recent DBpedia dump and a unified candidate result pool from the same set of retrieval models. Relevance judgments are also collected in a uniform way, using the same group of crowdsourcing workers, following the same assessment guidelines. The result is an up-to-date and consistent test collection.To facilitate further research, we also provide details about the pre-processing and indexing steps, and include baseline results from both classical and recently developed entity search methods.	2017	https://doi.org/10.1145/3077136.3080751	https://doi.org/10.1145/3077136.3080751	FALSE
Luggen, Michael and Audiffren, Julien and Difallah, Djellel and Cudr\'{e}-Mauroux, Philippe	Wiki2Prop: A Multimodal Approach for Predicting Wikidata Properties from Wikipedia	 Wikidata is rapidly emerging as a key resource for a multitude of online tasks such as Speech Recognition, Entity Linking, Question Answering, or Semantic Search. The value of Wikidata is directly linked to the rich information associated with each entity – that is, the properties describing each entity as well as the relationships to other entities. Despite the tremendous manual and automatic efforts the community invested in the Wikidata project, the growing number of entities (now more than 100 million) presents multiple challenges in terms of knowledge gaps in the graph that are hard to track. To help guide the community in filling the gaps in Wikidata, we propose to identify and rank the properties that an entity might be missing. In this work, we focus on entities with a dedicated Wikipedia page in any language to make predictions directly based on textual content. We show that this problem can be formulated as a multi-label classification problem where every property defined in Wikidata is a potential label. Our main contribution, Wiki2Prop, solves this problem using a multimodal Deep Learning method to predict which properties should be attached to a given entity, using its Wikipedia page embeddings. Moreover, Wiki2Prop is able to incorporate additional features in the form of multilingual embeddings and multimodal data such as images whenever available. We empirically evaluate our approach against the state of the art and show how Wiki2Prop significantly outperforms its competitors for the task of property prediction in Wikidata, and how the use of multilingual and multimodal data improves the results further. Finally, we make Wiki2Prop available as a property recommender system that can be activated and used directly in the context of a Wikidata entity page.	2021	https://doi.org/10.1145/3442381.3450082	https://doi.org/10.1145/3442381.3450082	FALSE
Heindorf, Stefan and Potthast, Martin and Stein, Benno and Engels, Gregor	Vandalism Detection in Wikidata	Wikidata is the new, large-scale knowledge base of the Wikimedia Foundation. Its knowledge is increasingly used within Wikipedia itself and various other kinds of information systems, imposing high demands on its integrity. Wikidata can be edited by anyone and, unfortunately, it frequently gets vandalized, exposing all information systems using it to the risk of spreading vandalized and falsified information. In this paper, we present a new machine learning-based approach to detect vandalism in Wikidata. We propose a set of 47 features that exploit both content and context information, and we report on 4 classifiers of increasing effectiveness tailored to this learning task. Our approach is evaluated on the recently published Wikidata Vandalism Corpus WDVC-2015 and it achieves an area under curve value of the receiver operating characteristic, ROC-AUC, of 0.991. It significantly outperforms the state of the art represented by the rule-based Wikidata Abuse Filter (0.865 ROC-AUC) and a prototypical vandalism detector recently introduced by Wikimedia within the Objective Revision Evaluation Service (0.859 ROC-AUC).	2016	https://doi.org/10.1145/2983323.2983740	https://doi.org/10.1145/2983323.2983740	FALSE
Tanase, Diana and Kapetanios, Epaminondas	Are SKOS Concept Schemes Ready for Multilingual Retrieval Applications?	This article describes our approach to accessing Knowledge Organization Systems expressed using the Simple Knowledge Organization System (SKOS) data model. We share the view that the Web is becoming a multilingual lexical resource and a distribution infrastructure for knowledge resources. We aim to tap into this for the particular use case of Cross-Language Information Retrieval systems. The SKOS framework allows the description of monolingual or multilingual thesauri, controlled vocabularies and other classification systems in a simple machine-understandable representation. It has support for decentralized distribution on the Web of any resource described with it and includes mechanisms to interconnect different concept schemes. Yet, when building our prototype CLIR system different processes require more than the existing content of a SKOS resource: concept descriptions, labels and basic inter-concept relations. For example the SKOS concept indexing phase entails identifying potential occurrences of a SKOS concept in a text and to disambiguate based on the semantics referenced to in the overall SKOS scheme. By design, the SKOS data model does not formally define semantics of its concepts thus we have built a set of three algorithms that help generate a multilingual dataset linking to the original SKOS dataset and providing more details about the lexical entities that describe concepts. This new dataset contains specific RDF triples that aid concept identification, disambiguation and translation in CLIR.	2012	https://doi.org/10.1145/2362499.2362520	https://doi.org/10.1145/2362499.2362520	FALSE
Diefenbach, Dennis and Singh, Kamal and Maret, Pierre	WDAqua-Core1: A Question Answering Service for RDF Knowledge Bases	In the last two decades a new part of the web grew significantly, namely the Semantic Web. It contains many Knowledge Bases (KB) about different areas like music, books, publications, live science and many more. Question Answering (QA) over KBs is seen as the most promising approach to bring this data to end-users. We describe WDAqua-core1, a QA service for querying RDF knowledge-bases. It is multilingual, it supports different RDF knowledge bases and it understands both full natural language questions and keyword questions.	2018	https://doi.org/10.1145/3184558.3191541	https://doi.org/10.1145/3184558.3191541	TRUE
Aleksandr Perevalov, Andreas Both, Dennis Diefenbach, Axel-Cyrille Ngonga Ngomo	Can Machine Translation be a Reasonable Alternative for Multilingual Question Answering Systems over Knowledge Graphs?	Providing access to information is the main and most important purpose of the Web. However, despite available easy-to-use tools (e.g., search engines, chatbots, question answering) the accessibility is typically limited by the capability of using the English language. This excludes a huge amount of people. In this work, we discuss Knowledge Graph Question Answering (KGQA) systems that aim at providing natural language access to data stored in Knowledge Graphs (KG). While several KGQA systems have been proposed, only very few have dealt with a language other than English. In this work, we follow our research agenda of enabling speakers of any language to access the knowledge stored in KGs. Because of the lack of native support for many languages, we use machine translation (MT) tools to evaluate KGQA systems regarding questions in languages that are unsupported by a KGQA system. In total, our evaluation is based on 8 different languages (including some that never were evaluated before). For the intensive evaluation, we extend the QALD-9 dataset for KGQA with Wikidata queries and high-quality translations. The extension was done in a crowdsourcing manner by native speakers of the different languages. By using multiple KGQA systems for the evaluation, we were enabled to investigate and answer the main research question: “Can MT be an alternative for multilingual KGQA systems?”. The evaluation results demonstrated that the monolingual KGQA systems can be effectively ported to the new languages with MT tools.	2022	https://doi.org/10.1145/3485447.3511940	https://dl.acm.org/doi/10.1145/3485447.3511940	TRUE
Srivastava, Nikit and Perevalov, Aleksandr and Kuchelev, Denis and Moussallem, Diego and Ngonga Ngomo, Axel-Cyrille and Both, Andreas	Lingua Franca – Entity-Aware Machine Translation Approach for Question Answering over Knowledge Graphs	This research paper proposes an approach called Lingua Franca that improves machine translation quality by utilizing information from a knowledge graph to translate named entities accurately. The accurate entity translation is crucial when applied to entity-oriented search including Knowledge Graph Question Answering systems. In a nutshell, the approach preserves recognized named entities with an entity-replacement technique during the translation process. It replaces the entities back with their labels found in a knowledge graph for the target language to ensure that questions are translated correctly before answering them using a Knowledge Graph Question Answering system. The paper also introduces an open-source modular framework that enables researchers to design their own named entity-aware machine translation pipelines. The presented experimental results demonstrate the effectiveness of the Lingua Franca approach in comparison to baseline Machine Translation models. The approach shows a statistically significant improvement in the quality provided by several Knowledge Graph Question Answering systems using Lingua Franca on different datasets	https://doi.org/10.1145/3587259.3627567	https://doi.org/10.1145/3587259.3627567